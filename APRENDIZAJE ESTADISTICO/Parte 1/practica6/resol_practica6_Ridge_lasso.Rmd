
---
title: "Ridge - Lasso"
output: pdf_document
---
# Regresión lineal con regularización

Para el caso de la regresión Ridge podremos usar los paquetes MASS y glmnet.
Seguiremos la práctica 6.5.2. del capítulo 6 del libro. 

### Observamos los datos
Veamos primero hasta que punto está justificado Ridge o/y Lasso.


```{r}
library(ISLR2)
library(MASS)
library(glmnet)
# Hitters <- read.csv("~/matematicas/aprendizaje estadistico/LIBRO/ALL CSV FILES - 2nd Edition/Hitters.csv")
names(Hitters)
dim(Hitters)
?Hitters
```

Hay que eliminar las muestras con NA en Salary

```{r}
sum(is.na(Hitters$Salary))
Hitters <- na.omit(Hitters)
dim(Hitters)
attach(Hitters)
```

Observamos que hay variables no numéricas,
```{r}
str(Hitters)
```

No hace falta redefinir las variables de "texto" para hacer la regresión lineal. 


```{r}
lineal <- lm(Salary ~ ., Hitters)
coef(lineal)
```
No es un modelo que vaya a tener un buen ajuste. 

Veámos primero estadísticos a partir del modelo:

```{r}
summary(lineal)
```

Observamos que R^2= 0.54: 54% de la variabilidad de la variable Salary se explica con el resto de las variables. Además los t values indican que ciertos parámetros del modelo podrían ser nulos.

Estudiemos/Estimemos el MSE:

```{r}
MSE_all <- mean(resid(lineal)^2) # cogiendo todas las observaciones
MSE_all
mean(Hitters$Salary)
```

Veamos con CV con 10 iteraciones una mejor estimación del MSE, y con bootstrapping, una mejor estimación de los SE.

 
```{r}
library(boot)
glm.fit <- glm(Salary ~ ., data = Hitters)
cv.glm(Hitters, glm.fit, K = 10)$delta[1] # mucho mayor que el MSE con todo el conjunto de entrenamiento
```

```{r}
bootfuncion <- function(data, index) {
    coef(lm(formula = Salary ~ ., data = Hitters, subset = index))
}
boot(Hitters, bootfuncion, 1000)
```

#### Estudiemos ahora la colinealidad:

Veamos el factor de inflación de la varianza (VIF: Variance Inflation Factor), 
VIF($\beta_j)=\frac{1}{1-R^2_{X_j|X_{-j}}}$.

```{r}
library(car)
vif(lineal)
```
Más indicadores:

```{r}
library(corrplot)
cor(Hitters[, c(-14, -15, -20)])
corrplot(cor(Hitters[, c(-14, -15, -20)]), type = "upper", tl.cex = 0.5)
```


Todas estas observaciones llevan a pensar que el modelo lineal así considerado no es el idóneo. La colinialidad que existe entre algunas variables implica la mala estabilidad del modelo de regresión lineal.



# Ridge y Lasso

Todo indica que si hacemos ridge o lasso, facilmente encontraremos un modelo mejor.

Para ello, utilizaremos el paquete glmnet. Esta función tiene una sintaxis ligeramente diferente a la de otras funciones de ajuste del modelo que hemos encontrado hasta ahora en este libro. En particular, como los predictores como la variable que se quiere predecir se introducen en forma vectorial/matricial.

La función model.matrix() no solo produce la matriz correspondiente correspondiente a los 19 predictores, sino que también transforma automáticamente cualquier variable cualitativa en variables dummy. Esta última propiedad es importante porque glmnet() solo puede tomar entradas numéricas.
 

```{r}
x <- model.matrix(Salary ~ ., Hitters)[, -1] ## ¿Por qué -1?
y <- Hitters$Salary
```

Nota: el condition number=$\sigma_1/\sigma_{p+1}$ de la matriz  $A^t\cdot A$  es 4.242998854.10^8, lo que indica que está mal condicionada, es decir, la solución por mínimo cuadrados no es nada estable.

Realizamos el ajuste de Ridge tomando valores diferentes del parámetro $\lambda$ (tuning parameter). Recuerda que a mayor valor de lambda, menos flexibilidad, menor valor numérico en valor absoluto de los parámetros del modelo porque su longitud es menor.

Si seguimos la práctica del libro, elegimos los lambda's nosotros.
Dicho esto, por defecto R elige 100 valores.

```{r}
# glmnet(x,y,alpha=0)
# seq(10, -2, length = 100)
malla <- 10^seq(10, -2, length = 100)
```
Aunque utilicemos la herramienta de glmnet, también se puede realizar el método de Ridge con "lm.ridge".

```{r}
malla_ridge <- glmnet(x, y, alpha = 0, lambda = malla)
```

Obtenemos así tantas columnas como valores de lambda; cada una nos da un modelo.

```{r}
dim(coef(malla_ridge))
malla_ridge$lambda[50]
coef(malla_ridge)[, 50]
```
La norma de los coeficientes estimados:

```{r}
sqrt(sum(coef(malla_ridge)[-1, 50]^2))
```
Si comparamos con otro valor de $\lambda$:

```{r}
malla_ridge$lambda[75]
coef(malla_ridge)[, 75]
```
La norma tiene que se mayor porque $\lambda$ es menor:

```{r}
sqrt(sum(coef(malla_ridge)[-1, 75]^2))
```

La norma de los coeficientes del modelo lineal es mucho mayor:

```{r}
sqrt(sum(coef(lineal)^2))
```
Con un plot visualizamos los diferentes valores de los parámetros para los diferentes valores de $\lambda$

```{r}
plot(malla_ridge, xvar = "lambda")
abline(v = log(malla_ridge$lambda[75]), col = "blue")
``` 
 

Podemos usar la función predict() para varios propósitos. Por ejemplo, podemos obtener los coeficientes de regresión de Ridge para un nuevo valor de $\lambda$, digamos 50:

```{r}
predict(malla_ridge, s = 50, type = "coefficients")[1:20, ]
``` 

Ahora dividimos la base de datos en un conjunto de entrenamiento y un conjunto de validación para estimar el MSE. 

```{r}
set.seed(1)
train <- sample(1:nrow(x), nrow(x) / 2)
# train
test <- (-train)
y.test <- y[test]
``` 

A continuación, obtenemos el modelo de Ridge en el conjunto de entrenamiento y evaluamos su MSE en el conjunto de validación, usando $\lambda$ = ridge.train$lambda[75].
 
```{r}  
ridge.train <- glmnet(x[train, ], y[train], alpha = 0, lambda = malla, thresh = 1e-12)
ridge.pred <- predict(ridge.train, s = ridge.train$lambda[75], newx = x[test, ])
```

MSE para este modelo:

```{r} 
mean((ridge.pred - y.test)^2)
```

Ahora comprobamos si hay algún beneficio en realizar la regresión Ridge con $\lambda$ = ridge.train$lambda[75] en lugar de solo realizar la regresión de mínimos cuadrados.

Podemos realizarlo de dos maneras diferentes. Una es utilizando el modelo "ridge.train", con s=0:
```{r} 
ridgelineal.pred <- predict(ridge.train, s = 0, newx = x[test, ], exact = T, x = x[train, ], y = y[train])
mean((ridgelineal.pred - y.test)^2)
```

Y otra es utilizando lm:
```{r}
# lm.train<-lm( Salary[train]~.,Hitters[train,  ])
lm.train <- lm(y ~ x, subset = train)
lm.pred <- predict(lm.train, Hitters)
mean((y - lm.pred)[-train]^2)
```
 Observamos que el MSE con el modelo lineal es mayor. 
 


### SELECCIÓN DEL PARÁMETRO DE PENALIZACIÓN 
En general, en lugar de elegir arbitrariamente $\lambda$, sería mejor usar la validación cruzada para elegir el parámetro $\lambda$ con la función cv.glmnet(). Por defecto, se hace la CV con 10 iteraciones (10-fold CV).

 
```{r} 
set.seed(1)
cv.out <- cv.glmnet(x[train, ], y[train], alpha = 0)
plot(cv.out)
bestlam <- cv.out$lambda.min
bestlam
```

MSE estimado con el conjunto test, asociado con este valor de lambda:
 
```{r} 
k1 <- glmnet(x[train, ], y[train], alpha = 0, lambda = bestlam)
ridge.pred <- predict(k1, newx = x[test, ])
mean((ridge.pred - y.test)^2)
```

Finalmente no olvidemos que se debe realizar el modelo Ridge con toda la muestra para su posible uso con un test set "futuro"

```{r}
ridge_TODO <- glmnet(x, y, alpha = 0, lambda = bestlam)
coef(ridge_TODO)[, 1]
ridge.predT <- predict(ridge_TODO, newx = x)
mean((ridge.predT - y)^2)
```

Este paso en el libro se hace de manera diferente.

# Lasso

```{r}
lasso.mod <- glmnet(x[train, ], y[train], alpha = 1, lambda = malla)
plot(lasso.mod, xvar = "lambda")
```

Podemos ver en el gráfico de coeficientes que, dependiendo de la elección del parámetro de ajuste $\lambda$, algunos de los coeficientes serán exactamente iguales a cero. Ahora realizamos la validación cruzada y calculamos el MSE.

```{r}
set.seed(1)
cv.out <- cv.glmnet(x[train, ], y[train], alpha = 1)
plot(cv.out)
bestlam <- cv.out$lambda.min
bestlam
lasso.pred <- predict(lasso.mod, s = bestlam, newx = x[test, ])
mean((lasso.pred - y.test)^2)
```
Mejor que el del modelo lineal y parecido al de Ridge. 
Sin embargo, aquí hay 11 variables.


```{r}
# coef(cv.out,s="lambda.min")
out <- glmnet(x, y, alpha = 1, lambda = malla)
lasso.coef <- predict(out, type = "coefficients", s = bestlam)[1:20, ]
lasso.coef[1:10]
```

También, si calculamos el modelo directamente con el mejor lambda, aunque no hallemos las mismas variables, la estimación de Salary es similar:

```{r}
lasso.Directo <- glmnet(x, y, alpha = 1, lambda = bestlam)
coef(lasso.Directo)
lasso.predDir <- predict(lasso.Directo, newx = x)
mean((lasso.predDir - y)^2)
lasso.predDir[1:10]
```
```{r}
predict(out, s = bestlam, newx = x)[1:10]
```


