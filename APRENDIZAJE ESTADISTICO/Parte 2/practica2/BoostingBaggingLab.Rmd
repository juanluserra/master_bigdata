---
title: "Práctica 2"
author: "Aprendizaje Estadístico, Máster en Tecnologías de Análisis de Datos Masivos: Big Data. Profesor: Juan A. Botía (juanbot@um.es)"
date: "`r Sys.Date()`"
output:
  
  html_document:
    df_print: paged
    highlight: kate
    number_sections: yes
    theme: spacelab
    toc: yes
    toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# What are we using today?

El paquete R [randomForest](https://cran.r-project.org/web/packages/randomForest/index.html). Aunque tenemos otras opciones que funcionan de forma similar, como
[ranger](https://cran.r-project.org/web/packages/ranger/index.html) y [Rborist](https://cran.r-project.org/web/packages/Rborist/vignettes/rborist.html).

Para el boosting también vamos a usar Gradient Boosting Machines,  [GBM](https://cran.r-project.org/web/packages/gbm/index.html).

# Bagging

Empezamos en esta sección aplicando *bagging* y *random forests* al conjunto de datos de `Boston`, que ya conocemos. Los algoritmos que necesitamos están en el paquete `randomForest`que ya hemos mencionado arriba. 

Los resultados exactos obtenidos en esta sección pueden depender de la versión de R y la versión del paquete `randomForest` instalada en tu ordenador.

Primero, echemos un vistazo al conjunto de datos de `Boston`, solo para recordar qué información contiene, teniendo en cuenta que estamos interesados en predecir el valor de una propiedad en los suburbios de Boston.

```{r}
set.seed(1234)
library(ISLR2)
data(Boston)
str(Boston)
```

Pero podemos hacerlo mejor, espera un momento.

```{r,result="asis"}
library(summarytools)
print(dfSummary(Boston), method = "render")
```

Recuerda que el *bagging* (bootstrap aggregating) es simplemente un caso especial de un random forest con $m=p$ siendo $p$ el número de predictores del dataset de training y $m$ el número de predictores que usamos en cada split del árbol que construimos en cada momento. 

Por lo tanto, la función **`randomForest()` se puede usar para realizar tanto random forests como bagging**. 

Realizamos bagging de la siguiente manera:

```{r}
set.seed(1)
train <- sample(1:nrow(Boston), nrow(Boston) / 2)
boston.test <- Boston[-train, "medv"]

library(randomForest)
bag.boston <- randomForest(medv ~ .,
  data = Boston,
  subset = train,
  mtry = 12, importance = TRUE
)
```

El argumento `mtry = 12` indica que se deben considerar los 12 predictores para cada división del árbol; en otras palabras, que se debe realizar *bagging*. ¿Cómo se comporta este modelo de  bagging en el conjunto de prueba?

Como estamos haciendo regresión, vamos a visualizarlo con un plot de dispersión contrastando la predicción de bagging, `yhat.bag` con la variable de salida `boston.test`

```{r}
yhat.bag <- predict(bag.boston, newdata = Boston[-train, ])
plot(yhat.bag, boston.test)
abline(0, 1)
mean((yhat.bag - boston.test)^2)
```

* Vemos que visualmente parece que hace un buen trabajo.

* El MSE (Error Cuadrático Medio) del conjunto de prueba asociado con el árbol de regresión bagging es de 23.42, aproximadamente dos tercios del obtenido utilizando un único árbol podado óptimamente. 

Podemos cambiar el número de árboles generados por `randomForest()` utilizando el argumento `ntree`:

```{r}
bag.boston <- randomForest(medv ~ .,
  data = Boston,
  subset = train, mtry = 12, ntree = 25
)
yhat.bag <- predict(bag.boston, newdata = Boston[-train, ])
mean((yhat.bag - boston.test)^2)
```

Para generar un random forest debemos procede exactamente de la misma manera, excepto que usaremos un valor más pequeño para el argumento `mtry`. Por defecto, `randomForest()` utiliza 

* $p/3$ variables al construir un random forest de árboles de regresión, y 

* $\sqrt{p}$ variables al construir un random forest de árboles de clasificación. 

Aquí usamos mtry = 6.

```{r}
rf.boston <- randomForest(medv ~ .,
  data = Boston,
  subset = train, mtry = 6, importance = TRUE
)
yhat.rf <- predict(rf.boston, newdata = Boston[-train, ])
mean((yhat.rf - boston.test)^2)
```

El MSE del conjunto de prueba es 20.17; esto indica que los random forests proporcionaron una mejora sobre el bagging en este caso.
Utilizando la función importance(), podemos ver la importancia de cada variable.

```{r}
importance(rf.boston)
```

Se reportan dos medidas de importancia de las variables. 

* La primera se basa en la disminución promedio de la precisión en las predicciones (error de clasificación o MSE, según hablemos de clasificación o regresión) en las muestras fuera de la bolsa (out of bag, OOB) cuando se permuta un predictor a lo largo de los árboles.

    + Permutar un predictor significa barajar los valores particulares de este predictor para los ejemplos OOB. Con esto conseguimos randomizar el efecto real de este predictor
    
    + Si, en promedio, el error aumenta mucho significa que la importancia de este predictor en el árbol es alta

* La segunda es una medida de la disminución total de la impureza de los nodos que resulta de las divisiones en esa variable, promediada sobre todos los árboles. 

    + En el caso de los árboles de regresión, la impureza de los nodos se mide mediante el RSS (Suma de Residuos al Cuadrado), y 
    
    + para los árboles de clasificación mediante la desviación.

Se pueden producir gráficos de estas medidas de importancia utilizando la función `varImpPlot()`.

```{r}
varImpPlot(rf.boston)
```

Los resultados indican que, en todos los árboles considerados en el random forest, la riqueza de la comunidad (lstat) y el tamaño de la vivienda (rm) son, con diferencia, las dos variables más importantes.

# Boosting

Ahora vamos a ver cómo aplicar Boosting. Para ello vamos a utilizar el paquete `gbm`, y dentro de este la función `gbm()`, para ajustar árboles de regresión potenciados (boosted regression trees) al conjunto de datos de Boston.

* Ejecutamos `gbm()` con la opción distribution = "gaussian" ya que se trata de un problema de regresión; 

* si fuera un problema de clasificación binaria, utilizaríamos distribution = "bernoulli". 

* El argumento `n.trees = 5000` indica que queremos 5000 árboles, 

* la opción `interaction.depth = 4` limita la profundidad de cada árbol.


Recuerda que `medv` es el valor medio de las viviendas ocupadas por sus propietarios en miles de dólares (consulta `help(Boston)` después de ejecutar `library(ISLR2); data(Boston))`.


```{r}
library(gbm)
boost.boston <- gbm(medv ~ .,
  data = Boston[train, ],
  distribution = "gaussian",
  n.trees = 5000,
  interaction.depth = 4
)
```


La función `summary()` produce un plot con la influencia relativa que tiene cada variable en la reducción del error cuadrático, si la distribución es Gausiana, como es este caso.

```{r}
summary(boost.boston, las = 2)
```

Vemos que lstat y rm son, con diferencia, las variables más importantes. También podemos producir gráficos de dependencia parcial para estas dos variables. Estos gráficos ilustran el efecto marginal de las variables seleccionadas en la respuesta después de integrar las demás variables. En este caso, como era de esperar, los precios medianos de las viviendas aumentan con rm y disminuyen con lstat.

```{r}
plot(boost.boston, i = "rm")
plot(boost.boston, i = "lstat")
```

Ahora usamos el modelo potenciado (boosted) para predecir medv en el conjunto de prueba:

```{r}
yhat.boost <- predict(boost.boston,
  newdata = Boston[-train, ], n.trees = 5000
)
mean((yhat.boost - boston.test)^2)
```

El MSE de prueba obtenido es 16.86: esto es superior al MSE de prueba de random forests y bagging (ver más abajo). Si queremos, podemos realizar boosting con un valor diferente del parámetro de reducción $\lambda$. El valor predeterminado es $0.001$, pero esto se puede modificar fácilmente. Aquí tomamos $\lambda = 0.2$.

```{r}
boost.boston <- gbm(medv ~ .,
  data = Boston[train, ],
  distribution = "gaussian", n.trees = 5000,
  interaction.depth = 4, shrinkage = 0.2, verbose = F
)
yhat.boost <- predict(boost.boston, newdata = Boston[-train, ], n.trees = 5000)
mean((yhat.boost - boston.test)^2)
```

En este caso, usar $\lambda = 0.2$ conduce a un MSE de prueba más alto que $\lambda = 0.001$.

# Ejercicios

Ejercicio 1. Estudio al efecto de `ntree` en el rendimiento sobre el conjunto de prueba, al predecir `medv` en el dataset `Boston`

```{r}
data(Boston)
train <- sample(1:nrow(Boston), nrow(Boston) / 2)
boston.test <- Boston[-train, "medv"]
```

```{r}
ntree_values <- c(10, 50, 100, 200, 500)
mse_values <- numeric(length(ntree_values))

for (i in 1:length(ntree_values)) {
  set.seed(1)
  bag.boston <- randomForest(medv ~ .,
                             data = Boston,
                             subset = train,
                             mtry = 12,
                             ntree = ntree_values[i],
                             importance = TRUE)
  yhat.bag <- predict(bag.boston, newdata = Boston[-train, ])
  mse_values[i] <- mean((yhat.bag - boston.test)^2)
}
```

```{r}
results <- data.frame(ntree = ntree_values, MSE = mse_values)
print(results)
```

```{r}
plot(ntree_values, mse_values, type = "b", xlab = "Número de árboles (ntree)", ylab = "MSE", main = "Efecto de ntree en el rendimiento del modelo")
```

Ejercicio 2. Compara los errores de test de Bagging, random forest y boosting. ¿Qué puedes decir de ellos? ¿Cómo podrías diseñar una comparación justa?

Ejercicio 3. Compara las estimaciones de importancias de las variables de los tres enfoques. ¿Qué puedes decir sobre ellas?
