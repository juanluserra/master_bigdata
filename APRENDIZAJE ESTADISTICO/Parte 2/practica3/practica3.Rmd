---
title: "Máquinas de vectores soporte"
author: "Aprendizaje Estadístico, Máster en Tecnologías de Análisis de Datos Masivos: Big Data. Profesor: Juan A. Botía (juanbot@um.es)"
date: "`r Sys.Date()`"
output:
  html_document:
    df_print: paged
    highlight: kate
    number_sections: yes
    theme: spacelab
    toc: yes
    toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introducción

En este guión de prácticas para SVM vamos a basarnos en el paquete `R` [e1071](https://cran.r-project.org/web/packages/e1071/index.html) que incluye métodos para desarrollo de clasificadores y algoritmos de regresión de varios tipos. Esta librería es muy popular y sigue estando mantenida perfectamente después de muchos años. La última actualización a noviembre de 2024 es de septiembre de este año.

## Clasificadores de vectores soporte

La librería e1071 contiene implementaciones para una variedad de métodos de aprendizaje estadístico. En particular, la función `svm()` puede usarse para 

* ajustar un clasificador de vectores soporte cuando se utiliza el argumento `kernel = "linear"`.

* ajustar una máquina de vectores soporte cuando es `kernel="polinomial"`  bien `kernel="radial"`

Además, tenemos el hiperparámetro `cost` (de alguna forma funciona al contrario de `C` en teoría), y **nos permite especificar lo que nos cuesta una restricción del problema de optimizaciuón.**

Recordemos que el incumplimiento puede ser de dos tipos:

* Un punto de una clase puede estar en el lado correcto de la frontera de decisión, pero entre la frontera y el margen

* El punto puede estar en el lado incorrecto y será clasificado incorrectamente.

Cuando el valor del parámetro `cost` es 

* pequeño, esto es equivalente a un valor de `C` alto. Por lo tanto, la distancia entre la frontera de decisión y los márgenes puede ser amplia.  y muchos vectores de soporte estarán en el margen o lo violarán.

* grande, los márgenes serán estrechos y habrá pocos vectores de soporte en el margen o violándolo.

Ahora usamos la función `svm()` para ajustar el clasificador de soporte vectorial con un valor dado para el parámetro de costo. Aquí demostramos el uso de esta función en un ejemplo bidimensional para que podamos trazar la frontera de decisión resultante. Comenzamos generando las observaciones, que pertenecen a dos clases, y verificamos si las clases son linealmente separables.



```{r}
set.seed(12345)
# Get a matrix with two columns and 20 examples
x <- matrix(rnorm(20 * 2), ncol = 2)
# Set -1 label for the 1st 10, +1 label for the 2nd 10
y <- c(rep(-1, 10), rep(1, 10))
# Separate the examples so there are actually two classes
# With small amount of errors (separate points by 1)
x[y == 1, ] <- x[y == 1, ] + 1
# Plot 2n column at x, 1st column at y (as the svm.plot method)
plot(x[, c(2, 1)], col = (3 - y), xlab = "x1 variable", ylab = "x2 variable")
```

Por supuesto que no lo son. ¿Por qué lo habrían de ser?

A continuación, ajustamos el clasificador de vectores soporte. Note que para que la función `svm()` clasifique, la variable dependiente debe ser de tipo factor. 

Creamos ahora el data frame correspondiente

```{r}
dat <- data.frame(x = x, y = as.factor(y))
library(e1071)
svmfit <- svm(y ~ ., data = dat, kernel = "linear", cost = 10, scale = FALSE)
```

El argumento `scale = FALSE` indica a la función `svm()` que no escale cada característica para que tenga media cero o desviación estándar uno; dependiendo de la aplicación, se podría preferir usar `scale = TRUE`.

Ahora podemos plotear la superficie de decisión del clasificador de vectores soporte correspondiente

```{r}
plot(svmfit, dat)
```

Los argumentos a la función `plot()` son la salida de `svm()` y los datos usados en la misma llamada

* La región del espacio de características asignada a la clase -1 se muestra en amarillo claro, y la región asignada a la clase +1 se muestra en rojo.

* La frontera de decisión entre las dos clases es lineal (porque usamos el argumento `kernel = "linear"`), aunque debido a la implementación de la función de trazado en esta librería, la frontera de decisión parece algo irregular en el gráfico.

* Aquí, la segunda característica se traza en el eje `x` y la primera característica en el eje `y`, lo cual contrasta con el comportamiento habitual de la función `plot()` en R.

* Los vectores de soporte se representan como cruces y las observaciones restantes como círculos. Vemos que hay siete vectores soporte. 


Podemos identificar cuáles son de la siguiente manera:


```{r}
svmfit$index
```

Podemos obtener información básica sobre el ajuste del clasificador de vectores soporte usando el comando `summary()`:

```{r}
summary(svmfit)
```

Esto nos indica, por ejemplo, que:

* Se utilizó un kernel lineal con costo = 10, y que

* Hubo siete vectores de soporte, cuatro en una clase y tres en la otra.

¿Qué sucede si en su lugar utilizamos un valor menor para el parámetro de costo?


```{r}
svmfit <- svm(y ~ ., data = dat, kernel = "linear", cost = 0.1, scale = FALSE)
plot(svmfit, dat)
```

Ahora que usamos un valor más pequeño para el parámetro de costo, obtenemos un mayor número de vectores de soporte, ya que el margen es más amplio.

```{r}
summary(svmfit)
```

Desafortunadamente, la función `svm()` no devuelve explícitamente los coeficientes de la frontera de decisión lineal obtenida al ajustar el clasificador de soporte vectorial, ni tampoco el ancho del margen.

La librería `e1071` incluye una función incorporada, `tune()`, para realizar validación cruzada. Por defecto, `tune()` realiza una validación cruzada de diez particiones (ten-fold cross-validation) en un conjunto de modelos de interés. Para usar esta función, proporcionamos información relevante sobre los modelos bajo consideración.

El siguiente comando indica que queremos comparar SVMs con un kernel lineal, utilizando un rango de valores del parámetro de costo:


```{r}
set.seed(1)
tune.out <- tune(svm, y ~ .,
    data = dat, kernel = "linear",
    ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100))
)
```

Podemos acceder fácilmente a los errores de validación cruzada para cada uno de estos modelos usando el comando `summary()`:

```{r}
summary(tune.out)
```

Vemos que cost = 0.1 (pero también 1 y 5) resulta en la menor tasa de error de validación cruzada. La función `tune()` almacena el mejor modelo obtenido, que puede ser accedido de la siguiente manera:

```{r}
bestmod <- tune.out$best.model
summary(bestmod)
```

La función `predict()` se puede usar para predecir la etiqueta de clase en un conjunto de observaciones de prueba, para cualquier valor dado del parámetro de costo. Comenzamos generando un conjunto de datos de prueba.


```{r}
xtest <- matrix(rnorm(20 * 2), ncol = 2)
ytest <- sample(c(-1, 1), 20, rep = TRUE)
xtest[ytest == 1, ] <- xtest[ytest == 1, ] + 1
testdat <- data.frame(x = xtest, y = as.factor(ytest))
```

Ahora predecimos las etiquetas de clase de estas observaciones de prueba. Aquí usamos el mejor modelo obtenido mediante validación cruzada para realizar las predicciones.

```{r}
ypred <- predict(bestmod, testdat)
table(predict = ypred, truth = testdat$y)
```

Por lo tanto, con este valor de costo, 17 de las observaciones de prueba se clasifican correctamente. ¿Qué sucede si en su lugar utilizamos cost = 0.01?

```{r}
svmfit <- svm(y ~ ., data = dat, kernel = "linear", cost = .01, scale = FALSE)
ypred <- predict(svmfit, testdat)
table(predict = ypred, truth = testdat$y)
```

En este caso, tres observaciones adicionales se clasifican incorrectamente.

Consideremos ahora una situación en la que las dos clases son linealmente separables. Entonces podemos encontrar un hiperplano separador usando la función svm(). Primero separamos más las dos clases en nuestros datos simulados para que sean linealmente separables:


```{r}
x[y == 1, ] <- x[y == 1, ] + 1
plot(x, col = (y + 5) / 2, pch = 19)
```

Ahora las observaciones son apenas linealmente separables. Ajustamos el clasificador de vectores soporte y trazamos el hiperplano resultante, usando un valor muy alto de costo para que no haya observaciones clasificadas incorrectamente.


```{r}
dat <- data.frame(x = x, y = as.factor(y))
svmfit <- svm(y ~ ., data = dat, kernel = "linear", cost = 1e5)
summary(svmfit)
plot(svmfit, dat)
```

No se cometieron errores de entrenamiento y solo se usaron tres vectores de soporte. Sin embargo, podemos ver en la figura que el margen es muy estrecho (porque las observaciones que no son vectores soporte, indicadas como círculos, están muy cerca de la frontera de decisión). Parece probable que este modelo tenga un rendimento bajo en datos de prueba. Ahora probamos con un valor menor de costo:


```{r}
svmfit <- svm(y ~ ., data = dat, kernel = "linear", cost = 1)
summary(svmfit)
plot(svmfit, dat)
```

Usando cost = 1, clasificamos incorrectamente una observación de entrenamiento, pero también obtenemos un margen mucho más amplio y utilizamos siete vectores de soporte. Parece probable que este modelo funcione mejor en datos de prueba que el modelo con cost = 1e5.


# Máquinas de vectores soporte

Para ajustar un SVM usando un kernel no lineal, usamos nuevamente la función `svm()`. Sin embargo, ahora usamos un valor diferente del parámetro kernel.

Para ajustar un SVM con un kernel polinomial usamos `kernel = "polynomial"`, y para ajustar un SVM con un kernel radial usamos `kernel = "radial"`. En el primer caso, también usamos el argumento degree para especificar un grado para el kernel polinomial, y en el segundo caso usamos gamma para especificar un valor de $\gamma$ para el kernel radial.

Primero generamos algunos datos con una frontera de clase no lineal, como sigue:

```{r}
set.seed(1)
# Generate 200 examples, two predictors
x <- matrix(rnorm(200 * 2), ncol = 2)
# Shift the 1st 100 to the right by adding 2 to
# the 1st 100 and to both columns
x[1:100, ] <- x[1:100, ] + 2
# Shift from 101 to 150 to the left doing -2
x[101:150, ] <- x[101:150, ] - 2
# Leave untouched from 151 to 200
y <- c(rep(1, 150), rep(2, 50))
dat <- data.frame(x = x, y = as.factor(y))
```

Graficando los datos queda claro que la frontera de clase es no lineal:

```{r}
plot(x[, c(2, 1)], col = y)
```

Los datos se dividen aleatoriamente en grupos de entrenamiento y prueba. Luego ajustamos los datos de entrenamiento usando la función svm() con un kernel radial y $\gamma = 1`:

```{r}
train <- sample(200, 100)
svmfit <- svm(y ~ ., data = dat[train, ], kernel = "radial", gamma = 1, cost = 1)
plot(svmfit, dat[train, ])
```

El gráfico muestra que el SVM resultante tiene una frontera decididamente no lineal. La función `summary()` puede usarse para obtener información sobre el ajuste del SVM:


```{r}
summary(svmfit)
```

Podemos ver en la figura que hay bastantes errores de entrenamiento en este ajuste del SVM. Si aumentamos el valor de costo, podemos reducir el número de errores de entrenamiento. Sin embargo, esto tiene el precio de una frontera de decisión más irregular. Lo cual podría implicar un riesgo más algo de caer en sobreajuste.


```{r}
svmfit <- svm(y ~ ., data = dat[train, ], kernel = "radial", gamma = 1, cost = 1e5)
plot(svmfit, dat[train, ])
```


Podemos realizar validación cruzada usando `tune()` para seleccionar la mejor combinación de $\gamma$ y costo para un SVM con un kernel radial:

```{r}
set.seed(1)
tune.out <- tune(svm, y ~ .,
    data = dat[train, ],
    kernel = "radial", ranges = list(
        cost = c(0.1, 1, 10, 100, 1000),
        gamma = c(0.5, 1, 2, 3, 4)
    )
)
summary(tune.out)
```

Por lo tanto, la mejor combinación de parámetros implica `cost = 1` y `gamma = 0.5`. 

Podemos observar las predicciones del conjunto de prueba para este modelo aplicando la función `predict()` a los datos. Note que para hacer esto utilizamos el subconjunto del marco de datos dat usando `-train` como índice.


```{r}
table(true = dat[-train, "y"], pred = predict(tune.out$best.model, newdata = dat[-train, ]))
```

El 12% de los ejemplos de test se clasifican de forma incorrecta.


# SVM con múltiples clases

Si la respuesta es un factor que contiene más de dos niveles, entonces la función `svm()` realizará una clasificación multiclase usando el enfoque uno contra uno (OVO). Exploramos este caso generando una tercera clase de observaciones:


```{r}
set.seed(1)
x <- rbind(x, matrix(rnorm(50 * 2), ncol = 2))
y <- c(y, rep(0, 50))
x[y == 0, 2] <- x[y == 0, 2] + 2
dat <- data.frame(x = x, y = as.factor(y))
par(mfrow = c(1, 1))
plot(x[, c(2, 1)], col = (y + 1))
```

Ahora ajustamos un SVM a los datos:

```{r}
svmfit <- svm(y ~ ., data = dat, kernel = "radial", cost = 10, gamma = 1)
plot(svmfit, dat)
```

# Una aplicación: datos de expresión génica

Examinamos el conjunto de datos Khan, que consiste en un número de muestras de tejido correspondientes a cuatro tipos distintos de tumores pequeños. Para cada muestra de tejido, se dispone de mediciones de expresión génica. El conjunto de datos consiste en datos de entrenamiento, `xtrain` y `ytrain`, y datos de prueba, `xtest` y `ytest`. Examinamos las dimensiones de los datos:

```{r}
library(ISLR2)
names(Khan)
dim(Khan$xtest)
length(Khan$ytrain)
length(Khan$ytest)
```

Este conjunto de datos consiste en mediciones de expresión para 2,308 genes. Los conjuntos de entrenamiento y prueba constan de 63 y 20 observaciones, respectivamente.


```{r}
table(Khan$ytrain)
table(Khan$ytest)
```

Usaremos un enfoque de soporte vectorial para predecir el subtipo de cáncer utilizando mediciones de expresión génica. En este conjunto de datos, hay un número muy grande de características en relación con el número de observaciones. Esto sugiere que deberíamos usar un kernel lineal, ya que la flexibilidad adicional que resultaría de usar un kernel polinomial o radial no es necesaria.


```{r}
dat <- data.frame(x = Khan$xtrain, y = as.factor(Khan$ytrain))
out <- svm(y ~ ., data = dat, kernel = "linear", cost = 10)
summary(out)
table(out$fitted, dat$y)
```

Vemos que no hay errores de entrenamiento. De hecho, esto no es sorprendente, porque el gran número de variables en relación con el número de observaciones implica que es fácil encontrar hiperplanos que separen completamente las clases. Nos interesa más el desempeño del clasificador de soporte vectorial en las observaciones de prueba que en las observaciones de entrenamiento.


```{r}
dat.te <- data.frame(x = Khan$xtest, y = as.factor(Khan$ytest))
pred.te <- predict(out, newdata = dat.te)
table(pred.te, dat.te$y)
```

Vemos que usando cost = 10 se obtienen dos errores en el conjunto de prueba con estos datos.