---
title: "Árboles de clasificación y regresión con Rpart"
author: "Aprendizaje Estadístico, Máster en Tecnologías de Análisis de Datos Masivos: Big Data. Profesor: Juan A. Botía (juanbot@um.es)"
date: "`r Sys.Date()`"
output:
  html_document:
    df_print: paged
    highlight: kate
    number_sections: yes
    theme: spacelab
    toc: yes
    toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introducción



El paquete R [Rpart](https://cran.r-project.org/web/packages/rpart/index.html), de *Recursive PARTitioning*, se utiliza para construir árboles de clasificación y regresión. Además utilizaremos como complemento el paquete `rpart.plot` para visualización de los árboles.

```{r}
library(rpart)
library(rpart.plot)
```

De especial interés para documentación son los ficheros

* [El manual de referencia](https://cran.r-project.org/web/packages/rpart/rpart.pdf) en donde están todas las llamadas y cuyo contenido también puedes obtener desde la consola de RStudio con `help(rpart)`

* [Introduction to Rpart](https://cran.r-project.org/web/packages/rpart/vignettes/longintro.pdf): un pdf con ejemplos de uso para regresión y clasificación y explicaciones sobre cómo funciona internamente y cómo se han de interpretar de manera adecuada los resultados.

* [El paquete rpart.plot](http://www.milbo.org/rpart-plot/index.html) para plotear los árboles de Rpart y este [tutorial](http://www.milbo.org/doc/prp.pdf) 


# Árboles de clasificación

Primero utilizamos árboles de clasificación para analizar el conjunto de datos `Carseats`. 
Utilizamos el paquete [summarytools](https://cran.r-project.org/web/packages/summarytools/vignettes/introduction.html) para visualizar de una forma visual, detallada y cómoda, el dataset.
```{r}
library(ISLR2)
library(summarytools)
attach(Carseats)
print(dfSummary(Carseats), method = "render")
```

Como vemos, hay 11 variables. Todas son numéricas excepto `Urban` y `US` de naturaleza booleana. Las variables numéricas se describen mediante sus `summary`, si son enteras se reporta el número de valores distintos, si son factores la distribución de valores, o bien histogramas para las continuas.

En estos datos, `Sales` es una variable continua, por lo que comenzamos recodificándola como una variable binaria. Usamos la función ifelse() para crear una variable llamada `High`, que toma el valor *Yes* si la variable `Sales` es mayor que 8, y toma el valor *No* en caso contrario. Luego tenemos columnas para % de valores válidos y missing. Ojo! Cuando el número de valores numéricos distintos es bajo, la variable puede ser en realidad un factor (ver `Education`).

```{r}
High <- factor(ifelse(Sales <= 8, "No", "Yes"))
table(High)
```

Usamos también la función `data.frame()` para combinar `High` con el resto de los datos de `Carseats`.

```{r}
Carseats <- data.frame(Carseats, High)
str(Carseats)
```

## Invocando a rpart

Ahora usamos la función `rpart()` para ajustar un árbol de clasificación para predecir High utilizando todas las variables excepto Sales. La sintaxis de la función `rpart()` es bastante similar a la de la función `lm()`. Es decir, definimos una fórmula para especificar la variable a predecir y las variables que actuarán de predictores (en este caso, todas menos `Sales`). 

```{r}
tree.carseats <- rpart(High ~ . - Sales, data = Carseats, method = "class", maxsurrogate = 0)
```

## Inspeccionando el resultado de la creación del árbol

La función `summary()` lista las variables que se utilizan como nodos internos en el árbol, el número de nodos terminales y la tasa de error (de entrenamiento).

```{r}
summary(tree.carseats)
```

Como vemos, `summary()` genera un montón de información que debemos ver por orden.

* Lo primero que aparece es la llamada que hemos hecho con los argumentos para sus parámetros correspondientes.

* Lo segundo es una lista de valores para el parámetro `cp` (costo-complejidad) que, como sabemos, es el parámetro central que modula cómo de importante es el error con respecto al tamaño del árbol. Cuando más pequeño, más aumenta el tamaño del árbol (`nsplit`) y más se reduce el error de validación cruzada (`xerror`). 

    + Vemos que cp empieza en 0.286 y termina en 0.1, y se prueban 8 valores distintos.
    
    + A medida que disminute cp, el número de splits aumenta, y el error relativo disminuye (es relativo ya que se expresa con respecto al split 0) en el que se parte de un error que tiene que ver con la distribución de clases original (en este caso, 0.59/0.41) y este sería el máximo. Por ejemplo, al probar el cp 0.036 (línea 4), se generan 4 splits. Pues el error relativo sería de $164\times 0.51/400=0.21$ y el de validación cruzada $164\times 0.67/400=0.27$.
    
* Luego tenemos una estimación de la importancia de las variables que tiene que ver con la frecuencia con que aparecen y cómo de arriba en el árbol. Como vemos, el precio es importante, pero también la localización  del producto.
    
* A continuación tenemos información extremadamente detallada sobre cada nodo. Debemos tener en cuenta que hay dos tipos de nodos, nodos internos y nodos hoja. 

     + Por ejemplo, el nodo 1 es el nodo raíz. ¿Qué nos dice este nodo? Si colapsáramos el nodo en hoja, su salida sería `No` (predicted class=No), ya que la clase mayoritaria es "No" con una probabilidad del 0.59. A la izquierda van 315 observaciones y el resto (85) a la derecha. ¿Cómo? Pues nos fijamos en "Primary splits" que nos dice todos los splits que se han probado y el primero es el que más ganancia tiene y, por tanto, el que se usa. En este caso, ha ganado "ShelveLoc", llevando dos valores a la izquierda y uno a la derecha. ¿Cómo descifrar LRL? Si usamos table como sigue, veremos que `Bad` y `Medium` van a la izquierda del nodo y `Good` va a la derecha. Como vemos, el nodo 1 genera dos nodos, un hijo `2` con 315 a la izquierda (que se pregunta por los valores `Bad` y `Medium` de `ShelveLoc`) y un hijo a la derecha `3` con 85 que se pregunta por el otro valor.

```{r}
table(Carseats$ShelveLoc)
```

* El recorrido del árbol es primero en anchura, de izquierda a derecha (no profundiza al siguiente nivel hasta haber descrito los del nivel actual), por lo que luego nos mostrará el nodo 2 y tras esto el 3. 

## El árbol en modo gráfico

Si lo ploteamos vamos a ver cómo es
     
```{r}
rpart.plot(tree.carseats)
```

Veremos que el orden del recorrido es el siguiente: (No,0.41,100), (No, 0.31,79), (Yes, 0.78,21), 4 (No, 0.25,67), 5 (Yes, 0.70,12), 6 (No, 0.25,3), 7 (Yes, 0.86,18), 8 (No, 0.18,56), 9 (Yes, 0.56,11), 10 (No, 0.30, 2), 11 (Yes, 0.81,9), 16 (No, 0.06,24), 17 (No, 0.27,32), 20 (No, 0.25, 5), 25 (Yes, 0.80,6), 34 (No, 0.19,27), 68 (No, 0.09, 16), 69 (No, 0.33,10), 138 (No, 0.09,6), 139 (Yes, 0.6, 5), 

* El primer nodo hoja que se genera es el 6, que aparece a como hijo del nodo más a la derecha en la visualización. Bajo él caen 12 observaciones, se predice "No" y el error estimado por ese nodo es del 25% (probabilidad de la clase minoritaria). P(node) es la probabilidad de que ese nodo se use para clasificar un ejemplo. En este caso, el nodo más importante es el 16 ya que casi un cuarto del dataset cae por la ruta que va desde la raíz a ese nodo.


## El árbol en modo texto


Finalmente, podemos imprimir el árbol en modo texto para centrarnos en él con todos los detalles.

```{r}
print(tree.carseats)
```


Esto es lo que nos cuenta el árbol línea a línea

* empezamos por la raíz, con 400 ejemplos, 164 de la clase Yes y siendo la clase mayoritaria No (59%)

* la siguiente línea está tabulada por lo que es un hijo de la anterior. En este caso el nodo raíz, con un split que, preguntando por los valores de `SelveLoc`, lleva todos los que tengan `Bad` y `Medium` a la izquierda (315 valores) y 98 a la derecha. En este nodo, `No` sigue siendo la etiqueta mayoritaria (68.8%) 

* El nodo 4 también cuelga del anterior y pregunta si el precio es igual o mayor que 92.5. Un total de 269 ejemplos van a la izquierda por cumplirlo y 66 a la derecha, sigue siendo `No` la mayoritaria con 75.4%.

* El nodo 8 cuelga del cuatro y en este se pregunta por el valor de `Adversiting`, con un 81.6% de los ejemplos cumpliendo el split ramificándose a la izquierda. Debajo de este nodo hay ramificación hoja para `CompPrice` y un hermano que a su vez se ramifica.

Y así sucesivamente.

## Evaluando el rendimiento

Para evaluar adecuadamente el rendimiento de un árbol de clasificación en estos datos, debemos estimar el error de prueba en lugar de simplemente calcular el error de entrenamiento.

* Dividimos las observaciones en un conjunto de entrenamiento y un conjunto de prueba, construimos el árbol utilizando el conjunto de entrenamiento y evaluamos su rendimiento en los datos de prueba.

* La función predict() se puede usar para este propósito.

* En el caso de un árbol de clasificación, el argumento type = "class" le indica a R que devuelva la predicción de clase real. Este enfoque lleva a predicciones correctas para alrededor del 77 % de las ubicaciones en el conjunto de datos de prueba.

```{r}
set.seed(2)
train <- sample(1:nrow(Carseats), 350)
Carseats.test <- Carseats[-train, ]
High.test <- High[-train]
tree.carseats <- rpart(High ~ . - Sales, Carseats[train, ],
  method = "class",
  control = rpart.control(xval = 10, minbucket = 2, cp = 0)
)
printcp(tree.carseats)
```


Como vemos, hay una lista de posibles árboles, uno para cada valor de CP. La columna xerror se refiere al error de validación cruzada, y parece que obtendremos el mejor resultado con xerror=0.68 y cp=0.021. ¿Por qué?

* Nota que los errores se expresan en términos relativos con respecto a la primera fila.

* Las columnas de error se han escalado de modo que el primer nodo tenga un error de 1. Dado que en este ejemplo el modelo sin divisiones debe hacer 142/350 clasificaciones incorrectas, multiplica las columnas 3-5 por 142 para obtener un resultado en términos de error absoluto. (Los cálculos se realizan en la escala de error absoluto y se imprimen en la escala relativa).
    
    
```{r}
fit7 <- prune(tree.carseats, cp = 0.021)
rpart.plot(fit7)
tree.pred <- predict(fit7, Carseats.test, type = "class")
table(tree.pred, High.test)
print(paste0("The hit rate of this tree on test is ", MLmetrics::Accuracy(tree.pred, High.test)))
print(paste0("The estimated crossvalidation error of this tree is ", 0.654 * 142 / 350))
```
 

El número 7 en fit7 se refiere al número de divisiones.

# Árboles de regresión con Rpart


Aquí ajustamos un árbol de regresión al conjunto de datos [Boston](https://www.kaggle.com/code/prasadperera/the-boston-housing-dataset). 
El conjunto de datos de vivienda de Boston está derivado de la información recopilada por el Censo de los EE. UU. en relación con la vivienda en el área de Boston, MA. A continuación se describen las columnas del conjunto de datos:

* CRIM - tasa de criminalidad per cápita por ciudad

* ZN - proporción de terreno residencial zonificado para lotes de más de 25,000 pies cuadrados

* INDUS - proporción de acres comerciales no minoristas por ciudad

* CHAS - variable dummy del río Charles (1 si el sector limita con el río; 0 en caso contrario)

* NOX - concentración de óxido de nitrógeno (partes por cada 10 millones)

* RM - número promedio de habitaciones por vivienda

* AGE - proporción de unidades ocupadas por sus propietarios construidas antes de 1940

* DIS - distancias ponderadas a cinco centros de empleo de Boston

* RAD - índice de accesibilidad a autopistas radiales

* TAX - impuesto de bienes inmuebles por cada $10,000
  
* PTRATIO - tasa de alumnos por profesor en la zona

* LSTAT - % de la población de nivel socioeconómico bajo
  
* MEDV - valor mediano de las viviendas ocupadas por sus propietarios en miles de dólares

Vamos primero a visualizarlo como se merece para tener una visual rápida del conjunto de datos.

```{r}
data(Boston)
help(Boston)
print(dfSummary(Boston), method = "render")
```

Como vemos, es un conjunto de datos de 506 ejemplos y 13 predictores. Uno de ellos, `medv` es el que queremos predecir. A simple vista vemos que no tiene valores nulos y que tiene atributos de todo tipo, incluyendo numéricos, entero (como `rad`) y booleanos como `chas`.  Podríamos plantearnos si realmente `rad` debería ser un factor. Sin embargo, si consultamos la ayuda veremos que nos da una idea de lo accesible que están las autovías 

Primero, creamos un conjunto de entrenamiento y ajustamos el árbol a los datos de entrenamiento.

```{r}
set.seed(1)
train <- sample(1:nrow(Boston), nrow(Boston) / 2)
tree.boston <- rpart(medv ~ ., data = Boston, subset = train, maxsurrogate = 0)
summary(tree.boston)
```

* Como vemos, a partir de probar se genera un árbol con 7 splits.

* Se han reportado 7 valores posibles de cp desde 0.55 a 0.01, hasta llegar a un valor de validación cruzada mínimo de 0.21 * 76.86.

* En el nodo 1, el valor predicho para el valor medio de la propiedad es 21.78 para la variable `medv`, usando 253 valores. De ahí, se ramifica usando `rm` con el split 6.95. 

* Una vez más tenemos estimaciones para las variables más importantes. Vemos que rm, el número de habitaciones por hogar, es la variable más determinante para el precio.

Si ploteamos el árbol correspondiente tendremos 

```{r}
rpart.plot(tree.boston)
```

De acuerdo con los resultados de la validación cruzada, podemos bien usar el árbol sin podar para hacer predicciones en el conjunto de prueba.

```{r}
yhat <- predict(tree.boston, newdata = Boston[-train, ])
boston.test <- Boston[-train, "medv"]
plot(yhat, boston.test)
abline(0, 1)
mean((yhat - boston.test)^2)
```

En otras palabras, el MSE del conjunto de prueba asociado con el árbol de regresión es de 35.29. La raíz cuadrada del MSE es entonces aproximadamente 5.941, lo que indica que este modelo conduce a predicciones de prueba que están (en promedio) a unos 5,941 dólares del valor medio real de la vivienda para el sector censal.

O bien podemos también decidir que sacrificamos un poco de error por usar un árbol más simple y quedarnos con 4 splits


```{r}
fit4 <- prune(tree.boston, cp = 0.032)
rpart.plot(fit4)
yhat <- predict(fit4, newdata = Boston[-train, ])
boston.test <- Boston[-train, "medv"]
plot(yhat, boston.test)
abline(0, 1)
mean((yhat - boston.test)^2)
```

E incluso obtenemos un error ligeramente mejor. 


# Ejercicios

## Ejercicio 1

Dibuja un ejemplo (de tu propia invención) de una partición del espacio de características bidimensional que podría resultar de una división binaria recursiva. Tu ejemplo debe contener al menos seis regiones. Dibuja un árbol de decisión correspondiente a esta partición. Asegúrate de etiquetar todos los aspectos de tus figuras, incluidas las regiones $R_1,R_2,\ldots,$ los puntos de corte $t_1,t_2,\ldots$ y así sucesivamente.


## Ejercicio 2


Considera el índice de Gini, el error de clasificación y la entropía en un problema de clasificación binario. Crea un solo gráfico que muestre cada una de estas cantidades como una función de $\hat{p}_{m1}$. El eje x debe mostrar $\hat{p}_{m1}$, desde 0 a 1, y el eje y debe mostrar el índice de Gini, la entropía y el error de clasificación. Nota que $\hat{p}_{m1}=1−\hat{p}_{m2}$ en un problema de clase binaria.

## Ejercicio 3

Este problema implica el conjunto de datos OJ, que forma parte del paquete ISLR2.

```{r}
library(gtsummary)
data(OJ)
print(dfSummary(OJ), method = "render")
```


Crea un conjunto de entrenamiento que contenga una muestra aleatoria de 800 observaciones y un conjunto de prueba con las observaciones restantes.

b. Ajusta un árbol rpart a los datos de entrenamiento, con Purchase como la respuesta y las otras variables como predictores. Usa la función summary() para producir estadísticas de resumen sobre el árbol y describe los resultados obtenidos. ¿Cuál sería el mejor árbol? ¿Cuántos nodos terminales tiene el árbol?
```{r}
train_index <- sample(1:nrow(OJ), 800)
OJ_train <- OJ[train_indices, ]
OJ_test <- OJ[-train_indices, ]

tree.oj <- rpart(Purchase ~ ., data = OJ_train, method = "class")
summary(tree.oj)
```

c. Escribe el nombre del objeto árbol para obtener una salida detallada en texto. Elige uno de los nodos terminales e interpreta la información mostrada.
```{r}

```

d. Crea un gráfico del árbol e interpreta los resultados.

e. Predice la respuesta en los datos de prueba y produce una matriz de confusión comparando las etiquetas de prueba con las etiquetas de prueba predichas. ¿Cuál es la tasa de error de prueba?

f. Aplica la función printcp() al conjunto de entrenamiento para determinar el tamaño óptimo del árbol.

g. Produce un gráfico con el tamaño del árbol en el eje x y la tasa de error de clasificación validada cruzada en el eje y.

h. ¿Qué tamaño de árbol corresponde a la tasa de error de clasificación validada cruzada más baja?

i. Produce un árbol podado correspondiente al tamaño óptimo del árbol obtenido utilizando validación cruzada. Si la validación cruzada no lleva a la selección de un árbol podado, entonces crea un árbol podado con cinco nodos terminales.

j. Compara las tasas de error de entrenamiento entre los árboles podados y no podados. ¿Cuál es mayor?

k. Compara las tasas de error de prueba entre los árboles podados y no podados. ¿Cuál es mayor?