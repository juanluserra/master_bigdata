{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Práctica 4: Programación en Apache Spark\n",
    "\n",
    "Se propone la realización de 4 scripts. Los scripts deben realizarse usando Notebooks o bien como scripts que se puedan enviar con `spark-submit`.\n",
    "\n",
    "### Normas:\n",
    "- Los scripts deben incluir comentarios que expliquen los pasos realizados.\n",
    "- La salida de los scripts debe seguir el formato indicado en cada uno de los ejercicios (incluyendo el nombre y orden de las columnas).\n",
    "- Se debe entregar un fichero comprimido con los scripts de la práctica debidamente comentados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 1\n",
    "\n",
    "Extraer información de los ficheros `cite75_99.txt` y `apat63_99.txt`. Crear un script que haga lo siguiente:\n",
    "\n",
    " a. A partir del fichero `cite75_99.txt` obtener el número de citas que ha recibido cada patente. Debes obtener un DataFrame de la siguiente forma en el fichero `dfCitas.parquet`:\n",
    "\n",
    "|NPatente|ncitas|\n",
    "|-------:|-----:|\n",
    "|4943137 |1     |\n",
    "|2959285 |1     |\n",
    "|3004604 |1     |\n",
    "|5060509 |1     |\n",
    "|5708825 |1     |\n",
    "|4549461 |1     |\n",
    "|4756599 |1     |\n",
    "\n",
    "\n",
    "b. A partir del fichero `apat63_99.txt`, crear un DataFrame que contenga el número de patente, el país y el año de concesión (columna `GYEAR`), descartando el resto de campos del fichero. Ese DataFrame debe tener la siguiente forma, y estar en el fichero `dfInfo.parquet`:\n",
    "\n",
    "|NPatente|País|Año |\n",
    "|-------:|---:|---:|\n",
    "|4101646 |JP  |1978|\n",
    "|4186332 |US  |1980|\n",
    "|4920512 |JP  |1990|\n",
    "|3512825 |ET  |1970|\n",
    "|3797992 |US  |1974|\n",
    "|5394547 |US  |1995|\n",
    "|4299230 |JP  |1981|\n",
    "|4348596 |US  |1982|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requisitos\n",
    "\n",
    "- Ambos DataFrames se debe salvar en formato Parquet con compresión gzip. Comprueba el número de particiones de cada DataFrame y el número de ficheros generados.\n",
    "- El script debe aceptar argumentos en línea de comandos, es decir, para su ejecución se debe poder indicar la ruta a los ficheros de entrada y el nombre de los directorios de salida. Por ejemplo, para la ejecución en local:\n",
    "\n",
    "```bash\n",
    "spark-submit --master 'local[*]' --num-executors 4 --driver-memory 4g p1.py path_a_cite75_99.txt path_a_apat63_99.txt dfCitas.parquet dfInfo.parquet\n",
    "```\n",
    "\n",
    "### Ejemplo:\n",
    "\n",
    "```python\n",
    "#! /usr/bin/env python3\n",
    "from pyspark.sql import SparkSession\n",
    "import sys\n",
    "#\n",
    "# Script para extraer información de los ficheros cite75_99.txt y apat63_99.txt. \n",
    "# a) A partir del fichero cite75_99.txt obtener el número de citas de cada patente. \n",
    "#    Debes obtener un DataFrame de la siguiente forma:\n",
    "#   +--------+------+ \n",
    "#   |NPatente|ncitas|\n",
    "#   +--------+------+\n",
    "#   | 3060453|  3   |\n",
    "#   | 3390168|  6   |\n",
    "#   | 3626542| 18   |\n",
    "#   | 3611507|  5   |\n",
    "#   | 3000113|  4   |\n",
    "#\n",
    "# b) A partir del fichero apat63_99.txt, crear un DataFrame que contenga el número de patente, \n",
    "# el país y el año, descartando el resto de campos del fichero.\n",
    "# Ese DataFrame debe tener la siguiente forma:\n",
    "#\n",
    "#   +--------+----+----+ \n",
    "#   |NPatente|País|Año |\n",
    "#   +--------+----+----+\n",
    "#   | 3070801| BE| 1963|\n",
    "#   | 3070802| US| 1963|\n",
    "#   | 3070803| US| 1963|\n",
    "#   | 3070804| US| 1963|\n",
    "#   | 3070805| US| 1963|\n",
    "#\n",
    "# Ejecutar en local con:\n",
    "# spark-submit --master 'local[*]' --driver-memory 4g p1.py path_a_cite75_99.txt path_a_apat63_99.txt dfCitas.parquet dfInfo.parquet\n",
    "# Ejecución en un cluster YARN:\n",
    "# spark-submit --master yarn --num-executors 8 --driver-memory 4g p1.py path_a_cite75_99.txt_en_HDFS path_a_apat63_99.txt_en_HDFS dfCitas.parquet dfInfo.parquet\n",
    "\n",
    "def main():\n",
    "    # Comprueba el número de argumentos\n",
    "    # sys.argv[1] es el primer argumento, sys.argv[2] el segundo, etc.\n",
    "    if len(sys.argv) != 5:\n",
    "        print(f\"Uso: {sys.argv[0]} cite75_99.txt apat63_99.txt dfCitas.parquet dfInfo.parquet\")\n",
    "        exit(-1)\n",
    "\n",
    "    spark: SparkSession = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"Practica 1 de Tomás\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # Cambio la verbosidad para reducir el número de\n",
    "    # mensajes por pantalla\n",
    "    spark.sparkContext.setLogLevel(\"FATAL\")\n",
    "    # Código del programa\n",
    "    ...\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "```\n",
    "\n",
    "**Nota:** Para hacer pruebas más rápidamente podéis hacer un sampleo de los ficheros grandes y trabajar con una versión más reducida."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ayuda en la realización del ejercicio:\n",
    "\n",
    "- Comprueba que tienes acceso a los ficheros\n",
    "- Cargamos datos en dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "format": "text/plain",
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "En el shell de Unix (aquí se muestra para `patentes-mini`):\n",
    "\n",
    "```bash\n",
    "# Descarga de los ficheros\n",
    "wget -qq https://github.com/dsevilla/tcdm-public/raw/refs/heads/24-25/datos/patentes-mini.tar.gz\n",
    "\n",
    "# Descomprimimos\n",
    "tar xzf patentes-mini.tar.gz\n",
    "\n",
    "# Listamos ficheros\n",
    "ls -lh patentes-mini/cite75_99.txt\n",
    "head patentes-mini/cite75_99.txt\n",
    "ls -lh patentes-mini/apat63_99.txt\n",
    "head patentes-mini/apat63_99.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el shell de Unix (aquí se muestra para `patentes`):\n",
    "\n",
    "```bash\n",
    "# Descarga de los ficheros\n",
    "wget -qq https://github.com/dsevilla/tcdm-public/raw/refs/heads/24-25/datos/patentes.7z\n",
    "\n",
    "# Descomprimimos\n",
    "7zr x patentes.7z\n",
    "\n",
    "# Listamos ficheros\n",
    "ls -lh patentes/cite75_99.txt\n",
    "head patentes/cite75_99.txt\n",
    "ls -lh patentes/apat63_99.txt\n",
    "head patentes/apat63_99.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para cargar los datos de los ficheros `cite75_99.txt` y `apat63_99.txt` podéis usar el siguiente código:\n",
    "\n",
    "```python\n",
    "def load_data(spark: SparkSession, path_cite: str, path_apat: str) -> tuple[DataFrame, DataFrame]:\n",
    "    cites: DataFrame = (spark\n",
    "        .read\n",
    "        .option(\"inferSchema\", \"true\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .csv(path_cite))\n",
    "    cites.printSchema()\n",
    "    cites.show()\n",
    "    print(cites.count())\n",
    "    apat: DataFrame = (spark\n",
    "        .read\n",
    "        .option(\"inferSchema\", \"true\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .csv(path_apat))\n",
    "    apat.printSchema()\n",
    "    apat.show()\n",
    "    print(apat.count())\n",
    "    return cites, apat\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 2\n",
    "\n",
    "Script que, a partir de los datos en Parquet de la práctica anterior, obtenga para cada país y para cada año el total de patentes, el total de citas obtenidas por todas las patentes, la media de citas y el máximo número de citas.\n",
    "- Obtener solo aquellos casos en los que existan valores en ambos ficheros (inner join).\n",
    "- Cada país tiene que aparecer con su nombre completo, obtenido del fichero `country_codes.txt`, residente en el disco local\n",
    "- El DataFrame generado debe estar ordenado por el máximo número de citas, país y año.\n",
    "\n",
    "Ejemplo de salida:\n",
    "\n",
    "\n",
    "|País         |Año |NumPatentes|TotalCitas|MediaCitas        |MaxCitas|\n",
    "|:------------|---:|----------:|---------:|-----------------:|-------:|\n",
    "|Japan        |1975|15         |17        |1.1333333333333333|3       |\n",
    "|United States|1982|129        |131       |1.0155038759689923|3       |\n",
    "|Germany      |1993|10         |11        |1.1               |2       |\n",
    "|Hungary      |1970|2          |3         |1.5               |2       |\n",
    "|Japan        |1983|29         |30        |1.0344827586206897|2       |\n",
    "|Japan        |1984|44         |45        |1.0227272727272727|2       |\n",
    "|Japan        |1986|51         |52        |1.0196078431372548|2       |\n",
    "|Sweden       |1972|2          |3         |1.5               |2       |\n",
    "|United States|1963|58         |59        |1.0172413793103448|2       |\n",
    "|United States|1965|79         |80        |1.0126582278481013|2       |\n",
    "|United States|1966|94         |95        |1.0106382978723405|2       |\n",
    "|United States|1967|122        |123       |1.0081967213114753|2       |\n",
    "|United States|1973|142        |144       |1.0140845070422535|2       |\n",
    "|United States|1974|168        |169       |1.005952380952381 |2       |\n",
    "\n",
    "\n",
    "### Requisitos\n",
    "- El DataFrame obtenido se debe guardar en un único fichero CSV sin comprimir y con cabecera.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ayuda en la realización del ejercicio:**\n",
    "\n",
    "- Lectura de fichero parquet.\n",
    "- Cargamos el fichero con los códigos del país en un diccionario.\n",
    "- Guardar un DataFrame en un único fichero CSV sin comprimir y con cabecera."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autoscroll": "auto"
   },
   "source": [
    "```python\n",
    "from pyspark import Broadcast\n",
    "# Leo el fichero de citas\n",
    "dfCitas: DataFrame = (spark.read.format(\"parquet\")\n",
    "               .option(\"mode\", \"FAILFAST\")\n",
    "               .load(\"patentes-mini/dfCitas.parquet\"))\n",
    "\n",
    "# Cargamos el fichero con los códigos del pais\n",
    "ccDict = dict()\n",
    "with open(\"patentes-mini/country_codes.txt\") as ccfile:\n",
    "    for row in ccfile.readlines():\n",
    "        code, country = row.split('\\t')\n",
    "        ccDict[code] = country.strip()\n",
    "bcastCCDict: Broadcast[dict[str,str]] = spark.sparkContext.broadcast(ccDict)\n",
    "\n",
    "# Lo guardamos como un único fichero CSV\n",
    "(dfCitas.coalesce(1)\n",
    "       .write.format(\"csv\")\n",
    "       .mode(\"overwrite\")\n",
    "       .option(\"header\", True)\n",
    "       .save(\"patentes-mini/p2\"))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 3\n",
    "\n",
    "Obtener a partir de los ficheros Parquet creados en el ejercicio 1 un DataFrame que proporcione, para un grupo de países especificado, las patentes ordenadas por número de citas, de mayor a menor, junto con una columna que indique el ango (posición de la patente en esa país/año según las citas obtenidas):\n",
    "\n",
    "La salida del script debe ser como sigue:\n",
    "\n",
    "|País|Año |Npatente|Ncitas|Rango|\n",
    "|:---|---:|-------:|-----:|----:|\n",
    "|ES  |1963|3093080 |20    |1    |\n",
    "|ES  |1963|3099309 |19    |2    |\n",
    "|ES  |1963|3081560 |9     |3    |\n",
    "|ES  |1963|3071439 |9     |3    |\n",
    "|ES  |1963|3074559 |6     |4    |\n",
    "|ES  |1963|3114233 |5     |5    |\n",
    "|ES  |1963|3094845 |4     |6    |\n",
    "|ES  |1963|3106762 |3     |7    |\n",
    "|ES  |1963|3088009 |3     |7    |\n",
    "|ES  |1963|3087842 |2     |8    |\n",
    "|ES  |1963|3078145 |2     |8    |\n",
    "|ES  |1963|3094806 |2     |8    |\n",
    "|ES  |1963|3073124 |2     |8    |\n",
    "|ES  |1963|3112201 |2     |8    |\n",
    "|ES  |1963|3102971 |1     |9    |\n",
    "|ES  |1963|3112703 |1     |9    |\n",
    "|ES  |1963|3095297 |1     |9    |\n",
    "|ES  |1964|3129307 |11    |1    |\n",
    "|ES  |1964|3133001 |10    |2    |\n",
    "|ES  |1964|3161239 |8     |3    |\n",
    "|... |... |...     |...   |...|\n",
    "|FR  |1963|3111006 |35    |1    |\n",
    "|FR  |1963|3083101 |22    |2    |\n",
    "|FR  |1963|3077496 |16    |3    |\n",
    "|FR  |1963|3072512 |15    |4    |\n",
    "|FR  |1963|3090203 |15    |4    |\n",
    "|FR  |1963|3086777 |14    |5    |\n",
    "|FR  |1963|3074344 |13    |6    |\n",
    "|FR  |1963|3096621 |13    |6    |\n",
    "|FR  |1963|3089153 |13    |6    |\n",
    "|... |... |...     |...   |...|\n",
    "\n",
    "\n",
    "### Requisitos\n",
    "* El DataFrame debe de estar ordenado por código del país y año (ascendente) y número de citas (descendente).\n",
    "* Utilizad funciones de ventana para obtener el rango.\n",
    "* NO reemplazar el código del país por su nombre completo.\n",
    "* La salida debe guardarse en un único fichero CSV sin comprimir y con cabecera.\n",
    "* Como en los casos anteriores, el script debe aceptar argumentos en línea de comandos, es decir, para su ejecución deberíamos poder indicar la ruta a los directorios de entrada creados en la práctica 1, la lista de países a analizar (separados por coma) y el nombre del directorio de salida."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 4\n",
    "\n",
    "Obtener a partir del fichero Parquet con la información de (Npatente, País y Año) un DataFrame que nos muestre el número de patentes asociadas a cada país por cada década (entendemos por década los años del 0 al 9, es decir de 1970 a 1979 es la década de los 70s). Adicionalmente, debe mostrar el aumento o disminución del número de patentes para cada país y década con respecto al la década anterior.\n",
    "\n",
    "El DataFrame generado tiene que ser como este:\n",
    "\n",
    "|País|Década|NPatentes|Dif |\n",
    "|:---|-----:|--------:|---:|\n",
    "|AD  |1980  |1        |0   |\n",
    "|AD  |1990  |5        |4   |\n",
    "|AE  |1980  |7        |0   |\n",
    "|AE  |1990  |11       |4   |\n",
    "|AG  |1970  |2        |0   |\n",
    "|AG  |1990  |7        |5   |\n",
    "|AI  |1990  |1        |0   |\n",
    "|AL  |1990  |1        |0   |\n",
    "|AM  |1990  |2        |0   |\n",
    "|AN  |1970  |1        |0   |\n",
    "|AN  |1980  |2        |1   |\n",
    "|AN  |1990  |5        |3   |\n",
    "|AR  |1960  |135      |0   |\n",
    "|AR  |1970  |239      |104 |\n",
    "|AR  |1980  |184      |-55 |\n",
    "|AR  |1990  |292      |108 |\n",
    "|... |...   |...      |...|\n",
    "\n",
    "### Requisitos\n",
    "* El DataFrame debe de estar ordenado por código del país y año.\n",
    "* NO reemplazar el código del país por su nombre completo.\n",
    "* La salida debe guardarse en un único fichero CSV sin comprimir y con cabecera.\n",
    "* Como en los casos anteriores, el script debe aceptar argumentos en línea de comandos, es decir, para su ejecución deberíamos poder indicar la ruta al directorio de entrada creado en la práctica 1 y el nombre del directorio de salida."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": "text/python",
   "file_extension": ".py",
   "mimetype": "text/python",
   "name": "python",
   "pygments_lexer": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
