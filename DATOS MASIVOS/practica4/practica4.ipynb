{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo primero es exportar las variables de entorno necesarias para poder trabajar con el cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Establecer las variables de entorno\n",
    "os.environ['PATH'] = os.path.expanduser('~/.local/bin') + os.pathsep + os.environ['PATH']\n",
    "os.environ['HADOOP_CONF_DIR'] = os.path.join(os.environ['HADOOP_HOME'], 'etc', 'hadoop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 1\n",
    "\n",
    "Extraer información de los ficheros `cite75_99.txt` y `apat63_99.txt`. Crear un script que haga lo siguiente:\n",
    "\n",
    " a. A partir del fichero `cite75_99.txt` obtener el número de citas que ha recibido cada patente. Debes obtener un DataFrame de la siguiente forma en el fichero `dfCitas.parquet`:\n",
    "\n",
    "|NPatente|ncitas|\n",
    "|-------:|-----:|\n",
    "|4943137 |1     |\n",
    "|2959285 |1     |\n",
    "|3004604 |1     |\n",
    "|5060509 |1     |\n",
    "|5708825 |1     |\n",
    "|4549461 |1     |\n",
    "|4756599 |1     |\n",
    "\n",
    "\n",
    "b. A partir del fichero `apat63_99.txt`, crear un DataFrame que contenga el número de patente, el país y el año de concesión (columna `GYEAR`), descartando el resto de campos del fichero. Ese DataFrame debe tener la siguiente forma, y estar en el fichero `dfInfo.parquet`:\n",
    "\n",
    "|NPatente|País|Año |\n",
    "|-------:|---:|---:|\n",
    "|4101646 |JP  |1978|\n",
    "|4186332 |US  |1980|\n",
    "|4920512 |JP  |1990|\n",
    "|3512825 |ET  |1970|\n",
    "|3797992 |US  |1974|\n",
    "|5394547 |US  |1995|\n",
    "|4299230 |JP  |1981|\n",
    "|4348596 |US  |1982|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prueba en local**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/01/03 11:10:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "+--------+------+                                                               \n",
      "|NPatente|ncitas|\n",
      "+--------+------+\n",
      "| 4943137|     1|\n",
      "| 2959285|     1|\n",
      "| 3004604|     1|\n",
      "| 5060509|     1|\n",
      "| 5708825|     1|\n",
      "| 4549461|     1|\n",
      "| 4756599|     1|\n",
      "| 4515376|     1|\n",
      "| 1144012|     1|\n",
      "| 3615674|     1|\n",
      "| 3475056|     1|\n",
      "| 3535500|     1|\n",
      "| 3997459|     1|\n",
      "| 4257490|     1|\n",
      "| 2252991|     1|\n",
      "| 2480679|     1|\n",
      "| 3515849|     1|\n",
      "| 2863595|     1|\n",
      "| 4090538|     1|\n",
      "| 2445046|     1|\n",
      "+--------+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------+-------+----+                                                         \n",
      "|NPatente|COUNTRY| Año|\n",
      "+--------+-------+----+\n",
      "| 4101646|     JP|1978|\n",
      "| 4186332|     US|1980|\n",
      "| 4920512|     JP|1990|\n",
      "| 3512825|     ET|1970|\n",
      "| 3797992|     US|1974|\n",
      "| 5394547|     US|1995|\n",
      "| 4299230|     JP|1981|\n",
      "| 4348596|     US|1982|\n",
      "| 4708861|     US|1987|\n",
      "| 4694033|     NL|1987|\n",
      "| 3087707|     US|1963|\n",
      "| 5114044|     US|1992|\n",
      "| 4775172|     DE|1988|\n",
      "| 5326076|     US|1994|\n",
      "| 4979948|     US|1990|\n",
      "| 4786608|     US|1988|\n",
      "| 4499670|     NO|1985|\n",
      "| 4393370|     JP|1983|\n",
      "| 4757365|     NL|1988|\n",
      "| 3404349|     US|1968|\n",
      "+--------+-------+----+\n",
      "only showing top 20 rows\n",
      "\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "!python3 ejercicio1.py patentes-mini/cite75_99.txt patentes-mini/apat63_99.txt dfCitas.parquet dfInfo.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "**Prueba en el cluster**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/01/03 11:12:41 INFO SparkContext: Running Spark version 3.5.3\n",
      "25/01/03 11:12:41 INFO SparkContext: OS info Linux, 5.15.153.1-microsoft-standard-WSL2, amd64\n",
      "25/01/03 11:12:41 INFO SparkContext: Java version 11.0.24\n",
      "25/01/03 11:12:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/01/03 11:12:42 INFO ResourceUtils: ==============================================================\n",
      "25/01/03 11:12:42 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "25/01/03 11:12:42 INFO ResourceUtils: ==============================================================\n",
      "25/01/03 11:12:42 INFO SparkContext: Submitted application: Ejercicio1\n",
      "25/01/03 11:12:42 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "25/01/03 11:12:42 INFO ResourceProfile: Limiting resource is cpus at 1 tasks per executor\n",
      "25/01/03 11:12:42 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "25/01/03 11:12:42 INFO SecurityManager: Changing view acls to: luser\n",
      "25/01/03 11:12:42 INFO SecurityManager: Changing modify acls to: luser\n",
      "25/01/03 11:12:42 INFO SecurityManager: Changing view acls groups to: \n",
      "25/01/03 11:12:42 INFO SecurityManager: Changing modify acls groups to: \n",
      "25/01/03 11:12:42 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: luser; groups with view permissions: EMPTY; users with modify permissions: luser; groups with modify permissions: EMPTY\n",
      "25/01/03 11:12:43 INFO Utils: Successfully started service 'sparkDriver' on port 44909.\n",
      "25/01/03 11:12:43 INFO SparkEnv: Registering MapOutputTracker\n",
      "25/01/03 11:12:43 INFO SparkEnv: Registering BlockManagerMaster\n",
      "25/01/03 11:12:43 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "25/01/03 11:12:43 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "25/01/03 11:12:43 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "25/01/03 11:12:43 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-4eac6755-9e14-41cb-9a98-1fe78370be83\n",
      "25/01/03 11:12:43 INFO MemoryStore: MemoryStore started with capacity 413.9 MiB\n",
      "25/01/03 11:12:43 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "25/01/03 11:12:43 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
      "25/01/03 11:12:44 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "25/01/03 11:12:44 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at resourcemanager/172.18.0.2:8032\n",
      "25/01/03 11:12:45 INFO AHSProxy: Connecting to Application History server at timelineserver/172.18.0.7:10200\n",
      "25/01/03 11:12:46 INFO Configuration: resource-types.xml not found\n",
      "25/01/03 11:12:46 INFO ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "25/01/03 11:12:46 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (4096 MB per container)\n",
      "25/01/03 11:12:46 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead\n",
      "25/01/03 11:12:46 INFO Client: Setting up container launch context for our AM\n",
      "25/01/03 11:12:46 INFO Client: Setting up the launch environment for our AM container\n",
      "25/01/03 11:12:46 INFO Client: Preparing resources for our AM container\n",
      "25/01/03 11:12:46 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "25/01/03 11:12:56 INFO Client: Uploading resource file:/tmp/spark-1e51ab6c-7313-491f-8123-96cd3f17fd0e/__spark_libs__6743444829542780711.zip -> hdfs://namenode:9000/user/luser/.sparkStaging/application_1735897749653_0001/__spark_libs__6743444829542780711.zip\n",
      "25/01/03 11:13:14 INFO Client: Uploading resource file:/home/luser/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip -> hdfs://namenode:9000/user/luser/.sparkStaging/application_1735897749653_0001/pyspark.zip\n",
      "25/01/03 11:13:15 INFO Client: Uploading resource file:/home/luser/.venv/lib/python3.10/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip -> hdfs://namenode:9000/user/luser/.sparkStaging/application_1735897749653_0001/py4j-0.10.9.7-src.zip\n",
      "25/01/03 11:13:16 INFO Client: Uploading resource file:/tmp/spark-1e51ab6c-7313-491f-8123-96cd3f17fd0e/__spark_conf__9977063894968468460.zip -> hdfs://namenode:9000/user/luser/.sparkStaging/application_1735897749653_0001/__spark_conf__.zip\n",
      "25/01/03 11:13:16 INFO SecurityManager: Changing view acls to: luser\n",
      "25/01/03 11:13:16 INFO SecurityManager: Changing modify acls to: luser\n",
      "25/01/03 11:13:16 INFO SecurityManager: Changing view acls groups to: \n",
      "25/01/03 11:13:16 INFO SecurityManager: Changing modify acls groups to: \n",
      "25/01/03 11:13:16 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: luser; groups with view permissions: EMPTY; users with modify permissions: luser; groups with modify permissions: EMPTY\n",
      "25/01/03 11:13:16 INFO Client: Submitting application application_1735897749653_0001 to ResourceManager\n",
      "25/01/03 11:13:18 INFO YarnClientImpl: Submitted application application_1735897749653_0001\n",
      "25/01/03 11:13:19 INFO Client: Application report for application_1735897749653_0001 (state: ACCEPTED)\n",
      "25/01/03 11:13:19 INFO Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: [vie. ene. 03 11:13:19 +0100 2025] Scheduler has assigned a container for AM, waiting for AM container to be launched\n",
      "\t ApplicationMaster host: N/A\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1735899197230\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://resourcemanager:8088/proxy/application_1735897749653_0001/\n",
      "\t user: luser\n",
      "25/01/03 11:13:42 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> resourcemanager, PROXY_URI_BASES -> http://resourcemanager:8088/proxy/application_1735897749653_0001), /proxy/application_1735897749653_0001\n",
      "25/01/03 11:13:42 INFO Client: Application report for application_1735897749653_0001 (state: RUNNING)\n",
      "25/01/03 11:13:42 INFO Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: N/A\n",
      "\t ApplicationMaster host: 172.18.0.6\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1735899197230\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://resourcemanager:8088/proxy/application_1735897749653_0001/\n",
      "\t user: luser\n",
      "25/01/03 11:13:42 INFO YarnClientSchedulerBackend: Application application_1735897749653_0001 has started running.\n",
      "25/01/03 11:13:42 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36379.\n",
      "25/01/03 11:13:42 INFO NettyBlockTransferService: Server created on namenode:36379\n",
      "25/01/03 11:13:42 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "25/01/03 11:13:42 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, namenode, 36379, None)\n",
      "25/01/03 11:13:42 INFO BlockManagerMasterEndpoint: Registering block manager namenode:36379 with 413.9 MiB RAM, BlockManagerId(driver, namenode, 36379, None)\n",
      "25/01/03 11:13:42 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, namenode, 36379, None)\n",
      "25/01/03 11:13:42 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, namenode, 36379, None)\n",
      "25/01/03 11:13:42 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\n",
      "25/01/03 11:13:43 INFO ServerInfo: Adding filter to /jobs: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "25/01/03 11:13:43 INFO ServerInfo: Adding filter to /jobs/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "25/01/03 11:13:43 INFO ServerInfo: Adding filter to /jobs/job: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "25/01/03 11:13:43 INFO ServerInfo: Adding filter to /jobs/job/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "25/01/03 11:13:43 INFO ServerInfo: Adding filter to /stages: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "25/01/03 11:13:43 INFO ServerInfo: Adding filter to /stages/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "25/01/03 11:13:43 INFO ServerInfo: Adding filter to /stages/stage: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "25/01/03 11:13:43 INFO ServerInfo: Adding filter to /stages/stage/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "25/01/03 11:13:43 INFO ServerInfo: Adding filter to /stages/pool: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "25/01/03 11:13:43 INFO ServerInfo: Adding filter to /stages/pool/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "25/01/03 11:13:43 INFO ServerInfo: Adding filter to /storage: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "25/01/03 11:13:43 INFO ServerInfo: Adding filter to /storage/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "25/01/03 11:13:43 INFO ServerInfo: Adding filter to /storage/rdd: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "25/01/03 11:13:43 INFO ServerInfo: Adding filter to /storage/rdd/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "25/01/03 11:13:43 INFO ServerInfo: Adding filter to /environment: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "25/01/03 11:13:43 INFO ServerInfo: Adding filter to /environment/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "25/01/03 11:13:43 INFO ServerInfo: Adding filter to /executors: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "25/01/03 11:13:43 INFO ServerInfo: Adding filter to /executors/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "25/01/03 11:13:43 INFO ServerInfo: Adding filter to /executors/threadDump: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "25/01/03 11:13:43 INFO ServerInfo: Adding filter to /executors/threadDump/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "25/01/03 11:13:43 INFO ServerInfo: Adding filter to /executors/heapHistogram: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "25/01/03 11:13:43 INFO ServerInfo: Adding filter to /executors/heapHistogram/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "25/01/03 11:13:43 INFO ServerInfo: Adding filter to /static: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "25/01/03 11:13:43 INFO ServerInfo: Adding filter to /: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "25/01/03 11:13:43 INFO ServerInfo: Adding filter to /api: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "25/01/03 11:13:43 INFO ServerInfo: Adding filter to /jobs/job/kill: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "25/01/03 11:13:43 INFO ServerInfo: Adding filter to /stages/stage/kill: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "25/01/03 11:13:43 INFO ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "25/01/03 11:13:43 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000000000(ns)\n",
      "+--------+------+\n",
      "|NPatente|ncitas|\n",
      "+--------+------+\n",
      "| 4943137|     1|\n",
      "| 2959285|     1|\n",
      "| 3004604|     1|\n",
      "| 5060509|     1|\n",
      "| 5708825|     1|\n",
      "| 4549461|     1|\n",
      "| 4756599|     1|\n",
      "| 4515376|     1|\n",
      "| 1144012|     1|\n",
      "| 3615674|     1|\n",
      "| 3475056|     1|\n",
      "| 3535500|     1|\n",
      "| 3997459|     1|\n",
      "| 4257490|     1|\n",
      "| 2252991|     1|\n",
      "| 2480679|     1|\n",
      "| 3515849|     1|\n",
      "| 2863595|     1|\n",
      "| 4090538|     1|\n",
      "| 2445046|     1|\n",
      "+--------+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------+-------+----+\n",
      "|NPatente|COUNTRY| Año|\n",
      "+--------+-------+----+\n",
      "| 4101646|     JP|1978|\n",
      "| 4186332|     US|1980|\n",
      "| 4920512|     JP|1990|\n",
      "| 3512825|     ET|1970|\n",
      "| 3797992|     US|1974|\n",
      "| 5394547|     US|1995|\n",
      "| 4299230|     JP|1981|\n",
      "| 4348596|     US|1982|\n",
      "| 4708861|     US|1987|\n",
      "| 4694033|     NL|1987|\n",
      "| 3087707|     US|1963|\n",
      "| 5114044|     US|1992|\n",
      "| 4775172|     DE|1988|\n",
      "| 5326076|     US|1994|\n",
      "| 4979948|     US|1990|\n",
      "| 4786608|     US|1988|\n",
      "| 4499670|     NO|1985|\n",
      "| 4393370|     JP|1983|\n",
      "| 4757365|     NL|1988|\n",
      "| 3404349|     US|1968|\n",
      "+--------+-------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!spark-submit --master yarn ejercicio1.py /user/luser/patentes-mini/cite75_99.txt /user/luser/patentes-mini/apat63_99.txt dfCitas.parquet dfInfo.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 2\n",
    "\n",
    "Script que, a partir de los datos en Parquet de la práctica anterior, obtenga para cada país y para cada año el total de patentes, el total de citas obtenidas por todas las patentes, la media de citas y el máximo número de citas.\n",
    "- Obtener solo aquellos casos en los que existan valores en ambos ficheros (inner join).\n",
    "- Cada país tiene que aparecer con su nombre completo, obtenido del fichero `country_codes.txt`, residente en el disco local\n",
    "- El DataFrame generado debe estar ordenado por el máximo número de citas, país y año.\n",
    "\n",
    "Ejemplo de salida:\n",
    "\n",
    "\n",
    "|País         |Año |NumPatentes|TotalCitas|MediaCitas        |MaxCitas|\n",
    "|:------------|---:|----------:|---------:|-----------------:|-------:|\n",
    "|Japan        |1975|15         |17        |1.1333333333333333|3       |\n",
    "|United States|1982|129        |131       |1.0155038759689923|3       |\n",
    "|Germany      |1993|10         |11        |1.1               |2       |\n",
    "|Hungary      |1970|2          |3         |1.5               |2       |\n",
    "|Japan        |1983|29         |30        |1.0344827586206897|2       |\n",
    "|Japan        |1984|44         |45        |1.0227272727272727|2       |\n",
    "|Japan        |1986|51         |52        |1.0196078431372548|2       |\n",
    "|Sweden       |1972|2          |3         |1.5               |2       |\n",
    "|United States|1963|58         |59        |1.0172413793103448|2       |\n",
    "|United States|1965|79         |80        |1.0126582278481013|2       |\n",
    "|United States|1966|94         |95        |1.0106382978723405|2       |\n",
    "|United States|1967|122        |123       |1.0081967213114753|2       |\n",
    "|United States|1973|142        |144       |1.0140845070422535|2       |\n",
    "|United States|1974|168        |169       |1.005952380952381 |2       |\n",
    "\n",
    "\n",
    "### Requisitos\n",
    "- El DataFrame obtenido se debe guardar en un único fichero CSV sin comprimir y con cabecera.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prueba en local**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/01/03 11:38:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "+-------------+----+-----------+----------+------------------+--------+         \n",
      "|         País| Año|NumPatentes|TotalCitas|        MediaCitas|MaxCitas|\n",
      "+-------------+----+-----------+----------+------------------+--------+\n",
      "|        Japan|1975|         15|        15|1.1333333333333333|       3|\n",
      "|United States|1982|        129|       129|1.0155038759689923|       3|\n",
      "|      Germany|1993|         10|        10|               1.1|       2|\n",
      "|      Hungary|1970|          2|         2|               1.5|       2|\n",
      "|        Japan|1983|         29|        29|1.0344827586206897|       2|\n",
      "|        Japan|1984|         44|        44|1.0227272727272727|       2|\n",
      "|        Japan|1986|         51|        51|1.0196078431372548|       2|\n",
      "|       Sweden|1972|          2|         2|               1.5|       2|\n",
      "|United States|1963|         58|        58|1.0172413793103448|       2|\n",
      "|United States|1965|         79|        79|1.0126582278481013|       2|\n",
      "|United States|1966|         94|        94|1.0106382978723405|       2|\n",
      "|United States|1967|        122|       122|1.0081967213114753|       2|\n",
      "|United States|1973|        142|       142|1.0140845070422535|       2|\n",
      "|United States|1974|        168|       168| 1.005952380952381|       2|\n",
      "|United States|1976|        171|       171|1.0116959064327486|       2|\n",
      "|United States|1977|        153|       153|1.0065359477124183|       2|\n",
      "|United States|1978|        132|       132|1.0075757575757576|       2|\n",
      "|United States|1980|        144|       144|1.0069444444444444|       2|\n",
      "|United States|1989|        188|       188|1.0106382978723405|       2|\n",
      "|United States|1990|        163|       163|1.0122699386503067|       2|\n",
      "+-------------+----+-----------+----------+------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "!python3 ejercicio2.py patentes-mini/country_codes.txt dfCitas.parquet dfInfo.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prueba en el cluster**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--   3 luser supergroup       3637 2024-12-13 16:29 /user/luser/patentes-mini/country_codes.txt\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /user/luser/patentes-mini/country_codes.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/01/03 11:39:24 INFO SparkContext: Running Spark version 3.5.3\n",
      "25/01/03 11:39:24 INFO SparkContext: OS info Linux, 5.15.153.1-microsoft-standard-WSL2, amd64\n",
      "25/01/03 11:39:24 INFO SparkContext: Java version 11.0.24\n",
      "25/01/03 11:39:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/01/03 11:39:25 INFO ResourceUtils: ==============================================================\n",
      "25/01/03 11:39:25 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "25/01/03 11:39:25 INFO ResourceUtils: ==============================================================\n",
      "25/01/03 11:39:25 INFO SparkContext: Submitted application: Ejercicio2\n",
      "25/01/03 11:39:25 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "25/01/03 11:39:25 INFO ResourceProfile: Limiting resource is cpus at 1 tasks per executor\n",
      "25/01/03 11:39:25 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "25/01/03 11:39:25 INFO SecurityManager: Changing view acls to: luser\n",
      "25/01/03 11:39:25 INFO SecurityManager: Changing modify acls to: luser\n",
      "25/01/03 11:39:25 INFO SecurityManager: Changing view acls groups to: \n",
      "25/01/03 11:39:25 INFO SecurityManager: Changing modify acls groups to: \n",
      "25/01/03 11:39:25 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: luser; groups with view permissions: EMPTY; users with modify permissions: luser; groups with modify permissions: EMPTY\n",
      "25/01/03 11:39:26 INFO Utils: Successfully started service 'sparkDriver' on port 34583.\n",
      "25/01/03 11:39:26 INFO SparkEnv: Registering MapOutputTracker\n",
      "25/01/03 11:39:26 INFO SparkEnv: Registering BlockManagerMaster\n",
      "25/01/03 11:39:26 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "25/01/03 11:39:26 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "25/01/03 11:39:26 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "25/01/03 11:39:26 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-b9bbbe57-58f7-470c-8c73-c5cd112aa6f9\n",
      "25/01/03 11:39:26 INFO MemoryStore: MemoryStore started with capacity 413.9 MiB\n",
      "25/01/03 11:39:26 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "25/01/03 11:39:27 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
      "25/01/03 11:39:27 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "25/01/03 11:39:28 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at resourcemanager/172.18.0.2:8032\n",
      "25/01/03 11:39:28 INFO AHSProxy: Connecting to Application History server at timelineserver/172.18.0.7:10200\n",
      "25/01/03 11:39:29 INFO Configuration: resource-types.xml not found\n",
      "25/01/03 11:39:29 INFO ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "25/01/03 11:39:29 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (4096 MB per container)\n",
      "25/01/03 11:39:29 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead\n",
      "25/01/03 11:39:29 INFO Client: Setting up container launch context for our AM\n",
      "25/01/03 11:39:29 INFO Client: Setting up the launch environment for our AM container\n",
      "25/01/03 11:39:29 INFO Client: Preparing resources for our AM container\n",
      "25/01/03 11:39:29 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "25/01/03 11:39:33 INFO Client: Uploading resource file:/tmp/spark-689cc996-ec20-4d5a-a182-d7bcf2757a69/__spark_libs__2223980118408025508.zip -> hdfs://namenode:9000/user/luser/.sparkStaging/application_1735897749653_0003/__spark_libs__2223980118408025508.zip\n",
      "25/01/03 11:39:38 INFO Client: Uploading resource file:/home/luser/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip -> hdfs://namenode:9000/user/luser/.sparkStaging/application_1735897749653_0003/pyspark.zip\n",
      "25/01/03 11:39:39 INFO Client: Uploading resource file:/home/luser/.venv/lib/python3.10/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip -> hdfs://namenode:9000/user/luser/.sparkStaging/application_1735897749653_0003/py4j-0.10.9.7-src.zip\n",
      "25/01/03 11:39:39 INFO Client: Uploading resource file:/tmp/spark-689cc996-ec20-4d5a-a182-d7bcf2757a69/__spark_conf__9246608938117141663.zip -> hdfs://namenode:9000/user/luser/.sparkStaging/application_1735897749653_0003/__spark_conf__.zip\n",
      "25/01/03 11:39:39 INFO SecurityManager: Changing view acls to: luser\n",
      "25/01/03 11:39:39 INFO SecurityManager: Changing modify acls to: luser\n",
      "25/01/03 11:39:39 INFO SecurityManager: Changing view acls groups to: \n",
      "25/01/03 11:39:39 INFO SecurityManager: Changing modify acls groups to: \n",
      "25/01/03 11:39:39 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: luser; groups with view permissions: EMPTY; users with modify permissions: luser; groups with modify permissions: EMPTY\n",
      "25/01/03 11:39:39 INFO Client: Submitting application application_1735897749653_0003 to ResourceManager\n",
      "25/01/03 11:39:40 INFO YarnClientImpl: Submitted application application_1735897749653_0003\n",
      "25/01/03 11:39:41 INFO Client: Application report for application_1735897749653_0003 (state: ACCEPTED)\n",
      "25/01/03 11:39:41 INFO Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: AM container is launched, waiting for AM container to Register with RM\n",
      "\t ApplicationMaster host: N/A\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1735900779979\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://resourcemanager:8088/proxy/application_1735897749653_0003/\n",
      "\t user: luser\n",
      "25/01/03 11:39:52 INFO Client: Application report for application_1735897749653_0003 (state: RUNNING)\n",
      "25/01/03 11:39:52 INFO Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: N/A\n",
      "\t ApplicationMaster host: 172.18.0.3\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1735900779979\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://resourcemanager:8088/proxy/application_1735897749653_0003/\n",
      "\t user: luser\n",
      "25/01/03 11:39:52 INFO YarnClientSchedulerBackend: Application application_1735897749653_0003 has started running.\n",
      "25/01/03 11:39:52 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42657.\n",
      "25/01/03 11:39:52 INFO NettyBlockTransferService: Server created on namenode:42657\n",
      "25/01/03 11:39:52 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "25/01/03 11:39:52 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, namenode, 42657, None)\n",
      "25/01/03 11:39:52 INFO BlockManagerMasterEndpoint: Registering block manager namenode:42657 with 413.9 MiB RAM, BlockManagerId(driver, namenode, 42657, None)\n",
      "25/01/03 11:39:52 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, namenode, 42657, None)\n",
      "25/01/03 11:39:52 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, namenode, 42657, None)\n",
      "25/01/03 11:39:53 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> resourcemanager, PROXY_URI_BASES -> http://resourcemanager:8088/proxy/application_1735897749653_0003), /proxy/application_1735897749653_0003\n",
      "25/01/03 11:39:53 INFO ServerInfo: Adding filter to /: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "25/01/03 11:39:53 INFO ServerInfo: Adding filter to /api: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "25/01/03 11:39:53 INFO ServerInfo: Adding filter to /jobs/job/kill: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "25/01/03 11:39:53 INFO ServerInfo: Adding filter to /stages/stage/kill: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "25/01/03 11:39:53 INFO ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "25/01/03 11:39:54 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\n",
      "25/01/03 11:39:57 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000000000(ns)\n",
      "+-------------+----+-----------+----------+------------------+--------+\n",
      "|         País| Año|NumPatentes|TotalCitas|        MediaCitas|MaxCitas|\n",
      "+-------------+----+-----------+----------+------------------+--------+\n",
      "|        Japan|1975|         15|        15|1.1333333333333333|       3|\n",
      "|United States|1982|        129|       129|1.0155038759689923|       3|\n",
      "|      Germany|1993|         10|        10|               1.1|       2|\n",
      "|      Hungary|1970|          2|         2|               1.5|       2|\n",
      "|        Japan|1983|         29|        29|1.0344827586206897|       2|\n",
      "|        Japan|1984|         44|        44|1.0227272727272727|       2|\n",
      "|        Japan|1986|         51|        51|1.0196078431372548|       2|\n",
      "|       Sweden|1972|          2|         2|               1.5|       2|\n",
      "|United States|1963|         58|        58|1.0172413793103448|       2|\n",
      "|United States|1965|         79|        79|1.0126582278481013|       2|\n",
      "|United States|1966|         94|        94|1.0106382978723405|       2|\n",
      "|United States|1967|        122|       122|1.0081967213114753|       2|\n",
      "|United States|1973|        142|       142|1.0140845070422535|       2|\n",
      "|United States|1974|        168|       168| 1.005952380952381|       2|\n",
      "|United States|1976|        171|       171|1.0116959064327486|       2|\n",
      "|United States|1977|        153|       153|1.0065359477124183|       2|\n",
      "|United States|1978|        132|       132|1.0075757575757576|       2|\n",
      "|United States|1980|        144|       144|1.0069444444444444|       2|\n",
      "|United States|1989|        188|       188|1.0106382978723405|       2|\n",
      "|United States|1990|        163|       163|1.0122699386503067|       2|\n",
      "+-------------+----+-----------+----------+------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!spark-submit --master yarn ejercicio2.py /user/luser/patentes-mini/country_codes.txt dfCitas.parquet dfInfo.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 3\n",
    "\n",
    "Obtener a partir de los ficheros Parquet creados en el ejercicio 1 un DataFrame que proporcione, para un grupo de países especificado, las patentes ordenadas por número de citas, de mayor a menor, junto con una columna que indique el ango (posición de la patente en esa país/año según las citas obtenidas):\n",
    "\n",
    "La salida del script debe ser como sigue:\n",
    "\n",
    "|País|Año |Npatente|Ncitas|Rango|\n",
    "|:---|---:|-------:|-----:|----:|\n",
    "|ES  |1963|3093080 |20    |1    |\n",
    "|ES  |1963|3099309 |19    |2    |\n",
    "|ES  |1963|3081560 |9     |3    |\n",
    "|ES  |1963|3071439 |9     |3    |\n",
    "|ES  |1963|3074559 |6     |4    |\n",
    "|ES  |1963|3114233 |5     |5    |\n",
    "|ES  |1963|3094845 |4     |6    |\n",
    "|ES  |1963|3106762 |3     |7    |\n",
    "|ES  |1963|3088009 |3     |7    |\n",
    "|ES  |1963|3087842 |2     |8    |\n",
    "|ES  |1963|3078145 |2     |8    |\n",
    "|ES  |1963|3094806 |2     |8    |\n",
    "|ES  |1963|3073124 |2     |8    |\n",
    "|ES  |1963|3112201 |2     |8    |\n",
    "|ES  |1963|3102971 |1     |9    |\n",
    "|ES  |1963|3112703 |1     |9    |\n",
    "|ES  |1963|3095297 |1     |9    |\n",
    "|ES  |1964|3129307 |11    |1    |\n",
    "|ES  |1964|3133001 |10    |2    |\n",
    "|ES  |1964|3161239 |8     |3    |\n",
    "|... |... |...     |...   |...|\n",
    "|FR  |1963|3111006 |35    |1    |\n",
    "|FR  |1963|3083101 |22    |2    |\n",
    "|FR  |1963|3077496 |16    |3    |\n",
    "|FR  |1963|3072512 |15    |4    |\n",
    "|FR  |1963|3090203 |15    |4    |\n",
    "|FR  |1963|3086777 |14    |5    |\n",
    "|FR  |1963|3074344 |13    |6    |\n",
    "|FR  |1963|3096621 |13    |6    |\n",
    "|FR  |1963|3089153 |13    |6    |\n",
    "|... |... |...     |...   |...|\n",
    "\n",
    "\n",
    "### Requisitos\n",
    "* El DataFrame debe de estar ordenado por código del país y año (ascendente) y número de citas (descendente).\n",
    "* Utilizad funciones de ventana para obtener el rango.\n",
    "* NO reemplazar el código del país por su nombre completo.\n",
    "* La salida debe guardarse en un único fichero CSV sin comprimir y con cabecera.\n",
    "* Como en los casos anteriores, el script debe aceptar argumentos en línea de comandos, es decir, para su ejecución deberíamos poder indicar la ruta a los directorios de entrada creados en la práctica 1, la lista de países a analizar (separados por coma) y el nombre del directorio de salida."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prueba en local**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/01/03 11:49:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "+----+----+--------+------+-----+                                               \n",
      "|País| Año|NPatente|Ncitas|Rango|\n",
      "+----+----+--------+------+-----+\n",
      "|  ES|1976| 3982002|     1|    1|\n",
      "|  ES|1977| 4003165|     1|    1|\n",
      "|  ES|1994| 5368732|     1|    1|\n",
      "|  FR|1963| 3087028|     1|    1|\n",
      "|  FR|1964| 3154455|     1|    1|\n",
      "|  FR|1965| 3193498|     1|    1|\n",
      "|  FR|1965| 3175771|     1|    2|\n",
      "|  FR|1965| 3218816|     1|    3|\n",
      "|  FR|1966| 3265033|     1|    1|\n",
      "|  FR|1966| 3252677|     1|    2|\n",
      "|  FR|1967| 3352057|     1|    1|\n",
      "|  FR|1967| 3356598|     1|    2|\n",
      "|  FR|1968| 3390460|     1|    1|\n",
      "|  FR|1968| 3405706|     1|    2|\n",
      "|  FR|1969| 3450524|     1|    1|\n",
      "|  FR|1969| 3449616|     1|    2|\n",
      "|  FR|1970| 3497939|     1|    1|\n",
      "|  FR|1970| 3497630|     1|    2|\n",
      "|  FR|1971| 3601261|     1|    1|\n",
      "|  FR|1971| 3565562|     1|    2|\n",
      "+----+----+--------+------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "!python3 ejercicio3.py dfCitas.parquet dfInfo.parquet \"ES,FR,US\" \"output_ejercicio3.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prueba en el cluster**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/01/03 11:50:22 INFO SparkContext: Running Spark version 3.5.3\n",
      "25/01/03 11:50:22 INFO SparkContext: OS info Linux, 5.15.153.1-microsoft-standard-WSL2, amd64\n",
      "25/01/03 11:50:22 INFO SparkContext: Java version 11.0.24\n",
      "25/01/03 11:50:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/01/03 11:50:22 INFO ResourceUtils: ==============================================================\n",
      "25/01/03 11:50:22 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "25/01/03 11:50:22 INFO ResourceUtils: ==============================================================\n",
      "25/01/03 11:50:22 INFO SparkContext: Submitted application: Ejercicio3\n",
      "25/01/03 11:50:22 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "25/01/03 11:50:22 INFO ResourceProfile: Limiting resource is cpus at 1 tasks per executor\n",
      "25/01/03 11:50:22 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "25/01/03 11:50:22 INFO SecurityManager: Changing view acls to: luser\n",
      "25/01/03 11:50:22 INFO SecurityManager: Changing modify acls to: luser\n",
      "25/01/03 11:50:22 INFO SecurityManager: Changing view acls groups to: \n",
      "25/01/03 11:50:22 INFO SecurityManager: Changing modify acls groups to: \n",
      "25/01/03 11:50:22 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: luser; groups with view permissions: EMPTY; users with modify permissions: luser; groups with modify permissions: EMPTY\n",
      "25/01/03 11:50:23 INFO Utils: Successfully started service 'sparkDriver' on port 40157.\n",
      "25/01/03 11:50:23 INFO SparkEnv: Registering MapOutputTracker\n",
      "25/01/03 11:50:23 INFO SparkEnv: Registering BlockManagerMaster\n",
      "25/01/03 11:50:23 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "25/01/03 11:50:23 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "25/01/03 11:50:23 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "25/01/03 11:50:24 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-4ab13c1a-10c9-4c01-b5bc-8bba1b52d6a3\n",
      "25/01/03 11:50:24 INFO MemoryStore: MemoryStore started with capacity 413.9 MiB\n",
      "25/01/03 11:50:24 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "25/01/03 11:50:24 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
      "25/01/03 11:50:24 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "25/01/03 11:50:25 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at resourcemanager/172.18.0.2:8032\n",
      "25/01/03 11:50:26 INFO AHSProxy: Connecting to Application History server at timelineserver/172.18.0.7:10200\n",
      "25/01/03 11:50:26 INFO Configuration: resource-types.xml not found\n",
      "25/01/03 11:50:26 INFO ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "25/01/03 11:50:26 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (4096 MB per container)\n",
      "25/01/03 11:50:26 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead\n",
      "25/01/03 11:50:26 INFO Client: Setting up container launch context for our AM\n",
      "25/01/03 11:50:26 INFO Client: Setting up the launch environment for our AM container\n",
      "25/01/03 11:50:26 INFO Client: Preparing resources for our AM container\n",
      "25/01/03 11:50:26 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "25/01/03 11:50:32 INFO Client: Uploading resource file:/tmp/spark-2bb0d141-87f4-4684-9471-45e2c413565a/__spark_libs__5531849126315561685.zip -> hdfs://namenode:9000/user/luser/.sparkStaging/application_1735897749653_0004/__spark_libs__5531849126315561685.zip\n",
      "25/01/03 11:50:42 INFO Client: Uploading resource file:/home/luser/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip -> hdfs://namenode:9000/user/luser/.sparkStaging/application_1735897749653_0004/pyspark.zip\n",
      "25/01/03 11:50:43 INFO Client: Uploading resource file:/home/luser/.venv/lib/python3.10/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip -> hdfs://namenode:9000/user/luser/.sparkStaging/application_1735897749653_0004/py4j-0.10.9.7-src.zip\n",
      "25/01/03 11:50:43 INFO Client: Uploading resource file:/tmp/spark-2bb0d141-87f4-4684-9471-45e2c413565a/__spark_conf__5628910387207291362.zip -> hdfs://namenode:9000/user/luser/.sparkStaging/application_1735897749653_0004/__spark_conf__.zip\n",
      "25/01/03 11:50:43 INFO SecurityManager: Changing view acls to: luser\n",
      "25/01/03 11:50:43 INFO SecurityManager: Changing modify acls to: luser\n",
      "25/01/03 11:50:43 INFO SecurityManager: Changing view acls groups to: \n",
      "25/01/03 11:50:43 INFO SecurityManager: Changing modify acls groups to: \n",
      "25/01/03 11:50:43 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: luser; groups with view permissions: EMPTY; users with modify permissions: luser; groups with modify permissions: EMPTY\n",
      "25/01/03 11:50:43 INFO Client: Submitting application application_1735897749653_0004 to ResourceManager\n",
      "25/01/03 11:50:44 INFO YarnClientImpl: Submitted application application_1735897749653_0004\n",
      "25/01/03 11:50:45 INFO Client: Application report for application_1735897749653_0004 (state: ACCEPTED)\n",
      "25/01/03 11:50:45 INFO Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: AM container is launched, waiting for AM container to Register with RM\n",
      "\t ApplicationMaster host: N/A\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1735901444041\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://resourcemanager:8088/proxy/application_1735897749653_0004/\n",
      "\t user: luser\n",
      "25/01/03 11:51:00 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> resourcemanager, PROXY_URI_BASES -> http://resourcemanager:8088/proxy/application_1735897749653_0004), /proxy/application_1735897749653_0004\n",
      "25/01/03 11:51:00 INFO Client: Application report for application_1735897749653_0004 (state: RUNNING)\n",
      "25/01/03 11:51:00 INFO Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: N/A\n",
      "\t ApplicationMaster host: 172.18.0.4\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1735901444041\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://resourcemanager:8088/proxy/application_1735897749653_0004/\n",
      "\t user: luser\n",
      "25/01/03 11:51:00 INFO YarnClientSchedulerBackend: Application application_1735897749653_0004 has started running.\n",
      "25/01/03 11:51:00 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36483.\n",
      "25/01/03 11:51:00 INFO NettyBlockTransferService: Server created on namenode:36483\n",
      "25/01/03 11:51:00 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "25/01/03 11:51:00 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, namenode, 36483, None)\n",
      "25/01/03 11:51:00 INFO BlockManagerMasterEndpoint: Registering block manager namenode:36483 with 413.9 MiB RAM, BlockManagerId(driver, namenode, 36483, None)\n",
      "25/01/03 11:51:00 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, namenode, 36483, None)\n",
      "25/01/03 11:51:00 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, namenode, 36483, None)\n",
      "25/01/03 11:51:00 INFO ServerInfo: Adding filter to /jobs: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "25/01/03 11:51:01 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\n",
      "25/01/03 11:51:01 INFO ServerInfo: Adding filter to /jobs/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "25/01/03 11:51:01 INFO ServerInfo: Adding filter to /jobs/job: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "25/01/03 11:51:01 INFO ServerInfo: Adding filter to /jobs/job/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "25/01/03 11:51:01 INFO ServerInfo: Adding filter to /stages: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "25/01/03 11:51:01 INFO ServerInfo: Adding filter to /stages/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "25/01/03 11:51:01 INFO ServerInfo: Adding filter to /stages/stage: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "25/01/03 11:51:01 INFO ServerInfo: Adding filter to /stages/stage/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "25/01/03 11:51:01 INFO ServerInfo: Adding filter to /stages/pool: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "25/01/03 11:51:01 INFO ServerInfo: Adding filter to /stages/pool/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "25/01/03 11:51:01 INFO ServerInfo: Adding filter to /storage: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "25/01/03 11:51:01 INFO ServerInfo: Adding filter to /storage/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "25/01/03 11:51:01 INFO ServerInfo: Adding filter to /storage/rdd: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "25/01/03 11:51:01 INFO ServerInfo: Adding filter to /storage/rdd/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "25/01/03 11:51:01 INFO ServerInfo: Adding filter to /environment: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "25/01/03 11:51:01 INFO ServerInfo: Adding filter to /environment/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "25/01/03 11:51:01 INFO ServerInfo: Adding filter to /executors: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "25/01/03 11:51:01 INFO ServerInfo: Adding filter to /executors/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "25/01/03 11:51:01 INFO ServerInfo: Adding filter to /executors/threadDump: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "25/01/03 11:51:01 INFO ServerInfo: Adding filter to /executors/threadDump/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "25/01/03 11:51:01 INFO ServerInfo: Adding filter to /executors/heapHistogram: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "25/01/03 11:51:01 INFO ServerInfo: Adding filter to /executors/heapHistogram/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "25/01/03 11:51:01 INFO ServerInfo: Adding filter to /static: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "25/01/03 11:51:01 INFO ServerInfo: Adding filter to /: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "25/01/03 11:51:01 INFO ServerInfo: Adding filter to /api: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "25/01/03 11:51:01 INFO ServerInfo: Adding filter to /jobs/job/kill: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "25/01/03 11:51:01 INFO ServerInfo: Adding filter to /stages/stage/kill: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "25/01/03 11:51:01 INFO ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "25/01/03 11:51:01 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000000000(ns)\n",
      "+----+----+--------+------+-----+\n",
      "|País| Año|NPatente|Ncitas|Rango|\n",
      "+----+----+--------+------+-----+\n",
      "|  ES|1976| 3982002|     1|    1|\n",
      "|  ES|1977| 4003165|     1|    1|\n",
      "|  ES|1994| 5368732|     1|    1|\n",
      "|  FR|1963| 3087028|     1|    1|\n",
      "|  FR|1964| 3154455|     1|    1|\n",
      "|  FR|1965| 3193498|     1|    1|\n",
      "|  FR|1965| 3175771|     1|    2|\n",
      "|  FR|1965| 3218816|     1|    3|\n",
      "|  FR|1966| 3265033|     1|    1|\n",
      "|  FR|1966| 3252677|     1|    2|\n",
      "|  FR|1967| 3352057|     1|    1|\n",
      "|  FR|1967| 3356598|     1|    2|\n",
      "|  FR|1968| 3390460|     1|    1|\n",
      "|  FR|1968| 3405706|     1|    2|\n",
      "|  FR|1969| 3450524|     1|    1|\n",
      "|  FR|1969| 3449616|     1|    2|\n",
      "|  FR|1970| 3497939|     1|    1|\n",
      "|  FR|1970| 3497630|     1|    2|\n",
      "|  FR|1971| 3601261|     1|    1|\n",
      "|  FR|1971| 3565562|     1|    2|\n",
      "+----+----+--------+------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!spark-submit --master yarn ejercicio3.py dfCitas.parquet dfInfo.parquet \"ES,FR,US\" \"output_ejercicio3.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 4\n",
    "\n",
    "Obtener a partir del fichero Parquet con la información de (Npatente, País y Año) un DataFrame que nos muestre el número de patentes asociadas a cada país por cada década (entendemos por década los años del 0 al 9, es decir de 1970 a 1979 es la década de los 70s). Adicionalmente, debe mostrar el aumento o disminución del número de patentes para cada país y década con respecto al la década anterior.\n",
    "\n",
    "El DataFrame generado tiene que ser como este:\n",
    "\n",
    "|País|Década|NPatentes|Dif |\n",
    "|:---|-----:|--------:|---:|\n",
    "|AD  |1980  |1        |0   |\n",
    "|AD  |1990  |5        |4   |\n",
    "|AE  |1980  |7        |0   |\n",
    "|AE  |1990  |11       |4   |\n",
    "|AG  |1970  |2        |0   |\n",
    "|AG  |1990  |7        |5   |\n",
    "|AI  |1990  |1        |0   |\n",
    "|AL  |1990  |1        |0   |\n",
    "|AM  |1990  |2        |0   |\n",
    "|AN  |1970  |1        |0   |\n",
    "|AN  |1980  |2        |1   |\n",
    "|AN  |1990  |5        |3   |\n",
    "|AR  |1960  |135      |0   |\n",
    "|AR  |1970  |239      |104 |\n",
    "|AR  |1980  |184      |-55 |\n",
    "|AR  |1990  |292      |108 |\n",
    "|... |...   |...      |...|\n",
    "\n",
    "### Requisitos\n",
    "* El DataFrame debe de estar ordenado por código del país y año.\n",
    "* NO reemplazar el código del país por su nombre completo.\n",
    "* La salida debe guardarse en un único fichero CSV sin comprimir y con cabecera.\n",
    "* Como en los casos anteriores, el script debe aceptar argumentos en línea de comandos, es decir, para su ejecución deberíamos poder indicar la ruta al directorio de entrada creado en la práctica 1 y el nombre del directorio de salida."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prueba en local**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/01/03 11:57:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "+-------+------+---------+---+                                                  \n",
      "|COUNTRY|Década|NPatentes|Dif|\n",
      "+-------+------+---------+---+\n",
      "|     AR|  1960|        1|  1|\n",
      "|     AR|  1980|        1|  0|\n",
      "|     AR|  1990|        1|  0|\n",
      "|     AT|  1960|        2|  2|\n",
      "|     AT|  1970|       31| 29|\n",
      "|     AT|  1980|       41| 10|\n",
      "|     AT|  1990|        7|-34|\n",
      "|     AU|  1970|       16| 16|\n",
      "|     AU|  1980|       19|  3|\n",
      "|     AU|  1990|       36| 17|\n",
      "|     BE|  1960|        2|  2|\n",
      "|     BE|  1970|       16| 14|\n",
      "|     BE|  1980|       15| -1|\n",
      "|     BE|  1990|       20|  5|\n",
      "|     BG|  1970|        1|  1|\n",
      "|     BG|  1980|        1|  0|\n",
      "|     BG|  1990|        1|  0|\n",
      "|     BR|  1990|        3|  3|\n",
      "|     BY|  1990|        1|  1|\n",
      "|     CA|  1960|       11| 11|\n",
      "+-------+------+---------+---+\n",
      "only showing top 20 rows\n",
      "\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "!python3 ejercicio4.py dfInfo.parquet \"output_ejercicio4.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prueba en el cluster**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/01/03 11:58:43 INFO SparkContext: Running Spark version 3.5.3\n",
      "25/01/03 11:58:43 INFO SparkContext: OS info Linux, 5.15.153.1-microsoft-standard-WSL2, amd64\n",
      "25/01/03 11:58:43 INFO SparkContext: Java version 11.0.24\n",
      "25/01/03 11:58:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/01/03 11:58:44 INFO ResourceUtils: ==============================================================\n",
      "25/01/03 11:58:44 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "25/01/03 11:58:44 INFO ResourceUtils: ==============================================================\n",
      "25/01/03 11:58:44 INFO SparkContext: Submitted application: Ejercicio4\n",
      "25/01/03 11:58:44 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "25/01/03 11:58:44 INFO ResourceProfile: Limiting resource is cpus at 1 tasks per executor\n",
      "25/01/03 11:58:44 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "25/01/03 11:58:44 INFO SecurityManager: Changing view acls to: luser\n",
      "25/01/03 11:58:44 INFO SecurityManager: Changing modify acls to: luser\n",
      "25/01/03 11:58:44 INFO SecurityManager: Changing view acls groups to: \n",
      "25/01/03 11:58:44 INFO SecurityManager: Changing modify acls groups to: \n",
      "25/01/03 11:58:44 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: luser; groups with view permissions: EMPTY; users with modify permissions: luser; groups with modify permissions: EMPTY\n",
      "25/01/03 11:58:45 INFO Utils: Successfully started service 'sparkDriver' on port 40365.\n",
      "25/01/03 11:58:45 INFO SparkEnv: Registering MapOutputTracker\n",
      "25/01/03 11:58:45 INFO SparkEnv: Registering BlockManagerMaster\n",
      "25/01/03 11:58:45 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "25/01/03 11:58:45 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "25/01/03 11:58:45 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "25/01/03 11:58:45 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-b2e9f9b7-a4f7-444a-b863-2e7715004ab1\n",
      "25/01/03 11:58:45 INFO MemoryStore: MemoryStore started with capacity 413.9 MiB\n",
      "25/01/03 11:58:46 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "25/01/03 11:58:46 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
      "25/01/03 11:58:46 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "25/01/03 11:58:47 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at resourcemanager/172.18.0.2:8032\n",
      "25/01/03 11:58:48 INFO AHSProxy: Connecting to Application History server at timelineserver/172.18.0.7:10200\n",
      "25/01/03 11:58:49 INFO Configuration: resource-types.xml not found\n",
      "25/01/03 11:58:49 INFO ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "25/01/03 11:58:49 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (4096 MB per container)\n",
      "25/01/03 11:58:49 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead\n",
      "25/01/03 11:58:49 INFO Client: Setting up container launch context for our AM\n",
      "25/01/03 11:58:49 INFO Client: Setting up the launch environment for our AM container\n",
      "25/01/03 11:58:49 INFO Client: Preparing resources for our AM container\n",
      "25/01/03 11:58:49 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "25/01/03 11:58:55 INFO Client: Uploading resource file:/tmp/spark-ad863f85-c24f-4ff0-923b-2b8e811a53e2/__spark_libs__13816752925185036482.zip -> hdfs://namenode:9000/user/luser/.sparkStaging/application_1735897749653_0005/__spark_libs__13816752925185036482.zip\n",
      "25/01/03 11:59:13 INFO Client: Uploading resource file:/home/luser/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip -> hdfs://namenode:9000/user/luser/.sparkStaging/application_1735897749653_0005/pyspark.zip\n",
      "25/01/03 11:59:13 INFO Client: Uploading resource file:/home/luser/.venv/lib/python3.10/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip -> hdfs://namenode:9000/user/luser/.sparkStaging/application_1735897749653_0005/py4j-0.10.9.7-src.zip\n",
      "25/01/03 11:59:14 INFO Client: Uploading resource file:/tmp/spark-ad863f85-c24f-4ff0-923b-2b8e811a53e2/__spark_conf__17660382893634550552.zip -> hdfs://namenode:9000/user/luser/.sparkStaging/application_1735897749653_0005/__spark_conf__.zip\n",
      "25/01/03 11:59:14 INFO SecurityManager: Changing view acls to: luser\n",
      "25/01/03 11:59:14 INFO SecurityManager: Changing modify acls to: luser\n",
      "25/01/03 11:59:14 INFO SecurityManager: Changing view acls groups to: \n",
      "25/01/03 11:59:14 INFO SecurityManager: Changing modify acls groups to: \n",
      "25/01/03 11:59:14 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: luser; groups with view permissions: EMPTY; users with modify permissions: luser; groups with modify permissions: EMPTY\n",
      "25/01/03 11:59:14 INFO Client: Submitting application application_1735897749653_0005 to ResourceManager\n",
      "25/01/03 11:59:15 INFO YarnClientImpl: Submitted application application_1735897749653_0005\n",
      "25/01/03 11:59:16 INFO Client: Application report for application_1735897749653_0005 (state: ACCEPTED)\n",
      "25/01/03 11:59:16 INFO Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: [vie. ene. 03 11:59:15 +0100 2025] Scheduler has assigned a container for AM, waiting for AM container to be launched\n",
      "\t ApplicationMaster host: N/A\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1735901954933\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://resourcemanager:8088/proxy/application_1735897749653_0005/\n",
      "\t user: luser\n",
      "25/01/03 11:59:34 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> resourcemanager, PROXY_URI_BASES -> http://resourcemanager:8088/proxy/application_1735897749653_0005), /proxy/application_1735897749653_0005\n",
      "25/01/03 11:59:34 INFO Client: Application report for application_1735897749653_0005 (state: RUNNING)\n",
      "25/01/03 11:59:34 INFO Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: N/A\n",
      "\t ApplicationMaster host: 172.18.0.4\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1735901954933\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://resourcemanager:8088/proxy/application_1735897749653_0005/\n",
      "\t user: luser\n",
      "25/01/03 11:59:34 INFO YarnClientSchedulerBackend: Application application_1735897749653_0005 has started running.\n",
      "25/01/03 11:59:34 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33363.\n",
      "25/01/03 11:59:34 INFO NettyBlockTransferService: Server created on namenode:33363\n",
      "25/01/03 11:59:34 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "25/01/03 11:59:34 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, namenode, 33363, None)\n",
      "25/01/03 11:59:34 INFO BlockManagerMasterEndpoint: Registering block manager namenode:33363 with 413.9 MiB RAM, BlockManagerId(driver, namenode, 33363, None)\n",
      "25/01/03 11:59:34 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, namenode, 33363, None)\n",
      "25/01/03 11:59:34 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, namenode, 33363, None)\n",
      "25/01/03 11:59:35 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\n",
      "25/01/03 11:59:35 INFO ServerInfo: Adding filter to /jobs: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "25/01/03 11:59:35 INFO ServerInfo: Adding filter to /jobs/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "25/01/03 11:59:35 INFO ServerInfo: Adding filter to /jobs/job: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "25/01/03 11:59:35 INFO ServerInfo: Adding filter to /jobs/job/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "25/01/03 11:59:35 INFO ServerInfo: Adding filter to /stages: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "25/01/03 11:59:35 INFO ServerInfo: Adding filter to /stages/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "25/01/03 11:59:35 INFO ServerInfo: Adding filter to /stages/stage: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "25/01/03 11:59:35 INFO ServerInfo: Adding filter to /stages/stage/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "25/01/03 11:59:35 INFO ServerInfo: Adding filter to /stages/pool: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "25/01/03 11:59:36 INFO ServerInfo: Adding filter to /stages/pool/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "25/01/03 11:59:36 INFO ServerInfo: Adding filter to /storage: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "25/01/03 11:59:36 INFO ServerInfo: Adding filter to /storage/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "25/01/03 11:59:36 INFO ServerInfo: Adding filter to /storage/rdd: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "25/01/03 11:59:36 INFO ServerInfo: Adding filter to /storage/rdd/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "25/01/03 11:59:36 INFO ServerInfo: Adding filter to /environment: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "25/01/03 11:59:36 INFO ServerInfo: Adding filter to /environment/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "25/01/03 11:59:36 INFO ServerInfo: Adding filter to /executors: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "25/01/03 11:59:36 INFO ServerInfo: Adding filter to /executors/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "25/01/03 11:59:36 INFO ServerInfo: Adding filter to /executors/threadDump: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "25/01/03 11:59:36 INFO ServerInfo: Adding filter to /executors/threadDump/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "25/01/03 11:59:36 INFO ServerInfo: Adding filter to /executors/heapHistogram: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "25/01/03 11:59:36 INFO ServerInfo: Adding filter to /executors/heapHistogram/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "25/01/03 11:59:36 INFO ServerInfo: Adding filter to /static: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "25/01/03 11:59:36 INFO ServerInfo: Adding filter to /: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "25/01/03 11:59:36 INFO ServerInfo: Adding filter to /api: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "25/01/03 11:59:36 INFO ServerInfo: Adding filter to /jobs/job/kill: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "25/01/03 11:59:36 INFO ServerInfo: Adding filter to /stages/stage/kill: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "25/01/03 11:59:36 INFO ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "25/01/03 11:59:36 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000000000(ns)\n",
      "+-------+------+---------+---+\n",
      "|COUNTRY|Década|NPatentes|Dif|\n",
      "+-------+------+---------+---+\n",
      "|     AR|  1960|        1|  1|\n",
      "|     AR|  1980|        1|  0|\n",
      "|     AR|  1990|        1|  0|\n",
      "|     AT|  1960|        2|  2|\n",
      "|     AT|  1970|       31| 29|\n",
      "|     AT|  1980|       41| 10|\n",
      "|     AT|  1990|        7|-34|\n",
      "|     AU|  1970|       16| 16|\n",
      "|     AU|  1980|       19|  3|\n",
      "|     AU|  1990|       36| 17|\n",
      "|     BE|  1960|        2|  2|\n",
      "|     BE|  1970|       16| 14|\n",
      "|     BE|  1980|       15| -1|\n",
      "|     BE|  1990|       20|  5|\n",
      "|     BG|  1970|        1|  1|\n",
      "|     BG|  1980|        1|  0|\n",
      "|     BG|  1990|        1|  0|\n",
      "|     BR|  1990|        3|  3|\n",
      "|     BY|  1990|        1|  1|\n",
      "|     CA|  1960|       11| 11|\n",
      "+-------+------+---------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!spark-submit --master yarn ejercicio4.py dfInfo.parquet \"output_ejercicio4.csv\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
