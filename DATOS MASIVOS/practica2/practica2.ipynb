{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos conectamos al servidor de jupiter del namenode. Para ello, hay que:\n",
    "\n",
    "- Activar el entorno virtual con `source venv/bin/activate`\n",
    "- Conectarse al servidor con `jupyter notebook --ip=0.0.0.0 --port=8888 --no-browser --NotebookApp.token=''`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez tenemos el servidor ejecutando, le damos a `select kernel` en VScode y seleccionamos `existing jupyter server`. En la ventana que aparece, ponemos `http://127.0.0.1:8888/tree` y ya podemos trabajar con el notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirmamos que estamos en el directorio correcto `(/home/luser)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/luser\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos la varaiable de entorno `CLASSPATH`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "# Establecer la variable de entorno\n",
    "classpath = subprocess.check_output('hdfs classpath --glob', shell=True).decode().strip()\n",
    "os.environ['CLASSPATH'] = classpath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejecución de código filesystem_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos un fichero de texto de prueba que contenga la siguiente información:\n",
    "```plaintext\n",
    "Esto es una prueba de un archivo de texto.\n",
    "\n",
    "Esto es una prueba de una linea de un archivo de texto.\n",
    "\n",
    "FIN.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Esto es una prueba de un archivo de texto.\n",
      "\n",
      "Esto es una prueba de una linea de un archivo de texto.\n",
      "\n",
      "FIN.\n",
      "\n",
      "\n",
      "Esto es una prueba de un archivo de texto.\n",
      "\n",
      "Esto es una prueba de una linea de un archivo de texto.\n",
      "\n",
      "FIN.\n"
     ]
    }
   ],
   "source": [
    "# Creamos el contenido del archivo\n",
    "content = '''\n",
    "Esto es una prueba de un archivo de texto.\n",
    "\n",
    "Esto es una prueba de una linea de un archivo de texto.\n",
    "\n",
    "FIN.\n",
    "'''\n",
    "print(content)\n",
    "\n",
    "# Escribimos el contenido con python\n",
    "with open('test_2.txt', 'w') as f:\n",
    "\tf.write(content)\n",
    "\n",
    "# Enseñamos el contenido del archivo con bash\n",
    "!cat test_2.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Movemos el fichero de prueba test.txt a hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 items\n",
      "drwxr-xr-x   - luser supergroup          0 2024-12-13 18:13 hdfs://namenode:9000/user/luser/.sparkStaging\n",
      "drwxr-xr-x   - luser supergroup          0 2024-12-04 18:01 hdfs://namenode:9000/user/luser/libros\n",
      "drwxr-xr-x   - luser supergroup          0 2024-12-13 16:27 hdfs://namenode:9000/user/luser/patentes\n",
      "drwxr-xr-x   - luser supergroup          0 2024-12-13 16:29 hdfs://namenode:9000/user/luser/patentes-mini\n",
      "-rw-r--r--   3 luser supergroup        106 2025-01-01 16:46 hdfs://namenode:9000/user/luser/test.txt\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Copiamos un fichero local a HDFS\n",
    "hdfs dfs -put -f test.txt hdfs://namenode:9000/user/luser\n",
    "\n",
    "# Comprobamos que el fichero se ha copiado correctamente\n",
    "hdfs dfs -ls hdfs://namenode:9000/user/luser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprobamos que el script filesystem_cat.py funciona correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-01 16:53:58,492 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Esto es una prueba de un archivo de texto.\n",
      "\n",
      "Esto es una prueba de una linea de un archivo de texto.\n",
      "\n",
      "FIN.\n"
     ]
    }
   ],
   "source": [
    "!python3 filesystem_cat.py hdfs://namenode:9000/user/luser/test.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acceso y gestión de HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arreglamos el código de copy_half_file.py para que funcione correctamente. El contenido es:\n",
    "```python\n",
    "#! /usr/bin/env python3\n",
    "from pyarrow.fs import FileSystem, FileInfo\n",
    "import sys\n",
    "import shutil\n",
    "\n",
    "def copy_half_file(uri1: str, uri2: str) -> None:\n",
    "    fs1, path = FileSystem.from_uri(uri1)\n",
    "    fs2, path2 = FileSystem.from_uri(uri2)\n",
    "\n",
    "    f_info: FileInfo = fs1.get_file_info(path)  # Obtener información del archivo en fs1\n",
    "\n",
    "    with fs1.open_input_file(path) as instream, \\\n",
    "         fs2.open_output_stream(path2) as outstream:\n",
    "        # Mover el puntero de instream a la mitad del archivo\n",
    "        instream.seek(f_info.size // 2)\n",
    "        # Copiar el resto del archivo en outstream\n",
    "        shutil.copyfileobj(instream, outstream)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Si no se proporcionan dos argumentos, se muestra un mensaje de error\n",
    "    if len(sys.argv) != 3:\n",
    "        print(\"Uso: {} <uri1> <uri2>\".format(sys.argv[0]))\n",
    "        sys.exit(1)\n",
    "\n",
    "    copy_half_file(sys.argv[1], sys.argv[2])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos un arhcivo .py que contenga el código anterior y lo movemos a hdfs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#! /usr/bin/env python3\n",
      "from pyarrow.fs import FileSystem, FileInfo\n",
      "import sys\n",
      "import shutil\n",
      "\n",
      "def copy_half_file(uri1: str, uri2: str) -> None:\n",
      "    fs1, path = FileSystem.from_uri(uri1)\n",
      "    fs2, path2 = FileSystem.from_uri(uri2)\n",
      "\n",
      "    f_info: FileInfo = fs1.get_file_info(path)  # Obtener información del archivo en fs1\n",
      "\n",
      "    with fs1.open_input_file(path) as instream,          fs2.open_output_stream(path2) as outstream:\n",
      "        # Mover el puntero de instream a la mitad del archivo\n",
      "        instream.seek(f_info.size // 2)\n",
      "        # Copiar el resto del archivo en outstream\n",
      "        shutil.copyfileobj(instream, outstream)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    # Si no se proporcionan dos argumentos, se muestra un mensaje de error\n",
      "    if len(sys.argv) != 3:\n",
      "        print(\"Uso: {} <uri1> <uri2>\".format(sys.argv[0]))\n",
      "        sys.exit(1)\n",
      "\n",
      "    copy_half_file(sys.argv[1], sys.argv[2])\n",
      "    \n",
      "    "
     ]
    }
   ],
   "source": [
    "\n",
    "# Creamos el archivo .py con el contenido anterior en un directorio con permisos de escritura\n",
    "with open(\"/home/luser/copy_half_file.py\", \"w\") as f:\n",
    "    f.write(\"\"\"\n",
    "#! /usr/bin/env python3\n",
    "from pyarrow.fs import FileSystem, FileInfo\n",
    "import sys\n",
    "import shutil\n",
    "\n",
    "def copy_half_file(uri1: str, uri2: str) -> None:\n",
    "    fs1, path = FileSystem.from_uri(uri1)\n",
    "    fs2, path2 = FileSystem.from_uri(uri2)\n",
    "\n",
    "    f_info: FileInfo = fs1.get_file_info(path)  # Obtener información del archivo en fs1\n",
    "\n",
    "    with fs1.open_input_file(path) as instream, \\\n",
    "         fs2.open_output_stream(path2) as outstream:\n",
    "        # Mover el puntero de instream a la mitad del archivo\n",
    "        instream.seek(f_info.size // 2)\n",
    "        # Copiar el resto del archivo en outstream\n",
    "        shutil.copyfileobj(instream, outstream)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Si no se proporcionan dos argumentos, se muestra un mensaje de error\n",
    "    if len(sys.argv) != 3:\n",
    "        print(\"Uso: {} <uri1> <uri2>\".format(sys.argv[0]))\n",
    "        sys.exit(1)\n",
    "\n",
    "    copy_half_file(sys.argv[1], sys.argv[2])\n",
    "    \n",
    "    \"\"\")\n",
    "    \n",
    "# Enseñamos el contenido del archivo con bash\n",
    "!cat /home/luser/copy_half_file.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecutamos el script con los siguientes argumentos:\n",
    "\n",
    "- `hdfs:///user/luser/test.txt`\n",
    "- `hdfs:///user/luser/test_half.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-01 16:55:22,897 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2025-01-01 16:55:27,207 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "na prueba de una linea de un archivo de texto.\n",
      "\n",
      "FIN.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "python3 copy_half_file.py hdfs://namenode:9000/user/luser/test.txt hdfs://namenode:9000/user/luser/test_half.txt\n",
    "python3 filesystem_cat.py hdfs://namenode:9000/user/luser/test_half.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probar el comando hdfs dfsadmin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora habría que desconectarse del servidor de jupyter y conectarse al servidor de jupyter de hdadmin. Los pasos serían los mismos pero con hdadmin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprobamos que estamos en el directorio de trabajo `/opt/bd`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/bd\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "drwxr-xr-x   - hdadmin supergroup          0 2025-01-01 16:59 /user/hdadmin/limited\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Creamos un directorio en HDFS\n",
    "hdfs dfs -mkdir -p /user/hdadmin/limited\n",
    "\n",
    "# Comprobamos que se ha creado\n",
    "hdfs dfs -ls /user/hdadmin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limitamos la cuota de archivos del directorio `/user/hdadmin/limited` a 4 archivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           4              -1            none             inf            1            4                424 /user/hdadmin/limited\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Limitamos la cuota de ficheros en el directorio\n",
    "hdfs dfsadmin -setQuota 4 /user/hdadmin/limited\n",
    "\n",
    "# Comprobamos que se ha limitado\n",
    "hdfs dfs -count -q /user/hdadmin/limited"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copiamos cinco archivos de prueba al directorio `/user/hdadmin/limited` para comprobar que no se pueden copiar más de 4 archivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cp: The NameSpace quota (directories and files) of directory /user/hdadmin/limited is exceeded: quota=4 file count=5\n",
      "cp: The NameSpace quota (directories and files) of directory /user/hdadmin/limited is exceeded: quota=4 file count=5\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'b'\\n# Copiamos el fichero /user/luser/test.txt en el directorio /user/hdadmin/limited cinco veces\\nhdfs dfs -cp -f /user/luser/test.txt /user/hdadmin/limited/test1.txt\\nhdfs dfs -cp -f /user/luser/test.txt /user/hdadmin/limited/test2.txt\\nhdfs dfs -cp -f /user/luser/test.txt /user/hdadmin/limited/test3.txt\\nhdfs dfs -cp -f /user/luser/test.txt /user/hdadmin/limited/test4.txt\\nhdfs dfs -cp -f /user/luser/test.txt /user/hdadmin/limited/test5.txt\\n'' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_cell_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbash\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m# Copiamos el fichero /user/luser/test.txt en el directorio /user/hdadmin/limited cinco veces\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mhdfs dfs -cp -f /user/luser/test.txt /user/hdadmin/limited/test1.txt\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mhdfs dfs -cp -f /user/luser/test.txt /user/hdadmin/limited/test2.txt\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mhdfs dfs -cp -f /user/luser/test.txt /user/hdadmin/limited/test3.txt\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mhdfs dfs -cp -f /user/luser/test.txt /user/hdadmin/limited/test4.txt\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mhdfs dfs -cp -f /user/luser/test.txt /user/hdadmin/limited/test5.txt\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py:2541\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2539\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[1;32m   2540\u001b[0m     args \u001b[38;5;241m=\u001b[39m (magic_arg_s, cell)\n\u001b[0;32m-> 2541\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2543\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2544\u001b[0m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2545\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2546\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/IPython/core/magics/script.py:155\u001b[0m, in \u001b[0;36mScriptMagics._make_script_magic.<locals>.named_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    154\u001b[0m     line \u001b[38;5;241m=\u001b[39m script\n\u001b[0;32m--> 155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshebang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/IPython/core/magics/script.py:315\u001b[0m, in \u001b[0;36mScriptMagics.shebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mraise_error \u001b[38;5;129;01mand\u001b[39;00m p\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;66;03m# If we get here and p.returncode is still None, we must have\u001b[39;00m\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;66;03m# killed it but not yet seen its return code. We don't wait for it,\u001b[39;00m\n\u001b[1;32m    313\u001b[0m     \u001b[38;5;66;03m# in case it's stuck in uninterruptible sleep. -9 = SIGKILL\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     rc \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m9\u001b[39m\n\u001b[0;32m--> 315\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(rc, cell)\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'\\n# Copiamos el fichero /user/luser/test.txt en el directorio /user/hdadmin/limited cinco veces\\nhdfs dfs -cp -f /user/luser/test.txt /user/hdadmin/limited/test1.txt\\nhdfs dfs -cp -f /user/luser/test.txt /user/hdadmin/limited/test2.txt\\nhdfs dfs -cp -f /user/luser/test.txt /user/hdadmin/limited/test3.txt\\nhdfs dfs -cp -f /user/luser/test.txt /user/hdadmin/limited/test4.txt\\nhdfs dfs -cp -f /user/luser/test.txt /user/hdadmin/limited/test5.txt\\n'' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Copiamos el fichero /user/luser/test.txt en el directorio /user/hdadmin/limited cinco veces\n",
    "hdfs dfs -cp -f /user/luser/test.txt /user/hdadmin/limited/test1.txt\n",
    "hdfs dfs -cp -f /user/luser/test.txt /user/hdadmin/limited/test2.txt\n",
    "hdfs dfs -cp -f /user/luser/test.txt /user/hdadmin/limited/test3.txt\n",
    "hdfs dfs -cp -f /user/luser/test.txt /user/hdadmin/limited/test4.txt\n",
    "hdfs dfs -cp -f /user/luser/test.txt /user/hdadmin/limited/test5.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 items\n",
      "-rw-r--r--   3 hdadmin supergroup        106 2025-01-01 17:32 /user/hdadmin/limited/test1.txt\n",
      "-rw-r--r--   3 hdadmin supergroup        106 2025-01-01 17:32 /user/hdadmin/limited/test2.txt\n",
      "-rw-r--r--   3 hdadmin supergroup        106 2025-01-01 17:32 /user/hdadmin/limited/test3.txt\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /user/hdadmin/limited"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comportanmiento:**\n",
    "\n",
    "El comando hdfs dfsadmin `bash -setQuota 4 /user/hdadmin/limited` establece una cuota de 4 para el directorio `/user/hdadmin/limited`. Sin embargo, esta cuota no se refiere al número de archivos, sino al número de nodos. En HDFS, cada archivo y cada directorio consume un nodo. Por lo tanto, si se tiene una cuota de 4 nodos, se pueden tener combinaciones de archivos y subdirectorios que sumen hasta 4 nodos en total.\n",
    "\n",
    "Parece ser que el propio directorio `/user/hdadmin/limited` ya consume un nodo, por lo que solo se pueden tener 3 archivos en ese directorio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probar el comando hdfs fsck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to namenode via http://namenode:9870/fsck?ugi=hdadmin&path=%2F\n",
      "FSCK started by hdadmin (auth:SIMPLE) from /172.18.0.2 for path / at Wed Jan 01 17:39:08 CET 2025\n",
      "\n",
      "\n",
      "Status: HEALTHY\n",
      " Number of data-nodes:\t4\n",
      " Number of racks:\t\t3\n",
      " Total dirs:\t\t\t42\n",
      " Total symlinks:\t\t0\n",
      "\n",
      "Replicated Blocks:\n",
      " Total size:\t840850759 B\n",
      " Total files:\t45\n",
      " Total blocks (validated):\t55 (avg. block size 15288195 B)\n",
      " Minimally replicated blocks:\t55 (100.0 %)\n",
      " Over-replicated blocks:\t0 (0.0 %)\n",
      " Under-replicated blocks:\t0 (0.0 %)\n",
      " Mis-replicated blocks:\t\t0 (0.0 %)\n",
      " Default replication factor:\t3\n",
      " Average block replication:\t3.0\n",
      " Missing blocks:\t\t0\n",
      " Corrupt blocks:\t\t0\n",
      " Missing replicas:\t\t0 (0.0 %)\n",
      " Blocks queued for replication:\t0\n",
      "\n",
      "Erasure Coded Block Groups:\n",
      " Total size:\t0 B\n",
      " Total files:\t0\n",
      " Total block groups (validated):\t0\n",
      " Minimally erasure-coded block groups:\t0\n",
      " Over-erasure-coded block groups:\t0\n",
      " Under-erasure-coded block groups:\t0\n",
      " Unsatisfactory placement block groups:\t0\n",
      " Average block group size:\t0.0\n",
      " Missing block groups:\t\t0\n",
      " Corrupt block groups:\t\t0\n",
      " Missing internal blocks:\t0\n",
      " Blocks queued for replication:\t0\n",
      "FSCK ended at Wed Jan 01 17:39:08 CET 2025 in 20 milliseconds\n",
      "\n",
      "\n",
      "The filesystem under path '/' is HEALTHY\n"
     ]
    }
   ],
   "source": [
    "!hdfs fsck /"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apagamos manualmente dos datanodes (datanode2 y datanode3) para simular un fallo en el sistema. Una vez apagados, tras unos 10 minutos comprobamos que no estén activos con `hdfs dfsadmin -report`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured Capacity: 2162202353664 (1.97 TB)\n",
      "Present Capacity: 2025699453797 (1.84 TB)\n",
      "DFS Remaining: 2024067268608 (1.84 TB)\n",
      "DFS Used: 1632185189 (1.52 GB)\n",
      "DFS Used%: 0.08%\n",
      "Replicated Blocks:\n",
      "\tUnder replicated blocks: 55\n",
      "\tBlocks with corrupt replicas: 0\n",
      "\tMissing blocks: 0\n",
      "\tMissing blocks (with replication factor 1): 0\n",
      "\tLow redundancy blocks with highest priority to recover: 2\n",
      "\tPending deletion blocks: 0\n",
      "Erasure Coded Block Groups: \n",
      "\tLow redundancy block groups: 0\n",
      "\tBlock groups with corrupt internal blocks: 0\n",
      "\tMissing block groups: 0\n",
      "\tLow redundancy blocks with highest priority to recover: 0\n",
      "\tPending deletion blocks: 0\n",
      "\n",
      "-------------------------------------------------\n",
      "Live datanodes (2):\n",
      "\n",
      "Name: 172.18.0.3:9866 (datanode1.hadoop-cluster)\n",
      "Hostname: datanode1\n",
      "Rack: /rack2\n",
      "Decommission Status : Normal\n",
      "Configured Capacity: 1081101176832 (1006.85 GB)\n",
      "DFS Used: 847691873 (808.42 MB)\n",
      "Non DFS Used: 13227495327 (12.32 GB)\n",
      "DFS Remaining: 1012033634304 (942.53 GB)\n",
      "DFS Used%: 0.08%\n",
      "DFS Remaining%: 93.61%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 0\n",
      "Last contact: Wed Jan 01 17:59:50 CET 2025\n",
      "Last Block Report: Wed Jan 01 15:44:27 CET 2025\n",
      "Num of Blocks: 55\n",
      "\n",
      "\n",
      "Name: 172.18.0.6:9866 (datanode5.hadoop-cluster)\n",
      "Hostname: datanode5\n",
      "Decommission Status : Normal\n",
      "Configured Capacity: 1081101176832 (1006.85 GB)\n",
      "DFS Used: 784493316 (748.15 MB)\n",
      "Non DFS Used: 13290693884 (12.38 GB)\n",
      "DFS Remaining: 1012033634304 (942.53 GB)\n",
      "DFS Used%: 0.07%\n",
      "DFS Remaining%: 93.61%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 0\n",
      "Last contact: Wed Jan 01 17:59:48 CET 2025\n",
      "Last Block Report: Wed Jan 01 15:44:27 CET 2025\n",
      "Num of Blocks: 53\n",
      "\n",
      "\n",
      "Dead datanodes (2):\n",
      "\n",
      "Name: 172.18.0.4:9866 (172.18.0.4)\n",
      "Hostname: datanode2\n",
      "Rack: /rack1\n",
      "Decommission Status : Normal\n",
      "Configured Capacity: 1081101176832 (1006.85 GB)\n",
      "DFS Used: 778539008 (742.47 MB)\n",
      "Non DFS Used: 12785340416 (11.91 GB)\n",
      "DFS Remaining: 1012544942080 (943.01 GB)\n",
      "DFS Used%: 0.07%\n",
      "DFS Remaining%: 93.66%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 0\n",
      "Last contact: Wed Jan 01 17:44:24 CET 2025\n",
      "Last Block Report: Wed Jan 01 17:29:48 CET 2025\n",
      "Num of Blocks: 0\n",
      "\n",
      "\n",
      "Name: 172.18.0.5:9866 (172.18.0.5)\n",
      "Hostname: datanode3\n",
      "Rack: /rack1\n",
      "Decommission Status : Normal\n",
      "Configured Capacity: 1081101176832 (1006.85 GB)\n",
      "DFS Used: 643678208 (613.86 MB)\n",
      "Non DFS Used: 12920201216 (12.03 GB)\n",
      "DFS Remaining: 1012544942080 (943.01 GB)\n",
      "DFS Used%: 0.06%\n",
      "DFS Remaining%: 93.66%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 0\n",
      "Last contact: Wed Jan 01 17:44:25 CET 2025\n",
      "Last Block Report: Wed Jan 01 15:44:27 CET 2025\n",
      "Num of Blocks: 0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfsadmin -report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez vemos que no están activos, volvemos a realizar el comando `hdfs fsck /` para comprobar el estado del sistema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to namenode via http://namenode:9870/fsck?ugi=hdadmin&path=%2F\n",
      " Under-replicated blocks:\t55 (100.0 %)\n"
     ]
    }
   ],
   "source": [
    "!hdfs fsck / | grep -i \"Under-replicated\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que aparecen 55 bloques under-replicated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probamos a hacer un `-get` del fichero `/user/luser/libros/random_words.txt.bz2` para ver si funciona correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hadoop\n",
      "hadoop-3.3.6\n",
      "random_words.txt.bz2\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "hdfs dfs -get -f /user/luser/libros/random_words.txt.bz2\n",
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que los archivos se han copiado correctamente. Esto es debido a que HDFS está diseñado para mantener la disponibilidad de los datos incluso en situaciones de fallo, siempre y cuando exista al menos una réplica de cada bloque."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, vamos a crear un nuevo contenedor que sea `datanode6`, como hicimos con `datanode5` en la práctica 1. Una vez tenemos el contenedor creado, vamos a ver con `hdfs dfsadmin -report` que el nuevo datanode está activo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured Capacity: 3243303530496 (2.95 TB)\n",
      "Present Capacity: 3034918886372 (2.76 TB)\n",
      "DFS Remaining: 3032375943168 (2.76 TB)\n",
      "DFS Used: 2542943204 (2.37 GB)\n",
      "DFS Used%: 0.08%\n",
      "Replicated Blocks:\n",
      "\tUnder replicated blocks: 0\n",
      "\tBlocks with corrupt replicas: 0\n",
      "\tMissing blocks: 0\n",
      "\tMissing blocks (with replication factor 1): 0\n",
      "\tLow redundancy blocks with highest priority to recover: 0\n",
      "\tPending deletion blocks: 0\n",
      "Erasure Coded Block Groups: \n",
      "\tLow redundancy block groups: 0\n",
      "\tBlock groups with corrupt internal blocks: 0\n",
      "\tMissing block groups: 0\n",
      "\tLow redundancy blocks with highest priority to recover: 0\n",
      "\tPending deletion blocks: 0\n",
      "\n",
      "-------------------------------------------------\n",
      "Live datanodes (3):\n",
      "\n",
      "Name: 172.18.0.3:9866 (datanode1.hadoop-cluster)\n",
      "Hostname: datanode1\n",
      "Rack: /rack2\n",
      "Decommission Status : Normal\n",
      "Configured Capacity: 1081101176832 (1006.85 GB)\n",
      "DFS Used: 847736832 (808.46 MB)\n",
      "Non DFS Used: 14469103616 (13.48 GB)\n",
      "DFS Remaining: 1010791981056 (941.37 GB)\n",
      "DFS Used%: 0.08%\n",
      "DFS Remaining%: 93.50%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 0\n",
      "Last contact: Wed Jan 01 18:16:51 CET 2025\n",
      "Last Block Report: Wed Jan 01 15:44:27 CET 2025\n",
      "Num of Blocks: 55\n",
      "\n",
      "\n",
      "Name: 172.18.0.4:9866 (datanode6.hadoop-cluster)\n",
      "Hostname: datanode6\n",
      "Rack: /rack1\n",
      "Decommission Status : Normal\n",
      "Configured Capacity: 1081101176832 (1006.85 GB)\n",
      "DFS Used: 847444964 (808.19 MB)\n",
      "Non DFS Used: 14469395484 (13.48 GB)\n",
      "DFS Remaining: 1010791981056 (941.37 GB)\n",
      "DFS Used%: 0.08%\n",
      "DFS Remaining%: 93.50%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 0\n",
      "Last contact: Wed Jan 01 18:16:54 CET 2025\n",
      "Last Block Report: Wed Jan 01 18:11:21 CET 2025\n",
      "Num of Blocks: 55\n",
      "\n",
      "\n",
      "Name: 172.18.0.6:9866 (datanode5.hadoop-cluster)\n",
      "Hostname: datanode5\n",
      "Decommission Status : Normal\n",
      "Configured Capacity: 1081101176832 (1006.85 GB)\n",
      "DFS Used: 847761408 (808.49 MB)\n",
      "Non DFS Used: 14469079040 (13.48 GB)\n",
      "DFS Remaining: 1010791981056 (941.37 GB)\n",
      "DFS Used%: 0.08%\n",
      "DFS Remaining%: 93.50%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 0\n",
      "Last contact: Wed Jan 01 18:16:53 CET 2025\n",
      "Last Block Report: Wed Jan 01 15:44:27 CET 2025\n",
      "Num of Blocks: 55\n",
      "\n",
      "\n",
      "Dead datanodes (1):\n",
      "\n",
      "Name: 172.18.0.5:9866 (172.18.0.5)\n",
      "Hostname: datanode3\n",
      "Rack: /rack1\n",
      "Decommission Status : Normal\n",
      "Configured Capacity: 1081101176832 (1006.85 GB)\n",
      "DFS Used: 643678208 (613.86 MB)\n",
      "Non DFS Used: 12920201216 (12.03 GB)\n",
      "DFS Remaining: 1012544942080 (943.01 GB)\n",
      "DFS Used%: 0.06%\n",
      "DFS Remaining%: 93.66%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 0\n",
      "Last contact: Wed Jan 01 17:44:25 CET 2025\n",
      "Last Block Report: Wed Jan 01 15:44:27 CET 2025\n",
      "Num of Blocks: 0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfsadmin -report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, vamos a ver que se han replicado todos los bloques correctamente y ya no hay ningún bloque under-replicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to namenode via http://namenode:9870/fsck?ugi=hdadmin&path=%2F\n",
      " Under-replicated blocks:\t0 (0.0 %)\n"
     ]
    }
   ],
   "source": [
    "!hdfs fsck / | grep -i \"Under-replicated\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
