{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_X8yt0lNDzZ6"
      },
      "source": [
        "# Sesión 6.2 - BERT Finetuning para clasificación de textos\n",
        "\n",
        "En esta segunda parte de la sesión vamos a ver como se puede reentrenar un modelo preentrenado de BERT/RoBERTa para poder conseguir un clasificador de texto como en las prácticas anteriores con el datasetEspañol.csv.\n",
        "\n",
        "**Es importante tener en cuenta que para hacer el finetuning de modelos Transformers como BERT o RoBERTa es muy recomendable tener una GPU para poder reducir de manera drástica el tiempo de procesamiento. Por lo tanto se recomienda cargar este notebook en COLAB**\n",
        "\n",
        "Lo primero que haremos será instalar las librerías de Huggingface transformers y datasets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fyMcbMTB5x_9"
      },
      "outputs": [],
      "source": [
        "# Install libraries\n",
        "!pip3 install transformers datasets torch\n",
        "!pip3 install accelerate evaluate\n",
        "\n",
        "# Descargamos nuestro datasetEspañol.csv\n",
        "!wget --no-check-certificate http://valencia.inf.um.es/valencia-tgine/datasetEspañol.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "cu5GLkgM5-Et"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "\n",
        "# Modelo BERT en español - BETO\n",
        "#path_bert_model = 'dccuchile/bert-base-spanish-wwm-uncased'\n",
        "# Modelo BERT multilingüe\n",
        "#path_bert_model = 'bert-base-multilingual-cased'\n",
        "# Modelo BERTIN basado en RoBERTa\n",
        "#path_bert_model = 'bertin-project/bertin-roberta-base-spanish'\n",
        "# Modelo MarIA basado en RoBERTa\n",
        "path_bert_model = 'PlanTL-GOB-ES/roberta-base-bne'\n",
        "\n",
        "# Modelo \"destilados\" de BERT en español\n",
        "#path_bert_model = 'CenIA/distillbert-base-spanish-uncased'\n",
        "\n",
        "# Modelo AlBERT en español\n",
        "#path_bert_model = 'CenIA/albert-base-spanish'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZe7dGf8EHso"
      },
      "source": [
        "## Apartado 1.1 Cargarmos un modelo de BERT/RoBERTa preentrenado para clasificación binaria\n",
        "\n",
        "A continuación vamos a cargar un modelo BERT/RoBERTa para clasificación binaria utilizando las librería KERAS de TensorFlow.\n",
        "\n",
        "El finetuning y uso de BERT/RoBERTa se puede hacer tanto con Pytorch como con KERAS.\n",
        "\n",
        "A continuación se muestra un ejemplo con KERAS que resulta más gráfico."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HMPbeVv2EWsp"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification, TFAutoModelForSequenceClassification\n",
        "from transformers import AutoTokenizer\n",
        "import torch\n",
        "import json\n",
        "\n",
        "# Cargamos un modelo de BERT preentrenado para clasificación. El número de etiquetas es 2\n",
        "NUM_LABELS = 2\n",
        "# La clase TFAutoModelForSequenceClassification es de Tensorflow y la AutoModelForSequenceClassification es de Pytorch\n",
        "bert_class_model = AutoModelForSequenceClassification.from_pretrained(path_bert_model, num_labels=NUM_LABELS)\n",
        "# Cargamos el Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(path_bert_model)\n",
        "\n",
        "# Probamos a clasificar estas frases\n",
        "textos = ['hay muchos más muertos por covid',\n",
        "          'el número de afectados por covid aumenta',\n",
        "          'vamos a salir de la pandemia',\n",
        "          'ánimo a todos'\n",
        "]\n",
        "\n",
        "# TEST\n",
        "\n",
        "for text in textos:\n",
        "  inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "  with torch.no_grad():\n",
        "    logits = bert_class_model(**inputs).logits\n",
        "  predicted_class_id = logits.argmax().item()\n",
        "  prediction= bert_class_model.config.id2label[predicted_class_id]\n",
        "  print(text,'=>', prediction)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHzu2kFRm2jT"
      },
      "source": [
        "## Apartado 1.2 Realizamos un Finetuning del modelo preentrenado de BERT/RoBERTa en Pytorch\n",
        "\n",
        "Se puede observar por los resultados anteriores que este modelo de BERT/RoBERTA es un modelo general y no está adaptado para hacer clasificación de texto en 'positivo' y 'negativo' y mucho menos para el dominio del estado de alarma.\n",
        "\n",
        "Vamos a cargar el dataset de siempre para poder hacer un finetuning de este modelo para mejorar de manera muy significativa los resultados del mismo.\n",
        "\n",
        "Para el entrenamiento de los modelos de DeepLearning es necesario tener 3 conjuntos de datos: Train, Eval y Test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8L82K4Zs5-h0"
      },
      "outputs": [],
      "source": [
        "import pandas\n",
        "df = pandas.read_csv(\"datasetEspañol.csv\",encoding=\"UTF-8\")\n",
        "\n",
        "p_train = 0.80 # Porcentaje de train.\n",
        "p_eval = 0.20 # Porcentaje de eval.\n",
        "p_test = 0.20 # Porcentaje de test\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df = df.sample(frac=1) #mezclamos el dataset\n",
        "\n",
        "\n",
        "# Ponemos en lower_case en los tweets\n",
        "df.tweet = df.tweet.apply (lambda x: x.lower())\n",
        "\n",
        "\n",
        "# Para poder entrenar es necesario codificar las etiquetas como números. Para eso codificaremos\n",
        "# los negativos con 0 y los positivos con 1\n",
        "df['_label'] = df['label'].apply(lambda x : 1 if x == 'positive' else 0)\n",
        "\n",
        "df_train, df_test = train_test_split (df, test_size = p_test)\n",
        "df_train, df_eval = train_test_split (df_train, test_size = p_eval)\n",
        "\n",
        "print(\"Ejemplos usados para entrenar: \", len(df_train))\n",
        "print(\"Ejemplos usados para evaluar: \", len(df_eval))\n",
        "print(\"Ejemplos usados para test: \", len(df_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKKL_yYOn1R0"
      },
      "source": [
        "A continuación preparamos los conjuntos de entrenamiento, evaluación y test en **Pytorch** para poder hacer el entrenamiento del modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-cgx0i515-gs"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "# Cargamos el modelo para clasificación en Pytorch\n",
        "bert_class_model_pytorch = AutoModelForSequenceClassification.from_pretrained(path_bert_model, num_labels=NUM_LABELS)\n",
        "\n",
        "# Los datasets se preparan dde manera distinta a Tensorflow\n",
        "class TGINEDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "import numpy as np\n",
        "from evaluate import load\n",
        "\n",
        "metric = load(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "batch_train_size = 16\n",
        "batch_val_size = 64\n",
        "\n",
        "# Definimos algunos training arguments como el tamaño del bach_size\n",
        "training_args = transformers.TrainingArguments (\n",
        "  output_dir = './results',\n",
        "  logging_dir = './logs',\n",
        "  per_device_train_batch_size = batch_train_size,\n",
        "  per_device_eval_batch_size = batch_val_size,\n",
        "  report_to=\"none\"  # Desactiva wandb\n",
        ")\n",
        "\n",
        "\n",
        "tokenized_train_dataset = tokenizer (df_train.tweet.tolist (),  truncation=True, padding = True)\n",
        "tokenized_eval_dataset = tokenizer (df_eval.tweet.tolist (), truncation=True, padding = True)\n",
        "tokenized_test_dataset = tokenizer (df_test.tweet.tolist (), truncation=True, padding = True)\n",
        "\n",
        "\n",
        "# Como antes, las etiquetas deben ser numéricas para poder entrenar.\n",
        "# Preparamos los 3 datasets para hacer el finetuning\n",
        "train_dataset = TGINEDataset (tokenized_train_dataset, df_train._label.tolist())\n",
        "eval_dataset = TGINEDataset (tokenized_eval_dataset, df_eval._label.tolist())\n",
        "test_dataset = TGINEDataset (tokenized_test_dataset, df_test._label.tolist())\n",
        "\n",
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer (\n",
        "    model = bert_class_model_pytorch,\n",
        "    args = training_args,\n",
        "    train_dataset = train_dataset,\n",
        "    eval_dataset = eval_dataset,\n",
        "    compute_metrics = compute_metrics,\n",
        ")\n",
        "trainer.train()\n",
        "\n",
        "\n",
        "print (\"PREDICCIONES SOBRE EVAL\")\n",
        "bert_class_model_pytorch.eval ()\n",
        "print (json.dumps (trainer.evaluate (), indent = 2))\n",
        "\n",
        "\n",
        "# Salvamos el modelo reentrenado\n",
        "modelo ='modeloReentrenadoPytorch'\n",
        "bert_class_model_pytorch.save_pretrained (modelo)\n",
        "tokenizer.save_pretrained (modelo)\n",
        "\n",
        "print (\"PREDICCIONES SOBRE TEST\")\n",
        "predictions = trainer.predict (test_dataset)\n",
        "print(json.dumps(predictions.metrics, indent = 2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52gGcxcm2Xem"
      },
      "source": [
        "Volvemos a clasificar las frases de antes y vemos que dan unos resultados mucho mejores con el entrenamiento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jOd8V2x32eGM"
      },
      "outputs": [],
      "source": [
        "# Cargamos el modelo ya con el FineTuning hecho\n",
        "bert_class_model = AutoModelForSequenceClassification.from_pretrained(modelo, num_labels=NUM_LABELS)\n",
        "# Cargamos el Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(modelo)\n",
        "\n",
        "# Probamos a clasificar estas frases\n",
        "textos = ['hay muchos más muertos por covid',\n",
        "          'el número de afectados por covid aumenta',\n",
        "          'vamos a salir de la pandemia',\n",
        "          'ánimo a todos'\n",
        "]\n",
        "\n",
        "# TEST\n",
        "\n",
        "for text in textos:\n",
        "  inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "  with torch.no_grad():\n",
        "    logits = bert_class_model(**inputs).logits\n",
        "  predicted_class_id = logits.argmax().item()\n",
        "  prediction= bert_class_model.config.id2label[predicted_class_id]\n",
        "  print(text,'=>', predicted_class_id, '=>', prediction, \"  \", logits.softmax(1))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jJG2oZPutEb"
      },
      "source": [
        "## Apartado 1.3 Compatibilidad entre modelos Pytorch y Tensorflow\n",
        "\n",
        "Se pueden cargar modelos de Pytorch en Tensorflow y viceversa. A continuación cargamos el modelo entrenado en Pytorch para evaluar las frases de los textos de antes y ver su clasificación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O5sksQhvU21P"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Probar a inferir nuevas frases\n",
        "textos = ['hay muchos más muertos por covid',\n",
        "          'el número de afectados por covid aumenta',\n",
        "          'vamos a salir de la pandemia',\n",
        "          'ánimo a todos'\n",
        "]\n",
        "\n",
        "\n",
        "tf_model = TFAutoModelForSequenceClassification.from_pretrained('modeloReentrenadoPytorch')\n",
        "\n",
        "# Imprimimos las predicciones obtenidas\n",
        "for text in textos:\n",
        "  predict_input = tokenizer.encode (text, truncation=True, padding=True, return_tensors=\"tf\")\n",
        "  tf_output = tf_model.predict(predict_input)[0]\n",
        "  tf_prediction = tf.nn.softmax(tf_output, axis=1).numpy()[0]\n",
        "  print(text,'=>', tf_prediction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZFc1rRsLtSs"
      },
      "source": [
        "## Apartado 1.4 Probamos a hacer Finetunning de un modelo \"destilado\" de BERT\n",
        "\n",
        "En el siguiente bloque de código hacemos el mismo entrenamiento en Pytorch pero con el modelo 'CenIA/distillbert-base-spanish-uncased' que es más pequeño y rápido de entrenar\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AjbBAU2MLtSt"
      },
      "outputs": [],
      "source": [
        "path_distilbert_model = 'CenIA/distillbert-base-spanish-uncased'\n",
        "\n",
        "# Cargamos el modelo para clasificación en Pytorch\n",
        "distilbert_class_model_pytorch = AutoModelForSequenceClassification.from_pretrained(path_distilbert_model, num_labels=NUM_LABELS)\n",
        "# Cargamos el tokenizer de este modelo\n",
        "distilbert_tokenizer = AutoTokenizer.from_pretrained(path_distilbert_model)\n",
        "\n",
        "# Definimos algunos training arguments como el tamaño del bach_size\n",
        "training_args = transformers.TrainingArguments (\n",
        "  output_dir = './results_distilbert',\n",
        "  logging_dir = './logs_distilbert',\n",
        "  per_device_train_batch_size = batch_train_size,\n",
        "  per_device_eval_batch_size = batch_val_size,\n",
        "  report_to=\"none\"  # Desactiva wandb\n",
        ")\n",
        "\n",
        "\n",
        "tokenized_train_dataset = distilbert_tokenizer (df_train.tweet.tolist (),  truncation=True, padding = True)\n",
        "tokenized_eval_dataset = distilbert_tokenizer (df_eval.tweet.tolist (), truncation=True, padding = True)\n",
        "tokenized_test_dataset = distilbert_tokenizer (df_test.tweet.tolist (), truncation=True, padding = True)\n",
        "\n",
        "\n",
        "# Como antes, las etiquetas deben ser numéricas para poder entrenar.\n",
        "# Preparamos los 3 datasets para hacer el finetuning\n",
        "train_dataset = TGINEDataset (tokenized_train_dataset, df_train._label.tolist())\n",
        "eval_dataset = TGINEDataset (tokenized_eval_dataset, df_eval._label.tolist())\n",
        "test_dataset = TGINEDataset (tokenized_test_dataset, df_test._label.tolist())\n",
        "\n",
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer (\n",
        "    model = distilbert_class_model_pytorch,\n",
        "    args = training_args,\n",
        "    train_dataset = train_dataset,\n",
        "    eval_dataset = eval_dataset,\n",
        "    compute_metrics = compute_metrics,\n",
        ")\n",
        "trainer.train()\n",
        "\n",
        "\n",
        "print (\"PREDICCIONES SOBRE EVAL\")\n",
        "distilbert_class_model_pytorch.eval ()\n",
        "print (json.dumps (trainer.evaluate (), indent = 2))\n",
        "\n",
        "\n",
        "# Salvamos el modelo reentrenado\n",
        "modelo ='modeloReentrenadoPytorchDistilbert'\n",
        "distilbert_class_model_pytorch.save_pretrained (modelo)\n",
        "distilbert_tokenizer.save_pretrained (modelo)\n",
        "\n",
        "print (\"PREDICCIONES SOBRE TEST\")\n",
        "predictions = trainer.predict (test_dataset)\n",
        "print(json.dumps(predictions.metrics, indent = 2))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mover el modelo a la GPU\n",
        "distilbert_class_model_pytorch = distilbert_class_model_pytorch.to(\"cuda\")\n",
        "\n",
        "# Probamos a clasificar estas frases\n",
        "textos = ['hay muchos más muertos por covid',\n",
        "          'el número de afectados por covid aumenta',\n",
        "          'vamos a salir de la pandemia',\n",
        "          'ánimo a todos'\n",
        "]\n",
        "\n",
        "# TEST\n",
        "\n",
        "for text in textos:\n",
        "  # Movemos los tensores del tokenizer a la GPU\n",
        "  inputs = distilbert_tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
        "  with torch.no_grad():\n",
        "    logits = distilbert_class_model_pytorch(**inputs).logits\n",
        "  predicted_class_id = logits.argmax().item()\n",
        "  prediction= distilbert_class_model_pytorch.config.id2label[predicted_class_id]\n",
        "  print(text,'=>', predicted_class_id, '=>', prediction, \"  \", logits.softmax(1))"
      ],
      "metadata": {
        "id": "1KWz4UEgNoNK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "oX797URvNnrV"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}