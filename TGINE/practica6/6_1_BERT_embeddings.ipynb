{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vbiR19aO-MWZ"
   },
   "source": [
    "# Sesión 6.1 BERT/RoBERTa y contextual word embeddings\n",
    "\n",
    "En esta sesión se muestra un ejemplo de funcionamiento de los word embeddings contextuales de los Transformers como BERT o RoBERTa.\n",
    "\n",
    "Primero instalaremos la libería de transformers y de datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bQFUC88UmGvO"
   },
   "outputs": [],
   "source": [
    "# Install libraries\n",
    "!pip3 install -U transformers datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "37DVYuWWgdj9"
   },
   "source": [
    "## Apartado 1.1 Descargamos el modelo de BERT preentrenado\n",
    "\n",
    "Descargamos un modelo de BERT preentrenado como BETO o mBERT.\n",
    "\n",
    "Cada word embedding de BERT está representado por vectores de 768 características para los modelos *base* y 1024 en los modelos *large*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZcSm8KH4mWxa"
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "# Modelo de BETO\n",
    "#path_bert_model = 'dccuchile/bert-base-spanish-wwm-uncased'\n",
    "# Modelo de multilingual BERT\n",
    "#path_bert_model = 'bert-base-multilingual-cased'\n",
    "# Modelo de distilbert en español\n",
    "path_bert_model = 'CenIA/distillbert-base-spanish-uncased'\n",
    "# Modelo de RoBERTa de MarIA\n",
    "path_roberta_model = 'PlanTL-GOB-ES/roberta-base-bne'\n",
    "\n",
    "# Obtenemos el tokenizer y el modelo de BERT\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained (path_bert_model)\n",
    "bert_model = transformers.AutoModel.from_pretrained (path_bert_model, output_hidden_states=True)\n",
    "\n",
    "# Obtenemos el tokenizer y el modelo de RoBERTa\n",
    "roberta_tokenizer = transformers.AutoTokenizer.from_pretrained (path_roberta_model)\n",
    "roberta_model = transformers.AutoModel.from_pretrained (path_roberta_model, output_hidden_states=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TACS9rziFefj"
   },
   "source": [
    "## Apartado 1.2 WordPiece tokenizer de BERT\n",
    "\n",
    "BERT usa lo que se llama un WordPieceTokenizer que trabaja dividiendo palabras en distintos \"trozos\" o tokens para tener en cuenta las inflexiones de las palabras como sufijos y conjugaciones verbales.\n",
    "\n",
    "Los tokens especiales que utiliza BERT son los siguientes:\n",
    "* [UNK] – The unknown token. Un token que no está en el vocabulario que no puede convertirse a ningún id y se usa este token para eso.\n",
    "* [SEP] – The separator token. Cuando se quieren separar secuencias de texto para tareas de clasificación ode pregunta-respuesta.\n",
    "* [PAD] – The token used for padding. El token que se utiliza de relleno para textos de distinto tamaño.\n",
    "* [CLS] – The classifier token. Se usa para la clasificación del fragmento de texto. Es el primer token del texto.\n",
    "* [MASK] – The token used for masking values. Este token se utiliza cuando se quiere entrenar con modelado del lenguaje con mask. Este token es el qeu se quiere predecir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sWCiOTdKFdr0"
   },
   "outputs": [],
   "source": [
    "text = \"Estudiaré la asignatura de tecnologías de gestión de información no estructurada y terminaré las prácticas en casa.\"\n",
    "tokenizer.tokenize(text, add_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MnILM7o4ggqM"
   },
   "source": [
    "##Apartado 1.3 BPE tokenizer de RoBERTA\n",
    "Los modelos RoBERTa por otro lado utilizan un tokenizer llamad BPE (Byte-Pair Encoding).\n",
    "Los tokens especiales que utiliza RoBERTa son los siguientes:\n",
    "* \\<s> Para indicar el inicio de la frase\n",
    "* \\</s> Para indicar el fin de la frase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NV_s6bPmgfVI"
   },
   "outputs": [],
   "source": [
    "text = \"Estudiaré la asignatura de tecnologías de gestión de información no estructurada y terminaré las prácticas en casa.\"\n",
    "roberta_tokenizer.tokenize(text, add_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3amQuFl1Rc7u"
   },
   "source": [
    "## Apartado 1.4 Word embeddings contextuales de BERT\n",
    "\n",
    "BERT tiene distintas capas ocultas dependiendo del modelo *base* o *large* preentrenado. En los modelos *base* el número de capas son 12 y en los modelos *large* este número es 24 que junto con la capa de entrada forman 13 y 25 capas respectivamente.\n",
    "\n",
    "Para cada token de nuestra frase tendremos entonces 13 capas de vectores de 768 características.\n",
    "\n",
    "Los embeddings de cada Token son distintos y dependen del contexto donde aparezcan en el texto. En el código siguiente se muestra un ejemplo de distintos vectores de la misma palabra \"naranja\" según su contexto. Para el cálculo del word embedding de la palabra utilizamos las 4 últimas capas y sumamos sus valores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "cQCR302vTF9N"
   },
   "outputs": [],
   "source": [
    " import numpy as np\n",
    " import torch\n",
    " from scipy import spatial\n",
    "\n",
    " from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    " def get_hidden_states(encoded, token_ids_word, model):\n",
    "     \"\"\"Push input IDs through model. Stack and sum `layers` (last four by default).\n",
    "        Select only those subword token outputs that belong to our word of interest\n",
    "        and average them.\"\"\"\n",
    "     with torch.no_grad():\n",
    "         output = model(**encoded)\n",
    "     # Get all hidden states\n",
    "     states = output.hidden_states\n",
    "     # Stack and sum the four last layers\n",
    "     output = torch.stack(states[-4:]).sum(0).squeeze()\n",
    "     # Only select the tokens that constitute the requested word\n",
    "     word_tokens_output = output[token_ids_word]\n",
    "\n",
    "     return word_tokens_output.mean(dim=0)\n",
    "\n",
    " # Esta función obtiene el vector del token que se encuentra en la posición idx de la frase.\n",
    " def get_word_vector(sent, idx, tokenizer, model):\n",
    "     \"\"\"Get a word vector by first tokenizing the input sentence, getting all token idxs\n",
    "        that make up the word of interest, and then `get_hidden_states`.\"\"\"\n",
    "     encoded = tokenizer.encode_plus(sent, return_tensors=\"pt\", add_special_tokens=False)\n",
    "     # get all token idxs that belong to the word of interest\n",
    "     token_ids_word = np.where(np.array(encoded.word_ids()) == idx)\n",
    "\n",
    "     return get_hidden_states(encoded, token_ids_word, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GoxlpnZEyhlb"
   },
   "outputs": [],
   "source": [
    "text1 = \"La camiseta que tengo es de color naranja y hace que se resalten mis ojos.\"\n",
    "token = 'naranja'\n",
    "# Obtenemos el id del token\n",
    "idx1=tokenizer.tokenize(text1).index(token)\n",
    "\n",
    "# Obtenemos la representación vectorial de token usando la función definida anteriormente\n",
    "word_embedding1 = get_word_vector(text1, idx1, tokenizer, bert_model)\n",
    "# imprimimos ese vector del token\n",
    "print('El vector del token ',token,' es: ', word_embedding1)\n",
    "print('La dimensión del vector es', word_embedding1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "97n5BiqcfTw6"
   },
   "outputs": [],
   "source": [
    "text2 = \"La fruta que más me gusta es la naranja y después el pomelo.\"\n",
    "idx2=tokenizer.tokenize(text2).index(token)\n",
    "word_embedding2 = get_word_vector(text2, idx2, tokenizer, bert_model)\n",
    "\n",
    "text3 = \"Lo que tienes que hacer es comer la naranja y el plátano.\"\n",
    "idx3=tokenizer.tokenize(text3).index(token)\n",
    "word_embedding3 = get_word_vector(text3, idx3, tokenizer, bert_model)\n",
    "\n",
    "text4 = \"Pinta la valla de naranja y el tejado de rojo.\"\n",
    "idx4=tokenizer.tokenize(text4).index(token)\n",
    "word_embedding4 = get_word_vector(text4, idx4, tokenizer, bert_model)\n",
    "\n",
    "# Imprimimos los textos para después ver las similitudes\n",
    "print(\"1: \",text1)\n",
    "print(\"2: \",text2)\n",
    "print(\"3: \",text3)\n",
    "print(\"4: \",text4)\n",
    "\n",
    "# Calculamos las similitudes entre los tokens en los distintos textos\n",
    "similarity1_2 = 1 - spatial.distance.cosine (word_embedding1, word_embedding2)\n",
    "print('La similitud entre el primer y segundo texto para el token ',token,' es:',similarity1_2)\n",
    "similarity1_3 = 1 - spatial.distance.cosine (word_embedding1, word_embedding3)\n",
    "print('La similitud entre el primer y tercer texto para el token ',token,' es:',similarity1_3)\n",
    "similarity1_4 = 1 - spatial.distance.cosine (word_embedding1, word_embedding4)\n",
    "print('La similitud entre el primer y cuarto texto para el token ',token,' es:',similarity1_4)\n",
    "similarity2_3 = 1 - spatial.distance.cosine (word_embedding2, word_embedding3)\n",
    "print('La similitud entre el segundo y tercer texto para el token ',token,' es:',similarity2_3)\n",
    "similarity2_4 = 1 - spatial.distance.cosine (word_embedding2, word_embedding4)\n",
    "print('La similitud entre el segundo y cuarto texto para el token ',token,' es:',similarity2_4)\n",
    "similarity3_4 = 1 - spatial.distance.cosine (word_embedding3, word_embedding4)\n",
    "print('La similitud entre el tercer y cuarto texto para el token ',token,' es:',similarity3_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IR2WfGvoyhUl"
   },
   "source": [
    "Imprimimos los tokens y los ids de los tokens asignados por el tokenizer para el texto 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GAGMZwgKZWhN"
   },
   "outputs": [],
   "source": [
    "tokens = tokenizer.tokenize(text1)\n",
    "print(tokens)\n",
    "encoded = tokenizer.encode_plus(text1, return_tensors=\"pt\", add_special_tokens=False)\n",
    "print(encoded.input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7pByCxa0lpYl"
   },
   "source": [
    "## Apartado 1.4 Word embeddings contextuales de RoBERTA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YIvJlnGvlmQ2"
   },
   "outputs": [],
   "source": [
    "text1 = \"La camiseta que tengo es de color naranja y hace que se resalten mis ojos.\"\n",
    "token = 'Ġnaranja'\n",
    "\n",
    "# Obtenemos el id del token\n",
    "idx1=roberta_tokenizer.tokenize(text1).index(token)\n",
    "\n",
    "# Obtenemos la representación vectorial de token usando la función definida anteriormente\n",
    "word_embedding1 = get_word_vector(text1, idx1, roberta_tokenizer, roberta_model)\n",
    "\n",
    "text2 = \"La fruta que más me gusta es la naranja y después el pomelo.\"\n",
    "idx2=roberta_tokenizer.tokenize(text2).index(token)\n",
    "word_embedding2 = get_word_vector(text2, idx2, roberta_tokenizer, roberta_model)\n",
    "\n",
    "text3 = \"Lo que tienes que hacer es comer la naranja y el plátano.\"\n",
    "idx3=roberta_tokenizer.tokenize(text3).index(token)\n",
    "word_embedding3 = get_word_vector(text3, idx3, roberta_tokenizer, roberta_model)\n",
    "\n",
    "text4 = \"Pinta la valla de naranja y el tejado de rojo.\"\n",
    "idx4=roberta_tokenizer.tokenize(text4).index(token)\n",
    "word_embedding4 = get_word_vector(text4, idx4, roberta_tokenizer, roberta_model)\n",
    "\n",
    "# Imprimimos los textos para después ver las similitudes\n",
    "print(\"1: \",text1)\n",
    "print(\"2: \",text2)\n",
    "print(\"3: \",text3)\n",
    "print(\"4: \",text4)\n",
    "\n",
    "# Calculamos las similitudes entre los tokens en los distintos textos\n",
    "similarity1_2 = 1 - spatial.distance.cosine (word_embedding1, word_embedding2)\n",
    "print('La similitud entre el primer y segundo texto para el token ',token,' es:',similarity1_2)\n",
    "similarity1_3 = 1 - spatial.distance.cosine (word_embedding1, word_embedding3)\n",
    "print('La similitud entre el primer y tercer texto para el token ',token,' es:',similarity1_3)\n",
    "similarity1_4 = 1 - spatial.distance.cosine (word_embedding1, word_embedding4)\n",
    "print('La similitud entre el primer y cuarto texto para el token ',token,' es:',similarity1_4)\n",
    "similarity2_3 = 1 - spatial.distance.cosine (word_embedding2, word_embedding3)\n",
    "print('La similitud entre el segundo y tercer texto para el token ',token,' es:',similarity2_3)\n",
    "similarity2_4 = 1 - spatial.distance.cosine (word_embedding2, word_embedding4)\n",
    "print('La similitud entre el segundo y cuarto texto para el token ',token,' es:',similarity2_4)\n",
    "similarity3_4 = 1 - spatial.distance.cosine (word_embedding3, word_embedding4)\n",
    "print('La similitud entre el tercer y cuarto texto para el token ',token,' es:',similarity3_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jQA-DlPWj72s"
   },
   "source": [
    "## Apartado 1.5 Bert sentence embeddigns\n",
    "Se puede calcular también el vector que representa toda el texto usando los sentence embeddings.\n",
    "\n",
    "Para eso debemos instalar la libraría \"sentence-transformers\" de SBERT.net\n",
    "\n",
    "Aunque se pueden utilizar los modelos de BERT y Roberta preentrenados, hay algunos otros modelos que se han entrenado para mejorar la similitud semántica de esos embeddings (Pretrained Models disponibles en https://www.sbert.net/docs/pretrained_models.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zBR28FaAvGGr"
   },
   "outputs": [],
   "source": [
    "# Instalamos primero la librería necesaria\n",
    "!pip3 install sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DxL72AFSlKA8"
   },
   "source": [
    "Probamos los BERT Sentence embeddins para calcular la similitud ente distintos textos como hicimos en la sesión anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sRGhgqyDNpeo"
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, models\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# Definimos un conjunto de textos\n",
    "textos=['El procesamiento del lenguaje natural (PLN o NLP) es un campo dentro de la inteligencia artificial y la lingüística aplicada que estudia las interacciones mediante uso del lenguaje natural entre los seres humanos y las máquinas. \\\n",
    "Más concretamente se centra en el procesamiento de las comunicaciones humanas, dividiéndolas en partes, e identificando los elementos más relevantes del mensaje.\\\n",
    "Con la Comprensión y Generación de Lenguaje Natural, busca que las máquinas consigan entender, interpretar y manipular el lenguaje humano.'\n",
    ", 'El procesamiento del lenguaje natural (NLP, por sus siglas en inglés) es una rama de la inteligencia artificial que ayuda a las computadoras a entender, interpretar y manipular el lenguaje humano. \\\n",
    "NLP toma elementos prestados de muchas disciplinas, incluyendo la ciencia de la computación y la lingüística computacional, en su afán por cerrar la brecha entre la comunicación humana y el entendimiento de las computadoras.\"\"\"], \"\"\"El procesamiento del lenguaje natural (PLN o NLP) es un campo dentro de la inteligencia artificial y la lingüística aplicada que estudia las interacciones mediante uso del lenguaje natural entre los seres humanos y las máquinas. Más concretamente se centra en el procesamiento de las comunicaciones humanas, dividiéndolas en partes, e identificando los elementos más relevantes del mensaje. Con la Comprensión y Generación de Lenguaje Natural, busca que las máquinas consigan entender, interpretar y manipular el lenguaje humano.'\n",
    ", 'La lingüística computacional es un campo interdisciplinario que se ocupa del desarrollo de formalismos del funcionamiento del lenguaje natural, tales que puedan ser transformados en programas ejecutables para un ordenador. \\\n",
    "Dicho desarrollo se sitúa entre el modelado basado en reglas y el modelado estadístico del lenguaje natural desde una perspectiva computacional, y en él participan lingüistas e informáticos especializados en inteligencia artificial, psicólogos cognoscitivos y expertos en lógica, entre otros.'\n",
    ", 'El aprendizaje automático es un tipo de inteligencia artificial (AI) que proporciona a las computadoras la capacidad de aprender, sin ser programadas explícitamente. El aprendizaje automático se centra en el desarrollo de programas informáticos que pueden cambiar cuando se exponen a nuevos datos.'\n",
    ", 'El  aprendizaje profundo es un tema que cada vez adquiere mayor relevancia en el campo de la inteligencia artificial (IA). Siendo una subcategoría del aprendizaje automático, el aprendizaje profundo trata del uso de redes neuronales para mejorar cosas tales como el reconocimiento de voz, la visión por ordenador y el procesamiento del lenguaje natural. \\\n",
    "Rápidamente se está convirtiendo en uno de los campos más solicitados en informática. \\\n",
    "En los últimos años, el aprendizaje profundo ha ayudado a lograr avances en áreas tan diversas como la percepción de objetos, el procesamiento del lenguaje natural y el reconocimiento de voz (todas ellas áreas especialmente complejas para los investigadores en IA).',\n",
    "'El coste de la energía va a subir mucho los próximos meses y la población va a tener que pagar cantidades excesivas a las eléctricas']\n",
    "\n",
    "# Calculamos la similitud usando sentence embeddings\n",
    "word_embedding_model = models.Transformer(path_bert_model, max_seq_length=768)\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
    "dense_model = models.Dense(in_features=pooling_model.get_sentence_embedding_dimension(), out_features=768, activation_function=nn.Tanh())\n",
    "\n",
    "model = SentenceTransformer(modules=[word_embedding_model, pooling_model, dense_model])\n",
    "\n",
    "embeddings = model.encode(textos)\n",
    "print(embeddings)\n",
    "\n",
    "results = cosine_similarity(embeddings[1::],embeddings[0].reshape(1,-1)).reshape(-1,) # Op -- (n_docs,1) -- Cosine Sim with each doc\n",
    "print('\\n',results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "701rR5yeOOVk"
   },
   "source": [
    "Probamos a entrenar el clasificador de tuits de prácticas anteriores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DATC8GdRi0m_"
   },
   "outputs": [],
   "source": [
    "# Descargamos el fichero de datasetEspañol.csv\n",
    "!wget --no-check-certificate http://valencia.inf.um.es/valencia-tgine/datasetEspañol.csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RFYlUHhYp7lg"
   },
   "source": [
    "Generamos los sentences embeddings de los tuits de entrenamiento y validación.\n",
    "Este proceso tarda mucho si se ejectua en CPU. Por favor, acordaos de usar un entorno con GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "Es3dbYxvOUeL"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "\n",
    "import pandas\n",
    "from sklearn.svm import LinearSVC\n",
    "df = pandas.read_csv(\"datasetEspañol.csv\",encoding=\"UTF-8\")\n",
    "\n",
    "p_train = 0.80 # Porcentaje de train.\n",
    "p_test = 0.20 # Porcentaje de train.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train, df_test = train_test_split(df, test_size = p_test)\n",
    "\n",
    "# Ponemos en lower_case los dos conjuntos de tweets\n",
    "df_train.tweet = df_train.tweet.apply(lambda x: x.lower())\n",
    "df_test.tweet = df_test.tweet.apply(lambda x: x.lower())\n",
    "\n",
    "sentence_train = model.encode(df_train.tweet.tolist())\n",
    "sentence_test = model.encode((df_test.tweet.tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dDqom3oXkemS"
   },
   "source": [
    "Entrenamos el modelo SVM que hemos usado en prácticas anteriores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CWzwoqZlke7r"
   },
   "outputs": [],
   "source": [
    "# Entrenamos el mismo SVN que en sesiones anteriores\n",
    "clf_sentence_embeddings = LinearSVC(random_state=0, tol=1e-5).fit(sentence_train, df_train.label)\n",
    "predicted = clf_sentence_embeddings.predict(sentence_test)\n",
    "\n",
    "accuracy = np.mean(predicted == df_test.label)\n",
    "print(\"Resultados Sentence Embeddings ----- Accuracy:\", accuracy)\n",
    "from sklearn import metrics\n",
    "print(metrics.classification_report(df_test.label, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UUMQz32qpvuw"
   },
   "source": [
    "Probamos algunos ejemplos de inferencia con el modelo ya entrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pR34jbqkpsK4"
   },
   "outputs": [],
   "source": [
    "# Probamos algunos ejemplos con los modelos inferidos\n",
    "textos = ['hay muchos más muertos por covid',\n",
    "          'el número de afectados por covid aumenta',\n",
    "          'vamos a salir de la pandemia',\n",
    "          'ánimo a todos'\n",
    "]\n",
    "# Codificamos estos documentos\n",
    "textos_SE = model.encode(textos)\n",
    "# Predecimos\n",
    "predicted = clf_sentence_embeddings.predict(textos_SE)\n",
    "\n",
    "# Imprimimos los textos y su predicción para TF\n",
    "for doc, category_tf in zip(textos, predicted):\n",
    "  print('TF: %r => %s' % (doc, category_tf))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
