{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrajzBdoGAki"
      },
      "source": [
        "# Sesión 5 - Word embeddings (Wor2vec, Glove, Fasttext) y Doc2Vec con GENSIM\n",
        "En esta sesión se verá cómo utilizar la librería Gensim para el uso de word embeddings.\n",
        "\n",
        "Los word embeddings son vectores de dimensión n que tratan de capturar el significado de la palabra y de su contexto en ese vector.\n",
        "Hay distintos modelos de word embeddings preentrenados con grandes corpus que se pueden descargar y utilizar para distintos idiomas.\n",
        "\n",
        "Primero instalaremos la librería gensim."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aWLFpiTGDgWO"
      },
      "outputs": [],
      "source": [
        "# Instalamos gensim\n",
        "!pip3 install -U gensim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwpkKOFzhlpV"
      },
      "source": [
        "## Apartado 1.1 Descargamos un modelo de word embeddings preentrenado (Resuelto)\n",
        "En la web https://github.com/dccuchile/spanish-word-embeddings existen distintos modelos de Word-embeddings preentrenados para el idioma español para poder utilizarse con la librería GENSIM.\n",
        "\n",
        "Se pueden descargar modelos preentrenados para otros idiomas y también se pueden generar modelos basados en Word2vec si se dispone de un corpus de entrenamiento.\n",
        "\n",
        "Descargamos distintos modelos y los descomprimimos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nRnx9h7whlG4"
      },
      "outputs": [],
      "source": [
        "# Descargamos Glove para español con formato vec para utilizarse con Gensim\n",
        "#!wget --no-check-certificate https://valencia.inf.um.es/valencia-tgine/glove-sbwc.i25.vec.gz\n",
        "#!gzip -d glove-sbwc.i25.vec.gz\n",
        "\n",
        "# Descargamos Fasttext con formato vec para utilizarse con Gensim\n",
        "#!wget --no-check-certificate https://valencia.inf.um.es/valencia-tgine/embeddings-m-model.vec.gz\n",
        "#!gzip -d embeddings-m-model.vec.gz\n",
        "# Descargamos Fasttext con formato binario para utilizarse con la librería Fasttext más adelante en este notebook\n",
        "#!wget --no-check-certificate https://valencia.inf.um.es/valencia-tgine/embeddings-m-model.vec.gz\n",
        "#!gzip -d embeddings-m-model.bin.gz\n",
        "\n",
        "# Esta es una versión muy reducida de fasttext\n",
        "# Descargamos esta versión muy reducida de fasttext desde un ordenador de la Facultad para ir más rápido.\n",
        "# Los enlaces originales son los siguientes\n",
        "# https://zenodo.org/record/3234051/files/embeddings-s-model.vec\n",
        "# https://zenodo.org/record/3234051/files/embeddings-s-model.bin\n",
        "\n",
        "!wget --no-check-certificate https://valencia.inf.um.es/valencia-tgine/embeddings-s-model.vec.gz\n",
        "!gzip -d embeddings-s-model.vec.gz\n",
        "!wget --no-check-certificate https://valencia.inf.um.es/valencia-tgine/embeddings-s-model.bin.gz\n",
        "!gzip -d embeddings-s-model.bin.gz\n",
        "\n",
        "# Descargamos el dataset en español que hemos usado en otras prácticas\n",
        "!wget --no-check-certificate https://valencia.inf.um.es/valencia-tgine/datasetEspañol.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "0as_MOVREyoL"
      },
      "outputs": [],
      "source": [
        " from gensim.models.keyedvectors import KeyedVectors\n",
        " import gensim\n",
        "# Cargamos el modelo de Glove en Gensim\n",
        " #glove_gensim_model = KeyedVectors.load_word2vec_format(\"glove-sbwc.i25.vec\")\n",
        "\n",
        " # Podemos cargar también los embeddings de Fasttext\n",
        " #glove_gensim_model = KeyedVectors.load_word2vec_format(\"embeddings-m-model.vec\")\n",
        " glove_gensim_model = KeyedVectors.load_word2vec_format(\"embeddings-s-model.vec\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTTuZiM6UQeQ"
      },
      "source": [
        "# Apartado 1.2 Trabajando con los vectores de palabras\n",
        "\n",
        "Imprimimos los vectores de la palabra \"murcia\" con el conjunto preentrenado de embeddings cargado en la celda anterior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S417MUK_lIzN"
      },
      "outputs": [],
      "source": [
        "word = 'murcia'\n",
        "if (glove_gensim_model.has_index_for(word)):\n",
        "  word_embedding = glove_gensim_model.get_vector(word)\n",
        "print(word_embedding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jksvuENwVoTl"
      },
      "source": [
        "Con los word embeddings podemos calcular la similitud de distintos términos usando la similidud del coseno. Para eso Gensim proporciona una función para determinar su similitud."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c5l422svFE1l"
      },
      "outputs": [],
      "source": [
        "# Calculamos la similitud entre las palabras alumno y estudiante\n",
        "palabra1=\"alumno\"\n",
        "palabra2=\"estudiante\"\n",
        "sim = glove_gensim_model.similarity(palabra1, palabra2)\n",
        "print('La similitud entre '+palabra1+' y '+palabra2+' es:',sim)\n",
        "\n",
        "# Calculamos la similitud entre 'españa' y 'francia'\n",
        "\n",
        "# Calculamos la similitud entre 'españa' y 'madrid'\n",
        "\n",
        "# Calculamos la similitud entre 'españa' y 'alumno'\n",
        "\n",
        "# Calculamos la similitud entre 'rojo' y 'azul'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCN-rv28XI8D"
      },
      "source": [
        "# Apartado 1.3 Obtenemos términos similares\n",
        "\n",
        "Con los word embeddings podemos obtener los términos más similares con respecto a uno dado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cWcJBbYHXuBI"
      },
      "outputs": [],
      "source": [
        "# Imprimimos las palabras más similares a 'madrid'\n",
        "palabra = 'madrid'\n",
        "print(glove_gensim_model.most_similar(palabra))\n",
        "\n",
        "# Imprimimos las palabras más similares a 'españa'\n",
        "\n",
        "# Imprimimos las palabras más similares a 'tenis'\n",
        "\n",
        "# Imprimimos las palabras más similares a 'amarillo'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_qlrQv3ZgGe"
      },
      "source": [
        "# Apartado 1.4 Haciendo analogías\n",
        "\n",
        "Con las similitudes de word embeddings se pueden hacer analogías haciendo operaciones con los vectores.\n",
        "\n",
        "Por ejemplo, podríamos hacer la siguiente analogía:\n",
        "\n",
        "*francés* es a *Francia* lo que *Italiano* es a ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8v-5rQE_H87d"
      },
      "outputs": [],
      "source": [
        "  # Francés es a Francia lo que Italiano es a ...\n",
        "  print(glove_gensim_model.most_similar(positive=[\"italiano\", \"francia\"], negative=[\"francés\"], topn=1))\n",
        "\n",
        "  # Francia es a París lo que España es a ...\n",
        "\n",
        "  # Hombre es a rey lo que mujer es a ...\n",
        "\n",
        "  # Cantar es a cantaba lo que temer es a ...\n",
        "\n",
        "  # Enfermera es a hospital lo que juez es a ...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1OMyo3rcydV"
      },
      "source": [
        "También se pueden mostrar más de 1 opción y no solamente la primera opción de la analogía."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yp3z7_rFrcJ-"
      },
      "outputs": [],
      "source": [
        "  # Francés es a Francia lo que Italiano es a ...\n",
        "  print(glove_gensim_model.most_similar(positive=[\"italiano\", \"francia\"], negative=[\"francés\"], topn=10))\n",
        "\n",
        "  # Francia es a París lo que España es a ,,,\n",
        "\n",
        "  # Hombre es a rey lo que mujer es a ...\n",
        "\n",
        "  # Cantar es a cantaba lo que temer es a ...\n",
        "\n",
        "  # Enfermera es a hospital lo que juez es a ...\n",
        "\n",
        "  # Enfermera es a hospital lo que profesor es a ...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wme5Mq0FfbQH"
      },
      "source": [
        "## Apartado 1.5 Detectando el término no relacionados\n",
        "\n",
        "También hay una función que permite determinar el término que no está relacionado con los demás términos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PI2WxOn3sBmt"
      },
      "outputs": [],
      "source": [
        "print(glove_gensim_model.doesnt_match(['blanco','azul','rojo','amarillo','verde','calamar']))\n",
        "\n",
        "# Ver qué palabra no encaja en la siguiente lista 'tenis', 'fútbol', 'baloncesto', 'informática', 'gimnasia'\n",
        "print(glove_gensim_model.doesnt_match(['tenis', 'fútbol', 'baloncesto', 'informática', 'gimnasia']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E78IfUFzHpwZ"
      },
      "source": [
        "# Apartado 1.6 Visualización de *vectores de palabras*\n",
        "\n",
        "En el siguiente ejemplo, se puede ver la visualización gráfica en 2D de la distancia entre distintas familias de palabras."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jated7_bHpX9"
      },
      "outputs": [],
      "source": [
        "# Función para pintar las palabras en una gráfica\n",
        "!pip3 install pandas\n",
        "!pip3 install sklearn\n",
        "!pip3 install nltk\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from matplotlib import pyplot as plt\n",
        "def display_wordlist(model, wordlist):\n",
        "    vectors = [model[word] for word in wordlist if model.has_index_for(word)]\n",
        "    word_labels = [word for word in wordlist if model.has_index_for(word)]\n",
        "    word_vec_zip = zip(word_labels, vectors)\n",
        "\n",
        "    # Convert to a dict and then to a DataFrame\n",
        "    word_vec_dict = dict(word_vec_zip)\n",
        "    df = pd.DataFrame.from_dict(word_vec_dict, orient='index')\n",
        "\n",
        "    # Reduces the dimensionality from 300 to 50 dimensions with PCA\n",
        "    reduc = PCA(n_components=len(wordlist)).fit_transform(df)\n",
        "\n",
        "    # Use tsne to reduce to 2 dimensions\n",
        "    tsne = TSNE(perplexity=5,n_components=2, random_state=0)\n",
        "    np.set_printoptions(suppress=True)\n",
        "    Y = tsne.fit_transform(reduc)\n",
        "\n",
        "    x_coords = Y[:, 0]\n",
        "    y_coords = Y[:, 1]\n",
        "    # display plot\n",
        "    plt.figure(figsize=(16, 8))\n",
        "    plt.plot(x_coords, y_coords, 'ro')\n",
        "\n",
        "    for label, x, y in zip(df.index, x_coords, y_coords):\n",
        "        plt.annotate(label, xy=(x, y), xytext=(5, 2), textcoords='offset points')\n",
        "    plt.xlim(Y[:, 0].min()-10, Y[:, 0].max()+10)\n",
        "    plt.ylim(Y[:, 1].min()-10, Y[:, 1].max()+10)\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mwa7LLXVH7L9"
      },
      "outputs": [],
      "source": [
        "# Visualizamos algunas palabras\n",
        "display_wordlist(glove_gensim_model, ['judo', 'karate','baloncesto','tenis','futbol','padel',\n",
        "                                      'hija','esposa','hijos','madre','hermana','hijas','matrimonio','viuda', 'casada',\n",
        "                                      'españa','madrid','francia','parís','italia','roma','alemania','berlín',\n",
        "                                      'azul','verde','rosa','amarillo','rojo','marrón','negro'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqp0Wm7hoM7P"
      },
      "source": [
        "# Apartado 1.7 Sentence embeddings\n",
        "\n",
        "Se pueden representar fragmentos de texto como frases o párrafos a partir de un vector de dimensión 300 simplemente haciendo una media aritmética de los vectores de los Tokens o palabras que aparecen en ese fragmento de texto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0bGOnsLcoMfD"
      },
      "outputs": [],
      "source": [
        "# Esta función calcula en un único vector de dimensión 300 la media aritmética de todos los vectores de tokens\n",
        "# de un array de textos dado.\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "def gensim_sentence_embeddings(textos, model, idiomaStopWords):\n",
        "    import nltk\n",
        "    from nltk.corpus import stopwords\n",
        "    from gensim.test.utils import datapath\n",
        "    import unicodedata\n",
        "    from tqdm import tqdm\n",
        "    import gensim\n",
        "    import multiprocessing\n",
        "    import random\n",
        "    import numpy as np\n",
        "#esta función devuelve los sentence embeddings de todos los textos enviados como parámetros\n",
        "    stopwords_list=stopwords.words(idiomaStopWords)\n",
        "    sentence_embeddings = []\n",
        "    for linea in textos:\n",
        "        line_vec = []\n",
        "        tokenized_sentence = nltk.tokenize.word_tokenize(linea)\n",
        "        count = 0\n",
        "        for token in tokenized_sentence:\n",
        "           if (token not in stopwords_list):\n",
        "            # Solamente calculamos los vectores de lo que no son stopwords\n",
        "            if model.has_index_for(token):\n",
        "              a = model.get_vector(token)\n",
        "              if len(line_vec) == 0 :\n",
        "                line_vec = a\n",
        "              else:\n",
        "                line_vec = line_vec + a\n",
        "              count = count + 1\n",
        "        if(count>0):\n",
        "          sentence_embeddings.append(line_vec/count)\n",
        "        else:\n",
        "          sentence_embeddings.append(np.zeros(300))\n",
        "    return sentence_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qu9fcxh9o1ON"
      },
      "outputs": [],
      "source": [
        "# Podemos calcular e imprimir los sentence embedings de varias frases.\n",
        "# Hay que tener en cuenta que si no existe ningún token conocido en el vocabulario obtendríamos un vector de 0s\n",
        "# Esto no ocurre con los modelos de Fasttext porque usan fragmentos de caracteres, pero sí con los de Glove\n",
        "print(gensim_sentence_embeddings(['me gusta el día','30 203 44 500'], glove_gensim_model, 'spanish'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_MFi4JdjJvS"
      },
      "source": [
        "Podemos probar ahora mediante sentence embeddings cómo pueden parecerse distintos fragmentos de texto o documentos mediante el uso de la similitud del coseno."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOCIIjxao4f7"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "# Definimos un conjunto de textos\n",
        "textos=['El procesamiento del lenguaje natural (PLN o NLP) es un campo dentro de la inteligencia artificial y la lingüística aplicada que estudia las interacciones mediante uso del lenguaje natural entre los seres humanos y las máquinas. \\\n",
        "Más concretamente se centra en el procesamiento de las comunicaciones humanas, dividiéndolas en partes, e identificando los elementos más relevantes del mensaje.\\\n",
        "Con la Comprensión y Generación de Lenguaje Natural, busca que las máquinas consigan entender, interpretar y manipular el lenguaje humano.'\n",
        ", 'El procesamiento del lenguaje natural (NLP, por sus siglas en inglés) es una rama de la inteligencia artificial que ayuda a las computadoras a entender, interpretar y manipular el lenguaje humano. \\\n",
        "NLP toma elementos prestados de muchas disciplinas, incluyendo la ciencia de la computación y la lingüística computacional, en su afán por cerrar la brecha entre la comunicación humana y el entendimiento de las computadoras.\"\"\"], \"\"\"El procesamiento del lenguaje natural (PLN o NLP) es un campo dentro de la inteligencia artificial y la lingüística aplicada que estudia las interacciones mediante uso del lenguaje natural entre los seres humanos y las máquinas. Más concretamente se centra en el procesamiento de las comunicaciones humanas, dividiéndolas en partes, e identificando los elementos más relevantes del mensaje. Con la Comprensión y Generación de Lenguaje Natural, busca que las máquinas consigan entender, interpretar y manipular el lenguaje humano.'\n",
        ", 'La lingüística computacional es un campo interdisciplinario que se ocupa del desarrollo de formalismos del funcionamiento del lenguaje natural, tales que puedan ser transformados en programas ejecutables para un ordenador. \\\n",
        "Dicho desarrollo se sitúa entre el modelado basado en reglas y el modelado estadístico del lenguaje natural desde una perspectiva computacional, y en él participan lingüistas e informáticos especializados en inteligencia artificial, psicólogos cognoscitivos y expertos en lógica, entre otros.'\n",
        ", 'El aprendizaje automático es un tipo de inteligencia artificial (AI) que proporciona a las computadoras la capacidad de aprender, sin ser programadas explícitamente. El aprendizaje automático se centra en el desarrollo de programas informáticos que pueden cambiar cuando se exponen a nuevos datos.'\n",
        ", 'El  aprendizaje profundo es un tema que cada vez adquiere mayor relevancia en el campo de la inteligencia artificial (IA). Siendo una subcategoría del aprendizaje automático, el aprendizaje profundo trata del uso de redes neuronales para mejorar cosas tales como el reconocimiento de voz, la visión por ordenador y el procesamiento del lenguaje natural. \\\n",
        "Rápidamente se está convirtiendo en uno de los campos más solicitados en informática. \\\n",
        "En los últimos años, el aprendizaje profundo ha ayudado a lograr avances en áreas tan diversas como la percepción de objetos, el procesamiento del lenguaje natural y el reconocimiento de voz (todas ellas áreas especialmente complejas para los investigadores en IA).',\n",
        "'El coste de la energía va a subir mucho los próximos meses y la población va a tener que pagar cantidades excesivas a las eléctricas']\n",
        "\n",
        "# Calculamos la similitud usando sentence embeddings\n",
        "sentence_embeddings = gensim_sentence_embeddings(textos, glove_gensim_model,'spanish')\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "if not sp.issparse(sentence_embeddings):\n",
        "            sentence_embeddings = sp.csr_matrix(sentence_embeddings, dtype=np.float64)\n",
        "\n",
        "# Calculamos la similitud de los documentos con el coseno para sentence embeddings\n",
        "results = cosine_similarity(sentence_embeddings[1::],sentence_embeddings[0]).reshape(-1,) # Op -- (n_docs,1) -- Cosine Sim with each doc\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mg8mo87onXZt"
      },
      "source": [
        "Probamos ahora a entrenar el dataset de la sesión anterior *Sesión 4* para probar su accuracy utilizando word embeddings preentrenados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GLOvVhSRtCJn"
      },
      "outputs": [],
      "source": [
        "import pandas\n",
        "df = pandas.read_csv(\"datasetEspañol.csv\",encoding=\"UTF-8\")\n",
        "\n",
        "p_train = 0.80 # Porcentaje de train.\n",
        "p_test = 0.20 # Porcentaje de train.\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df_train, df_test = train_test_split(df, test_size = p_test)\n",
        "\n",
        "# Ponemos en lower_case los dos conjuntos de tweets\n",
        "df_train.tweet = df_train.tweet.apply(lambda x: x.lower())\n",
        "df_test.tweet = df_test.tweet.apply(lambda x: x.lower())\n",
        "\n",
        "print(\"Ejemplos usados para entrenar: \", len(df_train))\n",
        "print(\"Ejemplos usados para test: \", len(df_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i-4qjPWLtDXM"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "sentence_train = gensim_sentence_embeddings(df_train.tweet, glove_gensim_model,'spanish')\n",
        "sentence_test = gensim_sentence_embeddings(df_test.tweet, glove_gensim_model,'spanish')\n",
        "\n",
        "clf_sentence_embeddings = LinearSVC(random_state=0, tol=1e-5).fit(sentence_train, df_train.label)\n",
        "predicted = clf_sentence_embeddings.predict(sentence_test)\n",
        "accuracy = np.mean(predicted == df_test.label)\n",
        "\n",
        "print(\"Resultados ----- Accuracy:\", accuracy)\n",
        "from sklearn import metrics\n",
        "print(metrics.classification_report(df_test.label, predicted))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "br6eDk7ks-2M"
      },
      "source": [
        "## Apartado 1.8 Sentence embeddings de Fasttext\n",
        "\n",
        "Nosotros hemos implementado una función para obtener los sentence embeddings directamente desde la librería Fasttext. Fasttext es otro tipo de word embeddings donde se tienen en cuenta los char n gramas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QD3V4paunwOT"
      },
      "outputs": [],
      "source": [
        "# Instalamos la librería Fasttext y descargamos el modelo preentrenado para el español en binario\n",
        "!pip3 install fasttext\n",
        "import fasttext\n",
        "# Los word embeddings preentrenados ya se han descargado anteriormente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "Fyi-C1ib0Uju"
      },
      "outputs": [],
      "source": [
        "# Cargamos el modelo usando la librería fasttext.\n",
        "# Esta librería es distinta de Gensim\n",
        "#ft_model = fasttext.load_model (\"embeddings-l-model.bin\")\n",
        "ft_model = fasttext.load_model (\"embeddings-s-model.bin\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1N8u_XKC25tU"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "# Definimos un conjunto de textos\n",
        "textos=['El procesamiento del lenguaje natural (PLN o NLP) es un campo dentro de la inteligencia artificial y la lingüística aplicada que estudia las interacciones mediante uso del lenguaje natural entre los seres humanos y las máquinas. \\\n",
        "Más concretamente se centra en el procesamiento de las comunicaciones humanas, dividiéndolas en partes, e identificando los elementos más relevantes del mensaje.\\\n",
        "Con la Comprensión y Generación de Lenguaje Natural, busca que las máquinas consigan entender, interpretar y manipular el lenguaje humano.'\n",
        ", 'El procesamiento del lenguaje natural (NLP, por sus siglas en inglés) es una rama de la inteligencia artificial que ayuda a las computadoras a entender, interpretar y manipular el lenguaje humano. \\\n",
        "NLP toma elementos prestados de muchas disciplinas, incluyendo la ciencia de la computación y la lingüística computacional, en su afán por cerrar la brecha entre la comunicación humana y el entendimiento de las computadoras.\"\"\"], \"\"\"El procesamiento del lenguaje natural (PLN o NLP) es un campo dentro de la inteligencia artificial y la lingüística aplicada que estudia las interacciones mediante uso del lenguaje natural entre los seres humanos y las máquinas. Más concretamente se centra en el procesamiento de las comunicaciones humanas, dividiéndolas en partes, e identificando los elementos más relevantes del mensaje. Con la Comprensión y Generación de Lenguaje Natural, busca que las máquinas consigan entender, interpretar y manipular el lenguaje humano.'\n",
        ", 'La lingüística computacional es un campo interdisciplinario que se ocupa del desarrollo de formalismos del funcionamiento del lenguaje natural, tales que puedan ser transformados en programas ejecutables para un ordenador. \\\n",
        "Dicho desarrollo se sitúa entre el modelado basado en reglas y el modelado estadístico del lenguaje natural desde una perspectiva computacional, y en él participan lingüistas e informáticos especializados en inteligencia artificial, psicólogos cognoscitivos y expertos en lógica, entre otros.'\n",
        ", 'El aprendizaje automático es un tipo de inteligencia artificial (AI) que proporciona a las computadoras la capacidad de aprender, sin ser programadas explícitamente. El aprendizaje automático se centra en el desarrollo de programas informáticos que pueden cambiar cuando se exponen a nuevos datos.'\n",
        ", 'El  aprendizaje profundo es un tema que cada vez adquiere mayor relevancia en el campo de la inteligencia artificial (IA). Siendo una subcategoría del aprendizaje automático, el aprendizaje profundo trata del uso de redes neuronales para mejorar cosas tales como el reconocimiento de voz, la visión por ordenador y el procesamiento del lenguaje natural. \\\n",
        "Rápidamente se está convirtiendo en uno de los campos más solicitados en informática. \\\n",
        "En los últimos años, el aprendizaje profundo ha ayudado a lograr avances en áreas tan diversas como la percepción de objetos, el procesamiento del lenguaje natural y el reconocimiento de voz (todas ellas áreas especialmente complejas para los investigadores en IA).',\n",
        "'El coste de la energía va a subir mucho los próximos meses y la población va a tener que pagar cantidades excesivas a las eléctricas']\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "df_textos = pd.DataFrame(textos)\n",
        "\n",
        "# Calculamos la similitud usando sentence embeddings\n",
        "fasttext_sentence_embeddings = df_textos[0].apply(lambda x: ft_model.get_sentence_vector(x))\n",
        "fasttext_sentence_embeddings = fasttext_sentence_embeddings.to_list()\n",
        "import numpy as np\n",
        "\n",
        "import scipy.sparse as sp\n",
        "if not sp.issparse(fasttext_sentence_embeddings):\n",
        "            fasttext_sentence_embeddings = sp.csr_matrix(fasttext_sentence_embeddings, dtype=np.float64)\n",
        "\n",
        "# Calculamos la similitud de los documentos con el coseno para sentence embeddings\n",
        "results = cosine_similarity(fasttext_sentence_embeddings[1::],fasttext_sentence_embeddings[0]).reshape(-1,) # Op -- (n_docs,1) -- Cosine Sim with each doc\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3OYTXx69tQH"
      },
      "source": [
        "Entrenamos ahora con los sentence embeddings de fasttext y obtenemos el accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oKs43XIr9sj_"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "fasttext_sentence_train = df_train.tweet.apply(lambda x: ft_model.get_sentence_vector(x))\n",
        "fasttext_sentence_test = df_test.tweet.apply(lambda x: ft_model.get_sentence_vector(x))\n",
        "\n",
        "clf_sentence_embeddings = LinearSVC(random_state=0, tol=1e-5).fit(fasttext_sentence_train.to_list(), df_train.label)\n",
        "predicted = clf_sentence_embeddings.predict(fasttext_sentence_test.to_list())\n",
        "accuracy = np.mean(predicted == df_test.label)\n",
        "\n",
        "print(\"Resultados sentence embeddings Fasttext ----- Accuracy:\", accuracy)\n",
        "from sklearn import metrics\n",
        "print(metrics.classification_report(df_test.label, predicted))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Si probamos con unos vectores de Fasttext de más dimensiones (100) en principio la clasificación debería ser mejor.\n",
        "!wget https://valencia.inf.um.es/valencia-tgine/embeddings-m-model.bin.gz\n",
        "!gzip -d embeddings-m-model.bin.gz"
      ],
      "metadata": {
        "id": "6lrIn0DBnW-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cargamos el modelo más grande\n",
        "ft_model = fasttext.load_model (\"embeddings-m-model.bin\")\n",
        "\n",
        "#Calculamos los embeddings y creamos el clasificador\n",
        "fasttext_sentence_train = df_train.tweet.apply(lambda x: ft_model.get_sentence_vector(x))\n",
        "fasttext_sentence_test = df_test.tweet.apply(lambda x: ft_model.get_sentence_vector(x))\n",
        "\n",
        "clf_sentence_embeddings = LinearSVC(random_state=0, tol=1e-5).fit(fasttext_sentence_train.to_list(), df_train.label)\n",
        "predicted = clf_sentence_embeddings.predict(fasttext_sentence_test.to_list())\n",
        "accuracy = np.mean(predicted == df_test.label)\n",
        "\n",
        "print(\"Resultados Fasttext modelo más grande ----- Accuracy:\", accuracy)\n",
        "from sklearn import metrics\n",
        "print(metrics.classification_report(df_test.label, predicted))"
      ],
      "metadata": {
        "id": "SlW0SoKtocFm"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}