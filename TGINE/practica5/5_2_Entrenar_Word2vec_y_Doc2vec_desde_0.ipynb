{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrajzBdoGAki"
      },
      "source": [
        "# Sesión 5 - Entrenar Word2vec y Doc2Vec desde 0\n",
        "\n",
        "En este notebook vamos a ver cómo se entrena un modelo sencillo de word2vec eligiendo las dimensiones de los vectores.\n",
        "\n",
        "Además, se creará un modelo de Doc2Vec a partir de un conjunto de documentos y se verá cómo se puede obtener las similitudes entre documentos.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aWLFpiTGDgWO"
      },
      "outputs": [],
      "source": [
        "# Instalamos gensim si no lo tenemos instalado\n",
        "!pip3 install -U gensim\n",
        "# Esto es por si no está ya instalado\n",
        "!pip3 install -U pandas\n",
        "!pip3 install -U nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXwlp73V7Ttp"
      },
      "source": [
        "# Apartado 1.1 Descargamos un corpus de prueba\n",
        "\n",
        "Vamos a probar con un corpus de noticias que se encuentra en la URL https://valencia.inf.um.es/valencia-tgine/corpusNoticias.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kWO6w9RLzzRm"
      },
      "outputs": [],
      "source": [
        "# Descargamos un corpus de noticias que he creado\n",
        "!wget --no-check-certificate https://valencia.inf.um.es/valencia-tgine/corpusNoticias.zip\n",
        "!unzip corpusNoticias.zip > extract.log"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ap0DBy7V7e-4"
      },
      "source": [
        "Leemos todos lo ficheros y los metemos en una variable *texts*\n",
        "\n",
        "Tened en cuenta que la codificación de caracteres en estos ficheros es UTF-8. Esto depende del contenido de las web a descargar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7YShf3_zDZNv"
      },
      "outputs": [],
      "source": [
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "\n",
        "my_path = \"corpusNoticias/\"\n",
        "texts = []\n",
        "for fn in listdir(my_path):\n",
        "  f = open(my_path+fn, encoding = \"utf-8\")\n",
        "  file_content = f.read()\n",
        "  texts.append(file_content)\n",
        "  f.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayP_P-SdgnYq",
        "tags": []
      },
      "source": [
        "# Apartado 1.2 Entrenamos un modelo word2vec a partir del corpus\n",
        "\n",
        "Aquí vamos a entrenar un modelo word2vec con la librería GENSIM. Como Tokenizer se utilizará el word_tokenize de NLTK, pero se podría usar cualquier otro tokenizador."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "VYOCayluhnw7"
      },
      "outputs": [],
      "source": [
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import numpy\n",
        "import pandas\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from gensim.models import Word2Vec,KeyedVectors\n",
        "from gensim.test.utils import datapath\n",
        "import re\n",
        "import unicodedata\n",
        "from tqdm import tqdm\n",
        "import gensim\n",
        "import multiprocessing\n",
        "import random\n",
        "from nltk.tokenize import word_tokenize\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FM6EVMswuu9-"
      },
      "outputs": [],
      "source": [
        "# Procesamos todos los textos y le aplicamos el word_tokenize de NLTK\n",
        "nltk.download('punkt_tab')\n",
        "train_texts=[]\n",
        "for text in texts:\n",
        "     train_texts.append(word_tokenize(text.lower()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nXcj2gSJjKBf"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "# define training data\n",
        "# train model\n",
        "# se puede entrenar el modelo con distintos parámetros como el tamaño del vector, tamaño de la ventana, las veces que debe\n",
        "# aparecer una palabra, etc.\n",
        "model = Word2Vec(train_texts, vector_size=100, window=10, min_count=1, workers=10)\n",
        "# summarize the loaded model\n",
        "print(model)\n",
        "# save model\n",
        "model.save('model.bin')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mgj_UBgG90GZ"
      },
      "outputs": [],
      "source": [
        "# Cargamos el modelo guardado\n",
        "new_model = Word2Vec.load('model.bin')\n",
        "\n",
        "# Probamos el nuevo modelo\n",
        "# Imprimimos el vector de la palabra 'energía'\n",
        "print(model.wv['energía'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOZqC1A3y9HB"
      },
      "outputs": [],
      "source": [
        "# Probamos similitudes\n",
        "new_model.wv.similarity(\"coronavirus\", \"covid\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ZPaD_Fvc2ZY"
      },
      "outputs": [],
      "source": [
        "# Probamos en listar alguna de las palabras más similares\n",
        "# Imprimimos las palabras más similares a 'covid'\n",
        "palabra = 'covid'\n",
        "print(new_model.wv.most_similar(palabra))\n",
        "\n",
        "# Imprimimos las palabras más similares a 'energía'\n",
        "palabra = 'energía'\n",
        "print(new_model.wv.most_similar(palabra))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EBffwV47-DbY"
      },
      "outputs": [],
      "source": [
        "# Probamos alguna analogía\n",
        "# Covid es a Vacunas lo que Salud es a ...\n",
        "print(new_model.wv.most_similar(positive=[\"salud\", \"vacunas\"], negative=[\"covid\"], topn=10))\n",
        "\n",
        "# Covid es a Vacunas lo que Guerra es a ...\n",
        "print(new_model.wv.most_similar(positive=[\"guerra\", \"vacunas\"], negative=[\"covid\"], topn=10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZuk9ucac2ZY"
      },
      "outputs": [],
      "source": [
        "# Probamos a mostrar términos similares\n",
        "# Imprimimos las palabras más similares a 'covid'\n",
        "palabra = 'covid'\n",
        "print(new_model.wv.most_similar(palabra))\n",
        "\n",
        "# Imprimimos las palabras más similares a 'ucrania'\n",
        "palabra = 'ucrania'\n",
        "print(new_model.wv.most_similar(palabra))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mca-aFsZIaoH"
      },
      "source": [
        "# Apartado 1.3 Entrenamos un Doc2Vec con los mismos textos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "yZ8Su_gJIcn3"
      },
      "outputs": [],
      "source": [
        "#Import all the dependencies\n",
        "from gensim.models import Doc2Vec\n",
        "from gensim.models.doc2vec import TaggedDocument\n",
        "\n",
        "#Necestiamos crear un TaggedDocument para cada uno de los textos indicando un índice de cada texto\n",
        "tagged_data = [TaggedDocument(words=_d, tags=[str(i)]) for i, _d in enumerate(train_texts)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "su6HsyG7I-UH"
      },
      "outputs": [],
      "source": [
        "# Definimos los parámetros de entrenamiento y entrenamos\n",
        "max_epochs = 10\n",
        "vec_size = 100\n",
        "alpha = 0.025\n",
        "\n",
        "doc2vec_model = Doc2Vec(vector_size=vec_size,\n",
        "                alpha=alpha,\n",
        "                min_alpha=0.00025,\n",
        "                min_count=1,\n",
        "                dm = 1,\n",
        "                epochs = max_epochs)\n",
        "\n",
        "doc2vec_model.build_vocab(tagged_data)\n",
        "\n",
        "for epoch in range(max_epochs):\n",
        "    doc2vec_model.train(tagged_data,\n",
        "                total_examples=doc2vec_model.corpus_count,\n",
        "                epochs=doc2vec_model.epochs)\n",
        "    # decrease the learning rate\n",
        "    doc2vec_model.alpha -= 0.0002\n",
        "    # fix the learning rate, no decay\n",
        "    doc2vec_model.min_alpha = model.alpha\n",
        "\n",
        "doc2vec_model.save(\"d2v.model\")\n",
        "print(\"Model Saved\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1gBkCLWVJahE"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Doc2Vec\n",
        "\n",
        "doc2vec_model= Doc2Vec.load(\"d2v.model\")\n",
        "# Probamos a encontrar textos similares a uno dado\n",
        "query_text = \"nadal\"\n",
        "test_data = word_tokenize(query_text)\n",
        "v1 = doc2vec_model.infer_vector(test_data)\n",
        "\n",
        "# Encontramos los documentos más similares\n",
        "similar_doc = doc2vec_model.dv.most_similar(v1)\n",
        "\n",
        "# Imprimimos los 5 documentos más similares\n",
        "top5_similar_doc = similar_doc[:5]\n",
        "print(top5_similar_doc)\n",
        "for doc in top5_similar_doc:\n",
        "  print(\"--------------------------\")\n",
        "  print(texts[int(doc[0])])\n",
        "  print('Similitud:',doc[1])\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}