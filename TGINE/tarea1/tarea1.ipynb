{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instalación de librerías"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo primero es instalar las librerías necesarias para realizar la tarea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scrapy in d:\\juanluhijo\\.venv\\lib\\site-packages (2.12.0)\n",
      "Requirement already satisfied: bs4 in d:\\juanluhijo\\.venv\\lib\\site-packages (0.0.2)\n",
      "Requirement already satisfied: elasticsearch in d:\\juanluhijo\\.venv\\lib\\site-packages (8.17.0)\n",
      "Requirement already satisfied: Twisted>=21.7.0 in d:\\juanluhijo\\.venv\\lib\\site-packages (from scrapy) (24.11.0)\n",
      "Requirement already satisfied: cryptography>=37.0.0 in d:\\juanluhijo\\.venv\\lib\\site-packages (from scrapy) (44.0.0)\n",
      "Requirement already satisfied: cssselect>=0.9.1 in d:\\juanluhijo\\.venv\\lib\\site-packages (from scrapy) (1.2.0)\n",
      "Requirement already satisfied: itemloaders>=1.0.1 in d:\\juanluhijo\\.venv\\lib\\site-packages (from scrapy) (1.3.2)\n",
      "Requirement already satisfied: parsel>=1.5.0 in d:\\juanluhijo\\.venv\\lib\\site-packages (from scrapy) (1.9.1)\n",
      "Requirement already satisfied: pyOpenSSL>=22.0.0 in d:\\juanluhijo\\.venv\\lib\\site-packages (from scrapy) (24.3.0)\n",
      "Requirement already satisfied: queuelib>=1.4.2 in d:\\juanluhijo\\.venv\\lib\\site-packages (from scrapy) (1.7.0)\n",
      "Requirement already satisfied: service-identity>=18.1.0 in d:\\juanluhijo\\.venv\\lib\\site-packages (from scrapy) (24.2.0)\n",
      "Requirement already satisfied: w3lib>=1.17.0 in d:\\juanluhijo\\.venv\\lib\\site-packages (from scrapy) (2.2.1)\n",
      "Requirement already satisfied: zope.interface>=5.1.0 in d:\\juanluhijo\\.venv\\lib\\site-packages (from scrapy) (7.2)\n",
      "Requirement already satisfied: protego>=0.1.15 in d:\\juanluhijo\\.venv\\lib\\site-packages (from scrapy) (0.3.1)\n",
      "Requirement already satisfied: itemadapter>=0.1.0 in d:\\juanluhijo\\.venv\\lib\\site-packages (from scrapy) (0.10.0)\n",
      "Requirement already satisfied: packaging in d:\\juanluhijo\\.venv\\lib\\site-packages (from scrapy) (24.2)\n",
      "Requirement already satisfied: tldextract in d:\\juanluhijo\\.venv\\lib\\site-packages (from scrapy) (5.1.3)\n",
      "Requirement already satisfied: lxml>=4.6.0 in d:\\juanluhijo\\.venv\\lib\\site-packages (from scrapy) (5.3.0)\n",
      "Requirement already satisfied: defusedxml>=0.7.1 in d:\\juanluhijo\\.venv\\lib\\site-packages (from scrapy) (0.7.1)\n",
      "Requirement already satisfied: PyDispatcher>=2.0.5 in d:\\juanluhijo\\.venv\\lib\\site-packages (from scrapy) (2.0.7)\n",
      "Requirement already satisfied: beautifulsoup4 in d:\\juanluhijo\\.venv\\lib\\site-packages (from bs4) (4.12.3)\n",
      "Requirement already satisfied: elastic-transport<9,>=8.15.1 in d:\\juanluhijo\\.venv\\lib\\site-packages (from elasticsearch) (8.15.1)\n",
      "Requirement already satisfied: cffi>=1.12 in d:\\juanluhijo\\.venv\\lib\\site-packages (from cryptography>=37.0.0->scrapy) (1.17.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.26.2 in d:\\juanluhijo\\.venv\\lib\\site-packages (from elastic-transport<9,>=8.15.1->elasticsearch) (2.3.0)\n",
      "Requirement already satisfied: certifi in d:\\juanluhijo\\.venv\\lib\\site-packages (from elastic-transport<9,>=8.15.1->elasticsearch) (2024.12.14)\n",
      "Requirement already satisfied: jmespath>=0.9.5 in d:\\juanluhijo\\.venv\\lib\\site-packages (from itemloaders>=1.0.1->scrapy) (1.0.1)\n",
      "Requirement already satisfied: attrs>=19.1.0 in d:\\juanluhijo\\.venv\\lib\\site-packages (from service-identity>=18.1.0->scrapy) (24.3.0)\n",
      "Requirement already satisfied: pyasn1 in d:\\juanluhijo\\.venv\\lib\\site-packages (from service-identity>=18.1.0->scrapy) (0.6.1)\n",
      "Requirement already satisfied: pyasn1-modules in d:\\juanluhijo\\.venv\\lib\\site-packages (from service-identity>=18.1.0->scrapy) (0.4.1)\n",
      "Requirement already satisfied: automat>=24.8.0 in d:\\juanluhijo\\.venv\\lib\\site-packages (from Twisted>=21.7.0->scrapy) (24.8.1)\n",
      "Requirement already satisfied: constantly>=15.1 in d:\\juanluhijo\\.venv\\lib\\site-packages (from Twisted>=21.7.0->scrapy) (23.10.4)\n",
      "Requirement already satisfied: hyperlink>=17.1.1 in d:\\juanluhijo\\.venv\\lib\\site-packages (from Twisted>=21.7.0->scrapy) (21.0.0)\n",
      "Requirement already satisfied: incremental>=24.7.0 in d:\\juanluhijo\\.venv\\lib\\site-packages (from Twisted>=21.7.0->scrapy) (24.7.2)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in d:\\juanluhijo\\.venv\\lib\\site-packages (from Twisted>=21.7.0->scrapy) (4.12.2)\n",
      "Requirement already satisfied: setuptools in d:\\juanluhijo\\.venv\\lib\\site-packages (from zope.interface>=5.1.0->scrapy) (75.7.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in d:\\juanluhijo\\.venv\\lib\\site-packages (from beautifulsoup4->bs4) (2.6)\n",
      "Requirement already satisfied: idna in d:\\juanluhijo\\.venv\\lib\\site-packages (from tldextract->scrapy) (3.10)\n",
      "Requirement already satisfied: requests>=2.1.0 in d:\\juanluhijo\\.venv\\lib\\site-packages (from tldextract->scrapy) (2.32.3)\n",
      "Requirement already satisfied: requests-file>=1.4 in d:\\juanluhijo\\.venv\\lib\\site-packages (from tldextract->scrapy) (2.1.0)\n",
      "Requirement already satisfied: filelock>=3.0.8 in d:\\juanluhijo\\.venv\\lib\\site-packages (from tldextract->scrapy) (3.16.1)\n",
      "Requirement already satisfied: pycparser in d:\\juanluhijo\\.venv\\lib\\site-packages (from cffi>=1.12->cryptography>=37.0.0->scrapy) (2.22)\n",
      "Requirement already satisfied: tomli in d:\\juanluhijo\\.venv\\lib\\site-packages (from incremental>=24.7.0->Twisted>=21.7.0->scrapy) (2.2.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\juanluhijo\\.venv\\lib\\site-packages (from requests>=2.1.0->tldextract->scrapy) (3.4.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Instalamos las librerías necesarias\n",
    "%pip install scrapy bs4 elasticsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compilar datos del documento web"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se va a crear un Crawler para extraer información de la página de Bulbapedia. Conretamente, el crawler va a extraer la información de todos lo Pokémon, empezará por un primer enlace (Abomasnow) y seguirá con los enlaces de la página para obtener la información del resto. \n",
    "\n",
    "Vamos a crear este crawler en un archivo `.py` a parte ([`crawler.py`](crawler.py#L15)).\n",
    "\n",
    "Una vez tenemos creado el crawler, vamos a ejecutarlo para obtener los datos. Los resultados de cada pokemon se guardarán en un archivo JSON cuyo nombre será el número de la pokédex del pokemon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"url\": \"https://bulbapedia.bulbagarden.net/wiki/Abomasnow_(Pok%C3%A9mon)\",\n",
      "    \"pokemon\": \"Abomasnow\",\n",
      "    \"pokedex_number\": \"0460\",\n",
      "    \"origin\": \"Abomasnow appears to be based on a cross between an evergreen tree covered in snow and a yeti. It may also be based on 樹氷 juhyou, trees completely covered in snow known as \\\"snow monsters\\\". It also resembles a snowy owl.\",\n",
      "    \"description\": \"Abomasnow is a large, bipedal Pokémon covered in shaggy, white fur. It has pale purple eyes with bushy eyebrows and long strands of fur covering its mouth. Two tufts of longer fur grow on both its back and chest, with the chest fur being longer on the female. Its hands, feet, and tail are dark green and spiky, similar to the foliage of an evergreen tree. On its back are four green spikes that resemble pinecones. Abomasnow lives in snowy mountains and appears only when snow flowers bloom. Once the flowers die, it retreats to isolated areas because Abomasnow prefers to be alone and not associate with others. It is able to create blizzards to hide itself and can also cause whiteout conditions with just a shake of its massive body. Abomasnow is also known as the \\\"Ice Monster\\\" and \\\"abominable snowman\\\". It protects Snover from packs of Galarian Darumaka, who chase them for the Berries they grow, by swinging its arms like hammers. Abomasnow is powerful enough to split huge boulders in two. As mentioned in the Sleep Style Dex, a sleeping Abomasnow can still cause blizzards by occasionally swinging its arms. Researchers acknowledged that it is unknown why Abomasnow does this, though it is theorized that it is either a habit to hide itself or just sleepy flailing. Prior to Generation VI, Abomasnow and its pre-evolved form, Snover, were the only known Pokémon that could have Snow Warning as an Ability.\"\n",
      "}\n",
      "Se alcanzó el límite de 500 archivos JSON. Terminando el rastreo.\n"
     ]
    }
   ],
   "source": [
    "from crawler import BulbaSpyder\n",
    "import scrapy, os\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "# Creamos un proceso de Crawler podemos poner distintas settings que están definidas en la documentación.\n",
    "# Entre ellas podemos ocular los logs del proceso de Crawling.\n",
    "process = CrawlerProcess(settings={\n",
    "    \"LOG_ENABLED\": False,\n",
    "    # Used for pipeline 1\n",
    "})\n",
    "\n",
    "# Comprobamos que existe la carpeta y si no existe la creamos\n",
    "if not os.path.exists('bulbapedia'):\n",
    "    os.mkdir('bulbapedia')\n",
    "\n",
    "# Creamos el proceso con el RSSSpider\n",
    "process.crawl(BulbaSpyder)\n",
    "# Ejecutamos el Crawler\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Buscador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importamos las librerías necesarias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch, helpers\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos conectamos a Elasticsearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Información del clúster:\n",
      "{'cluster_name': 'docker-cluster', 'status': 'yellow', 'timed_out': False, 'number_of_nodes': 1, 'number_of_data_nodes': 1, 'active_primary_shards': 27, 'active_shards': 27, 'relocating_shards': 0, 'initializing_shards': 0, 'unassigned_shards': 2, 'delayed_unassigned_shards': 0, 'number_of_pending_tasks': 0, 'number_of_in_flight_fetch': 0, 'task_max_waiting_in_queue_millis': 0, 'active_shards_percent_as_number': 93.10344827586206}\n"
     ]
    }
   ],
   "source": [
    "# Conexión a Elasticsearch\n",
    "client = Elasticsearch(['http://localhost:9200'])\n",
    "\n",
    "def obtener_informacion_cluster():\n",
    "    try:\n",
    "        # Obtener información sobre el clúster de Elasticsearch\n",
    "        info_cluster = client.cluster.health()\n",
    "        print(\"Información del clúster:\")\n",
    "        print(info_cluster)\n",
    "    except Exception as e:\n",
    "        print(\"Error al obtener información del clúster:\", e)\n",
    "\n",
    "# Llamar a la función para obtener información del clúster\n",
    "obtener_informacion_cluster()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos el índice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True, 'shards_acknowledged': True, 'index': 'bulbapedia_index'})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Nos conectamos a Elasticsearch\n",
    "es = Elasticsearch(\n",
    "    [{'host': 'localhost', 'port': 9200, 'scheme': 'http'}],\n",
    "    verify_certs=False\n",
    ")\n",
    "\n",
    "# Nombramos el índice\n",
    "index_name = 'bulbapedia_index'\n",
    "\n",
    "# Borramos el índice si ya existe\n",
    "if es.indices.exists(index=index_name):\n",
    "    es.indices.delete(index=index_name)\n",
    "\n",
    "# Creamos el índice\n",
    "es.indices.create(\n",
    "    index=index_name,\n",
    "    body={\n",
    "        \"mappings\": {\n",
    "            \"properties\": {\n",
    "                \"url\": {\"type\": \"keyword\"},\n",
    "                \"name\": {\"type\": \"keyword\"},\n",
    "                \"origin\": {\"type\": \"text\"},\n",
    "                \"description\": {\"type\": \"text\"},\n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos los datos en Elasticsearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(501, [])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "# Creamos una lista para guardar los documentos\n",
    "documents: list = [] \n",
    "\n",
    "# Directorio donde se encuentran los archivos JSON\n",
    "directory: Path = Path('bulbapedia')\n",
    "\n",
    "# Creamos un bucle que recorra los archivos JSON de la carpeta bulbapedia\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.json'):\n",
    "        file_path = directory / filename\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            data = json.load(file)\n",
    "            document = {\n",
    "                \"_index\": index_name,\n",
    "                \"_id\": uuid.uuid4(),\n",
    "                \"_source\": {\n",
    "                    \"url\": data[\"url\"],\n",
    "                    \"number\": data[\"pokedex_number\"],\n",
    "                    \"name\": data[\"pokemon\"],\n",
    "                    \"origin\": data[\"origin\"],\n",
    "                    \"description\": data[\"description\"]\n",
    "                },\n",
    "            }\n",
    "            documents.append(document)\n",
    "\n",
    "# Insertamos los documentos en Elasticsearch\n",
    "helpers.bulk(es, documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents in index 'bulbapedia_index': 501\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Comprobamos que se han insertado los documentos\n",
    "time.sleep(1)\n",
    "count = es.count(index=index_name)['count']\n",
    "print(f\"Number of documents in index '{index_name}': {count}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
