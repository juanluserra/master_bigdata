{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEdGLPXLdvCG"
      },
      "source": [
        "# Sesión 4.1 - Procesamiento de texto con la librería scikit-learn\n",
        "En esta sesión de prácticas vamos a ver cómo podemo usar las funciones de tratamiento de texto de la librería scikit-learn para calcular TF, TF-IDF y BM25.\n",
        "\n",
        "Primero, instalaremos la librerías necesarias."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "an8uq5WbeOZ6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-learn in c:\\users\\x542ua\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.5.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\x542ua\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (2.1.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\x542ua\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\x542ua\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\x542ua\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (3.5.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "SYSTEM_WGETRC = c:/progra~1/wget/etc/wgetrc\n",
            "syswgetrc = C:\\Program Files (x86)\\GnuWin32/etc/wgetrc\n",
            "--2024-10-24 18:55:05--  https://valencia.inf.um.es/valencia-tgine/BM25.py\n",
            "Resolviendo valencia.inf.um.es... 155.54.204.133\n",
            "Connecting to valencia.inf.um.es|155.54.204.133|:443... conectado.\n",
            "OpenSSL: error:1407742E:SSL routines:SSL23_GET_SERVER_HELLO:tlsv1 alert protocol version\n",
            "No se pudo establecer la conexi�n SSL.\n",
            "SYSTEM_WGETRC = c:/progra~1/wget/etc/wgetrc\n",
            "syswgetrc = C:\\Program Files (x86)\\GnuWin32/etc/wgetrc\n",
            "--2024-10-24 18:55:05--  https://valencia.inf.um.es/valencia-tgine/datasetEspa%F1ol.csv\n",
            "Resolviendo valencia.inf.um.es... 155.54.204.133\n",
            "Connecting to valencia.inf.um.es|155.54.204.133|:443... conectado.\n",
            "OpenSSL: error:1407742E:SSL routines:SSL23_GET_SERVER_HELLO:tlsv1 alert protocol version\n",
            "No se pudo establecer la conexi�n SSL.\n"
          ]
        }
      ],
      "source": [
        "!pip3 install -U scikit-learn\n",
        "# Descargamos un fichero python con la implementación del BM25\n",
        "!wget --no-check-certificate https://valencia.inf.um.es/valencia-tgine/BM25.py\n",
        "\n",
        "# Descargamos el fichero datasetEspañol.csv\n",
        "!wget --no-check-certificate https://valencia.inf.um.es/valencia-tgine/datasetEspañol.csv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "n8Wt0FkzyW19"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "MarKmlc0Cyaj"
      },
      "outputs": [],
      "source": [
        "# Importamos del fichero BM25.py\n",
        "from BM25 import BM25Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "p1ldLK4Hye0H"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>twitter_id</th>\n",
              "      <th>twitter_created_at</th>\n",
              "      <th>tweet</th>\n",
              "      <th>corpus</th>\n",
              "      <th>user</th>\n",
              "      <th>agreement</th>\n",
              "      <th>votes</th>\n",
              "      <th>score</th>\n",
              "      <th>label</th>\n",
              "      <th>__split</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5954</th>\n",
              "      <td>1274324047581581316</td>\n",
              "      <td>2020-06-20 16:51:43</td>\n",
              "      <td>No me fío nada! ? en #estadodealarma demostrar...</td>\n",
              "      <td>Estado de alarma nacional (oficial)</td>\n",
              "      <td>GuzmanitaMaria</td>\n",
              "      <td>100</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>negative</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5955</th>\n",
              "      <td>1274321386446733315</td>\n",
              "      <td>2020-06-20 16:41:08</td>\n",
              "      <td>@roldanfj1 @MikiyDuarte @diariocadiz @realDona...</td>\n",
              "      <td>Estado de alarma nacional (oficial)</td>\n",
              "      <td>ByChanchi</td>\n",
              "      <td>100</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>negative</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5956</th>\n",
              "      <td>1274340519271858178</td>\n",
              "      <td>2020-06-20 17:57:10</td>\n",
              "      <td>Con el fin del #EstadodeAlarma se acaban, tamb...</td>\n",
              "      <td>Estado de alarma nacional (oficial)</td>\n",
              "      <td>Javiersilvestre</td>\n",
              "      <td>100</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>negative</td>\n",
              "      <td>val</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5957</th>\n",
              "      <td>1274367246979211269</td>\n",
              "      <td>2020-06-20 19:43:22</td>\n",
              "      <td>@horaciorlarreta. @AsisOberdan. @luisnovaresio...</td>\n",
              "      <td>Estado de alarma nacional (oficial)</td>\n",
              "      <td>juliodebarna</td>\n",
              "      <td>100</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>negative</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5958</th>\n",
              "      <td>1274368625047220224</td>\n",
              "      <td>2020-06-20 19:48:51</td>\n",
              "      <td>En la última semana se han registrado 36 falle...</td>\n",
              "      <td>Estado de alarma nacional (oficial)</td>\n",
              "      <td>mallorcadiario</td>\n",
              "      <td>100</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>negative</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               twitter_id   twitter_created_at  \\\n",
              "5954  1274324047581581316  2020-06-20 16:51:43   \n",
              "5955  1274321386446733315  2020-06-20 16:41:08   \n",
              "5956  1274340519271858178  2020-06-20 17:57:10   \n",
              "5957  1274367246979211269  2020-06-20 19:43:22   \n",
              "5958  1274368625047220224  2020-06-20 19:48:51   \n",
              "\n",
              "                                                  tweet  \\\n",
              "5954  No me fío nada! ? en #estadodealarma demostrar...   \n",
              "5955  @roldanfj1 @MikiyDuarte @diariocadiz @realDona...   \n",
              "5956  Con el fin del #EstadodeAlarma se acaban, tamb...   \n",
              "5957  @horaciorlarreta. @AsisOberdan. @luisnovaresio...   \n",
              "5958  En la última semana se han registrado 36 falle...   \n",
              "\n",
              "                                   corpus             user  agreement  votes  \\\n",
              "5954  Estado de alarma nacional (oficial)   GuzmanitaMaria        100      1   \n",
              "5955  Estado de alarma nacional (oficial)        ByChanchi        100      1   \n",
              "5956  Estado de alarma nacional (oficial)  Javiersilvestre        100      1   \n",
              "5957  Estado de alarma nacional (oficial)     juliodebarna        100      1   \n",
              "5958  Estado de alarma nacional (oficial)   mallorcadiario        100      1   \n",
              "\n",
              "      score     label __split  \n",
              "5954     -1  negative    test  \n",
              "5955     -1  negative   train  \n",
              "5956     -1  negative     val  \n",
              "5957     -1  negative   train  \n",
              "5958     -1  negative   train  "
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Leemos los datos del dataset en español de la primera sesión\n",
        "data = pd.read_csv(\"datasetEspañol.csv\",encoding=\"UTF-8\")\n",
        "data.tail()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oPOoOqEf5NG"
      },
      "source": [
        "## Apartado 1.1 - Obtener TF del conjunto de texto (Resuelto)\n",
        "\n",
        "Calculamos la matriz de TF usando la clase CountVectorizer sobre un conjunto de textos.\n",
        "\n",
        "Se puede consultar información de esta clase en la siguiente URL:\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer\n",
        "\n",
        "Hay que tener en cuenta que esta clase tiene muchos parámetros en su método de creación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "g1axNe99hQG0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(3, 36)\n",
            "La asignatura de TGINE es una asignatura del máster de BigData que se estudia en la Universidad de Murcia.\n",
            "En la asignatura de máster vemos una introducción al procesamiento del lenguaje natural y tecnologías de procesamiento\n",
            "de información no estructurada.\n",
            "\n",
            "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
            "\twith 25 stored elements and shape (1, 36)>\n",
            "  Coords\tValues\n",
            "  (0, 16)\t3\n",
            "  (0, 1)\t3\n",
            "  (0, 4)\t6\n",
            "  (0, 31)\t1\n",
            "  (0, 8)\t1\n",
            "  (0, 32)\t2\n",
            "  (0, 5)\t2\n",
            "  (0, 21)\t2\n",
            "  (0, 2)\t1\n",
            "  (0, 27)\t1\n",
            "  (0, 29)\t1\n",
            "  (0, 10)\t1\n",
            "  (0, 7)\t2\n",
            "  (0, 33)\t1\n",
            "  (0, 20)\t1\n",
            "  (0, 35)\t1\n",
            "  (0, 15)\t1\n",
            "  (0, 0)\t1\n",
            "  (0, 25)\t2\n",
            "  (0, 18)\t1\n",
            "  (0, 22)\t1\n",
            "  (0, 30)\t1\n",
            "  (0, 14)\t1\n",
            "  (0, 24)\t1\n",
            "  (0, 9)\t1\n",
            "No me gusta el chocolate ni las fresas\n",
            "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
            "\twith 8 stored elements and shape (1, 36)>\n",
            "  Coords\tValues\n",
            "  (0, 24)\t1\n",
            "  (0, 19)\t1\n",
            "  (0, 13)\t1\n",
            "  (0, 6)\t1\n",
            "  (0, 3)\t1\n",
            "  (0, 23)\t1\n",
            "  (0, 17)\t1\n",
            "  (0, 11)\t1\n",
            "El profesor de la asignatura TGINE es Rafael Valencia García.\n",
            "\n",
            "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
            "\twith 10 stored elements and shape (1, 36)>\n",
            "  Coords\tValues\n",
            "  (0, 16)\t1\n",
            "  (0, 1)\t1\n",
            "  (0, 4)\t1\n",
            "  (0, 31)\t1\n",
            "  (0, 8)\t1\n",
            "  (0, 6)\t1\n",
            "  (0, 26)\t1\n",
            "  (0, 28)\t1\n",
            "  (0, 34)\t1\n",
            "  (0, 12)\t1\n",
            "Mostramos los items del diccionario\n",
            "dict_items([('la', 16), ('asignatura', 1), ('de', 4), ('tgine', 31), ('es', 8), ('una', 32), ('del', 5), ('máster', 21), ('bigdata', 2), ('que', 27), ('se', 29), ('estudia', 10), ('en', 7), ('universidad', 33), ('murcia', 20), ('vemos', 35), ('introducción', 15), ('al', 0), ('procesamiento', 25), ('lenguaje', 18), ('natural', 22), ('tecnologías', 30), ('información', 14), ('no', 24), ('estructurada', 9), ('me', 19), ('gusta', 13), ('el', 6), ('chocolate', 3), ('ni', 23), ('las', 17), ('fresas', 11), ('profesor', 26), ('rafael', 28), ('valencia', 34), ('garcía', 12)])\n",
            "Tamaño vocabulario: 36\n",
            "Código de la palabra TGINE es: 31\n"
          ]
        }
      ],
      "source": [
        "texto = \"\"\"La asignatura de TGINE es una asignatura del máster de BigData que se estudia en la Universidad de Murcia.\n",
        "En la asignatura de máster vemos una introducción al procesamiento del lenguaje natural y tecnologías de procesamiento\n",
        "de información no estructurada.\n",
        "\"\"\"\n",
        "texto2 = \"No me gusta el chocolate ni las fresas\"\n",
        "\n",
        "texto3 = \"\"\"El profesor de la asignatura TGINE es Rafael Valencia García.\n",
        "\"\"\"\n",
        "# Calculamos la matriz de TF usando la función fit_transform\n",
        "count_vect = CountVectorizer()\n",
        "X_counts = count_vect.fit_transform([texto,texto2,texto3])\n",
        "\n",
        "# Mostramos entonces el número de textos y el número de tokens únicos\n",
        "print(X_counts.shape)\n",
        "\n",
        "# X_counts es una matriz dispersa con el TF de cada token en cada texto\n",
        "# Imprimimos los textos y su correspondientes TF\n",
        "print(texto)\n",
        "print(X_counts[0])\n",
        "print(texto2)\n",
        "print(X_counts[1])\n",
        "print(texto3)\n",
        "print(X_counts[2])\n",
        "\n",
        "#Los tokens de todo el vocabulario se representan con ids que hacen referencia a cada token.\n",
        "print(\"Mostramos los items del diccionario\")\n",
        "print(count_vect.vocabulary_.items())\n",
        "print(\"Tamaño vocabulario:\", str(len(count_vect.vocabulary_.items())))\n",
        "\n",
        "#Mostramos el código de una palabra determinada\n",
        "#hay que tener en cuenta que todos los tokens se guardan en minúsculas\n",
        "palabra_a_buscar=\"TGINE\"\n",
        "print(\"Código de la palabra\", palabra_a_buscar, \"es:\", count_vect.vocabulary_.get(palabra_a_buscar.lower()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1hMjf7pTYDl"
      },
      "source": [
        "## Apartado 1.2 Calculamos el TF sin tener en cuenta las stopwords\n",
        "Para eso hacemos uso del parámetro stop_words de CountVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "2xBK8v0wTYDl"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(3, 22)\n",
            "La asignatura de TGINE es una asignatura del máster de BigData que se estudia en la Universidad de Murcia.\n",
            "En la asignatura de máster vemos una introducción al procesamiento del lenguaje natural y tecnologías de procesamiento\n",
            "de información no estructurada.\n",
            "\n",
            "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
            "\twith 15 stored elements and shape (1, 22)>\n",
            "  Coords\tValues\n",
            "  (0, 0)\t3\n",
            "  (0, 18)\t1\n",
            "  (0, 12)\t2\n",
            "  (0, 1)\t1\n",
            "  (0, 4)\t1\n",
            "  (0, 19)\t1\n",
            "  (0, 11)\t1\n",
            "  (0, 21)\t1\n",
            "  (0, 9)\t1\n",
            "  (0, 14)\t2\n",
            "  (0, 10)\t1\n",
            "  (0, 13)\t1\n",
            "  (0, 17)\t1\n",
            "  (0, 8)\t1\n",
            "  (0, 3)\t1\n",
            "No me gusta el chocolate ni las fresas\n",
            "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
            "\twith 3 stored elements and shape (1, 22)>\n",
            "  Coords\tValues\n",
            "  (0, 7)\t1\n",
            "  (0, 2)\t1\n",
            "  (0, 5)\t1\n",
            "El profesor de la asignatura TGINE es Rafael Valencia García.\n",
            "\n",
            "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
            "\twith 6 stored elements and shape (1, 22)>\n",
            "  Coords\tValues\n",
            "  (0, 0)\t1\n",
            "  (0, 18)\t1\n",
            "  (0, 15)\t1\n",
            "  (0, 16)\t1\n",
            "  (0, 20)\t1\n",
            "  (0, 6)\t1\n",
            "Mostramos los items del diccionario\n",
            "dict_items([('asignatura', 0), ('tgine', 18), ('máster', 12), ('bigdata', 1), ('estudia', 4), ('universidad', 19), ('murcia', 11), ('vemos', 21), ('introducción', 9), ('procesamiento', 14), ('lenguaje', 10), ('natural', 13), ('tecnologías', 17), ('información', 8), ('estructurada', 3), ('gusta', 7), ('chocolate', 2), ('fresas', 5), ('profesor', 15), ('rafael', 16), ('valencia', 20), ('garcía', 6)])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\X542UA\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "# Descargamos las stopwords de NLTK\n",
        "# Si no tenemos instalado NLTK lo instalamos\n",
        "# !pip3 install -U nltk\n",
        "nltk.download('stopwords')\n",
        "stopwords_sp = nltk.corpus.stopwords.words('spanish')\n",
        "\n",
        "# Calculamos la matriz de TF usando la función fit_transform quitando las stopwords\n",
        "count_vect = CountVectorizer(stop_words=stopwords_sp)\n",
        "X_counts = count_vect.fit_transform([texto, texto2, texto3])\n",
        "\n",
        "# Mostramos entonces el número de textos y el número de tokens únicos\n",
        "print(X_counts.shape)\n",
        "\n",
        "# X_counts es una matriz dispersa con el TF de cada token en cada texto\n",
        "# Imprimimos los textos y su correspondientes TF\n",
        "print(texto)\n",
        "print(X_counts[0])\n",
        "print(texto2)\n",
        "print(X_counts[1])\n",
        "print(texto3)\n",
        "print(X_counts[2])\n",
        "\n",
        "#Los tokens de todo el vocabulario se representan con ids que hacen referencia a cada token.\n",
        "print(\"Mostramos los items del diccionario\")\n",
        "print(count_vect.vocabulary_.items())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fn7dSDhxjE4R"
      },
      "source": [
        "## Apartado 1.3 TF-IDF (Resuelto)\n",
        "Obtenemos el TF-IDF utilizando la clase TfidfTransformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "03E_i6avjAiz"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(3, 22)\n",
            "La asignatura de TGINE es una asignatura del máster de BigData que se estudia en la Universidad de Murcia.\n",
            "En la asignatura de máster vemos una introducción al procesamiento del lenguaje natural y tecnologías de procesamiento\n",
            "de información no estructurada.\n",
            "\n",
            "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
            "\twith 15 stored elements and shape (1, 22)>\n",
            "  Coords\tValues\n",
            "  (0, 0)\t0.4582996650455791\n",
            "  (0, 1)\t0.20086966141832005\n",
            "  (0, 3)\t0.20086966141832005\n",
            "  (0, 4)\t0.20086966141832005\n",
            "  (0, 8)\t0.20086966141832005\n",
            "  (0, 9)\t0.20086966141832005\n",
            "  (0, 10)\t0.20086966141832005\n",
            "  (0, 11)\t0.20086966141832005\n",
            "  (0, 12)\t0.4017393228366401\n",
            "  (0, 13)\t0.20086966141832005\n",
            "  (0, 14)\t0.4017393228366401\n",
            "  (0, 17)\t0.20086966141832005\n",
            "  (0, 18)\t0.15276655501519304\n",
            "  (0, 19)\t0.20086966141832005\n",
            "  (0, 21)\t0.20086966141832005\n",
            "No me gusta el chocolate ni las fresas\n",
            "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
            "\twith 3 stored elements and shape (1, 22)>\n",
            "  Coords\tValues\n",
            "  (0, 2)\t0.5773502691896257\n",
            "  (0, 5)\t0.5773502691896257\n",
            "  (0, 7)\t0.5773502691896257\n",
            "El profesor de la asignatura TGINE es Rafael Valencia García.\n",
            "\n",
            "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
            "\twith 6 stored elements and shape (1, 22)>\n",
            "  Coords\tValues\n",
            "  (0, 0)\t0.3349067026613031\n",
            "  (0, 6)\t0.4403620672313486\n",
            "  (0, 15)\t0.4403620672313486\n",
            "  (0, 16)\t0.4403620672313486\n",
            "  (0, 18)\t0.3349067026613031\n",
            "  (0, 20)\t0.4403620672313486\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "tfidf_transformer = TfidfTransformer()\n",
        "X_TFIDF = tfidf_transformer.fit_transform(X_counts)\n",
        "\n",
        "# Mostramos entonces el número de textos y el número de tokens únicos\n",
        "print(X_TFIDF.shape)\n",
        "\n",
        "# X_counts es una matriz dispersa con el TF de cada token en cada texto\n",
        "# Imprimimos los textos y su correspondientes TF\n",
        "print(texto)\n",
        "print(X_TFIDF[0])\n",
        "print(texto2)\n",
        "print(X_TFIDF[1])\n",
        "print(texto3)\n",
        "print(X_TFIDF[2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liEFSQ-rpfBd"
      },
      "source": [
        "## Apartado 1.4 BM25 (Resuelto)\n",
        "\n",
        "Calculamos el BM25 que tiene en cuenta tanto la transformación del TF como la normalización de la longitud del documento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "fCDDgaBwpepN"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(3, 22)\n",
            "La asignatura de TGINE es una asignatura del máster de BigData que se estudia en la Universidad de Murcia.\n",
            "En la asignatura de máster vemos una introducción al procesamiento del lenguaje natural y tecnologías de procesamiento\n",
            "de información no estructurada.\n",
            "\n",
            "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
            "\twith 15 stored elements and shape (1, 22)>\n",
            "  Coords\tValues\n",
            "  (0, 0)\t0.3171561694008161\n",
            "  (0, 1)\t0.23754431912466717\n",
            "  (0, 3)\t0.23754431912466717\n",
            "  (0, 4)\t0.23754431912466717\n",
            "  (0, 8)\t0.23754431912466717\n",
            "  (0, 9)\t0.23754431912466717\n",
            "  (0, 10)\t0.23754431912466717\n",
            "  (0, 11)\t0.23754431912466717\n",
            "  (0, 12)\t0.35076637777287306\n",
            "  (0, 13)\t0.23754431912466717\n",
            "  (0, 14)\t0.35076637777287306\n",
            "  (0, 17)\t0.23754431912466717\n",
            "  (0, 18)\t0.18065857750679398\n",
            "  (0, 19)\t0.23754431912466717\n",
            "  (0, 21)\t0.23754431912466717\n",
            "No me gusta el chocolate ni las fresas\n",
            "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
            "\twith 3 stored elements and shape (1, 22)>\n",
            "  Coords\tValues\n",
            "  (0, 2)\t0.5773502691896257\n",
            "  (0, 5)\t0.5773502691896257\n",
            "  (0, 7)\t0.5773502691896257\n",
            "El profesor de la asignatura TGINE es Rafael Valencia García.\n",
            "\n",
            "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
            "\twith 6 stored elements and shape (1, 22)>\n",
            "  Coords\tValues\n",
            "  (0, 0)\t0.3349067026613031\n",
            "  (0, 6)\t0.4403620672313486\n",
            "  (0, 15)\t0.4403620672313486\n",
            "  (0, 16)\t0.4403620672313486\n",
            "  (0, 18)\t0.3349067026613031\n",
            "  (0, 20)\t0.4403620672313486\n"
          ]
        }
      ],
      "source": [
        "bm25_transformer = BM25Transformer(k=1.2, b=0.5)\n",
        "X_BM25 = bm25_transformer.fit_transform(X_counts)\n",
        "\n",
        "# Mostramos entonces el número de textos y el número de tokens únicos\n",
        "print(X_BM25.shape)\n",
        "\n",
        "# X_counts es una matriz dispersa con el TF de cada token en cada texto\n",
        "# Imprimimos los textos y su correspondientes TF\n",
        "print(texto)\n",
        "print(X_BM25[0])\n",
        "print(texto2)\n",
        "print(X_BM25[1])\n",
        "print(texto3)\n",
        "print(X_BM25[2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHsOfKhgeaCw"
      },
      "source": [
        "# Apartado 1.5 Desarrollamos un simple buscador con TFIDF y BM25 (Resuelto)\n",
        "\n",
        "A continuación vamos a procesar los tuits del archivo de la primera sesión \"datosEspañol.csv\" y calculamos el TF, el TFIDF y el BM25 de manera similar a como se ha hecho anteriormente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Sxt-6GdmzVlA"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(5959, 23922)\n",
            "Hoy merendola deliciosa! Latte Macchiato Caramelo con Leche Condensada y Gofre! ???. #yomequedoencasa #todovaasalirbien #undiamenos #actitudpositiva #lattemacchiato #gofre #delicious #strong #stronger #smile…\n",
            "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
            "\twith 18 stored elements and shape (1, 23922)>\n",
            "  Coords\tValues\n",
            "  (0, 11621)\t1\n",
            "  (0, 14830)\t1\n",
            "  (0, 6839)\t1\n",
            "  (0, 13387)\t1\n",
            "  (0, 14082)\t1\n",
            "  (0, 4063)\t1\n",
            "  (0, 13452)\t1\n",
            "  (0, 5362)\t1\n",
            "  (0, 10888)\t2\n",
            "  (0, 23725)\t1\n",
            "  (0, 22067)\t1\n",
            "  (0, 22606)\t1\n",
            "  (0, 1018)\t1\n",
            "  (0, 13388)\t1\n",
            "  (0, 6843)\t1\n",
            "  (0, 21278)\t1\n",
            "  (0, 21279)\t1\n",
            "  (0, 20860)\t1\n"
          ]
        }
      ],
      "source": [
        "# Calculamos la matriz de TF usando la función fit_transform\n",
        "count_vect = CountVectorizer(stop_words=stopwords_sp)\n",
        "X_counts = count_vect.fit_transform(data['tweet'])\n",
        "\n",
        "# Mostramos entonces el número de textos y el número de tokens únicos\n",
        "print(X_counts.shape)\n",
        "\n",
        "# X_counts es una matriz dispersa con el TF de cada token en cada texto\n",
        "# Imprimimos el primer tuit y su correspondientes TF\n",
        "print(data['tweet'][0])\n",
        "print(X_counts[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "CC1CNRQlATwZ"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3657"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# El vocabulario que forma los tokens del objeto vectorizer se puede obtener de la siguiente manera\n",
        "# obtenemos el id del token 'buenosdias' que proviene de un hashtag\n",
        "count_vect.vocabulary_.get('buenosdias')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "R9in80glzWGo"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(5959, 23922)\n",
            "Hoy merendola deliciosa! Latte Macchiato Caramelo con Leche Condensada y Gofre! ???. #yomequedoencasa #todovaasalirbien #undiamenos #actitudpositiva #lattemacchiato #gofre #delicious #strong #stronger #smile…\n",
            "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
            "\twith 18 stored elements and shape (1, 23922)>\n",
            "  Coords\tValues\n",
            "  (0, 1018)\t0.22166053291045124\n",
            "  (0, 4063)\t0.24015722738486325\n",
            "  (0, 5362)\t0.24015722738486325\n",
            "  (0, 6839)\t0.24015722738486325\n",
            "  (0, 6843)\t0.2157059272965902\n",
            "  (0, 10888)\t0.4803144547697265\n",
            "  (0, 11621)\t0.09324696322101765\n",
            "  (0, 13387)\t0.24015722738486325\n",
            "  (0, 13388)\t0.24015722738486325\n",
            "  (0, 13452)\t0.21084066025562398\n",
            "  (0, 14082)\t0.24015722738486325\n",
            "  (0, 14830)\t0.24015722738486325\n",
            "  (0, 20860)\t0.2157059272965902\n",
            "  (0, 21278)\t0.24015722738486325\n",
            "  (0, 21279)\t0.24015722738486325\n",
            "  (0, 22067)\t0.1503006371554885\n",
            "  (0, 22606)\t0.17275793273390508\n",
            "  (0, 23725)\t0.049423969305183627\n",
            "(5959, 23922)\n",
            "Hoy merendola deliciosa! Latte Macchiato Caramelo con Leche Condensada y Gofre! ???. #yomequedoencasa #todovaasalirbien #undiamenos #actitudpositiva #lattemacchiato #gofre #delicious #strong #stronger #smile…\n",
            "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
            "\twith 18 stored elements and shape (1, 23922)>\n",
            "  Coords\tValues\n",
            "  (0, 1018)\t0.23594747583990278\n",
            "  (0, 4063)\t0.2556363591756769\n",
            "  (0, 5362)\t0.2556363591756769\n",
            "  (0, 6839)\t0.2556363591756769\n",
            "  (0, 6843)\t0.2296090711371575\n",
            "  (0, 10888)\t0.3582417323903334\n",
            "  (0, 11621)\t0.09925711768735897\n",
            "  (0, 13387)\t0.2556363591756769\n",
            "  (0, 13388)\t0.2556363591756769\n",
            "  (0, 13452)\t0.22443021740740127\n",
            "  (0, 14082)\t0.2556363591756769\n",
            "  (0, 14830)\t0.2556363591756769\n",
            "  (0, 20860)\t0.2296090711371575\n",
            "  (0, 21278)\t0.2556363591756769\n",
            "  (0, 21279)\t0.2556363591756769\n",
            "  (0, 22067)\t0.1599881381152023\n",
            "  (0, 22606)\t0.183892899762864\n",
            "  (0, 23725)\t0.052609549613678995\n"
          ]
        }
      ],
      "source": [
        "# Calculamos ahora el TFIDF\n",
        "X_TFIDF = tfidf_transformer.fit_transform(X_counts)\n",
        "# Mostramos entonces el número de textos y el número de tokens únicos\n",
        "print(X_TFIDF.shape)\n",
        "\n",
        "# X_counts es una matriz dispersa con el TF de cada token en cada texto\n",
        "# Imprimimos el primer tuit y su correspondientes TF\n",
        "print(data['tweet'][0])\n",
        "print(X_TFIDF[0])\n",
        "\n",
        "# Calculamos también el BM25\n",
        "X_BM25 = bm25_transformer.fit_transform(X_counts)\n",
        "# Mostramos entonces el número de textos y el número de tokens únicos\n",
        "print(X_BM25.shape)\n",
        "\n",
        "# X_counts es una matriz dispersa con el TF de cada token en cada texto\n",
        "# Imprimimos el primer tuit y su correspondientes TF\n",
        "print(data['tweet'][0])\n",
        "print(X_BM25[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "0b40Vpc8zjAl"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(5959, 23922)\n",
            "(5959, 23922)\n"
          ]
        }
      ],
      "source": [
        "# Podemos ver que el tamaño de la matriz de textos y el número total de tokens es el mismo tanto\n",
        "# para TFIDF como para BM25\n",
        "print(X_TFIDF.shape) # (Number of tweets, Number of unique words)\n",
        "print(X_BM25.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNqweJpqfpcS"
      },
      "source": [
        "Realizamos una consulta cualquiera y la metemos en el string \"query\" para a continuación calcular la similitud del coseno usando el TF-IDF y el BM25."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "qQg-FiyLz1mZ"
      },
      "outputs": [],
      "source": [
        "query = \"semana santa\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "eS-z3SCdz7y6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Aunque sean unos momentos difíciles, debemos tomarlo con el mayor positivismo posible! La Semana Santa no es sólo salir a beber y a ver procesiones, es mucho más que eso. Disfrutemos de esta semana de una forma verdadera! Feliz Domingo de Ramos y Semana Santa!!! #YoMeQuedoEnCasa -- 0.5341953468689503 -- 804\n",
            "Es tiempo de recogimiento ?. Esta Semana Santa no se sale. #SemanaSanta #YoMeQuedoEnCasa #QuedateEnCasa -- 0.4618900044681435 -- 4863\n",
            "Buen dia amigos espero que estén todos bien!!! Feliz inicio de semana arriba ese ánimo todo lo mejor, que sea una Semana Santa de bendiciones!!! A cuidarse . #QuedateEnCasa -- 0.450087834395828 -- 31\n",
            "Esta Semana Santa quiero salvar vidas: #YoMeQuedoEnCasa . Si te vas de “vacaciones”, ¿cuántas muertes caerán sobre tus espaldas la semana que viene...?. #QuedateEnTuCasa ?? -- 0.40809989875165065 -- 1372\n",
            "¡Feliz Semana Santa! Aunque sea en casa ?. #AyudaADomicilio #Mayores #QuéDateEnCasa #Santander #Cantabria -- 0.4044128930022516 -- 1967\n",
            "La procesión de carros de compra que estoy viendo desde mi balcón sirve como Semana Santa? #YoMeQuedoEnCasa -- 0.38389199667649315 -- 156\n",
            "Último día de teletrabajo hasta después de Semana Santa ???. Gracias equipo TeSera por vuestro compromiso y responsabilidad #todosjuntospodemos #yomequedoencasa ?????? -- 0.3505556476212133 -- 1353\n",
            "#QuedateEnCasa #YoMeQuedoEnCasa #COVIDー19 #JuntosNosCuidamos Disfrutando de la Semana Santa atraves de la plataforma @NefliCofrades ¡MUCHAS GRACIAS A TODO EL EQUIPO! ???? -- 0.34283890481106116 -- 813\n",
            "¡Feliz Semana Santa en casa!. Laboratorios Mahen, ¡siente la conexión con la naturaleza! ?. #SemanaSanta #YoMeQuedoEnCasa #LaboratoriosMahen -- 0.33651344818376056 -- 1327\n",
            "A este paso no creo que las escuelas abran hasta después de Semana Santa y ya veremos si más allá. #coronavirus #COVID19 #quedatencasa #estadodealarma -- 0.33605409906504957 -- 4170\n"
          ]
        }
      ],
      "source": [
        "# Transformamos la query a TF-ID y sacamos los resultados de la comparación con la función del coseno\n",
        "# cosine_similarity\n",
        "query_vec = count_vect.transform([query]) # Ip -- (n_docs,x), Op -- (n_docs,n_Feats)\n",
        "query_vec_TFIDF = tfidf_transformer.fit_transform(query_vec)\n",
        "results = cosine_similarity(X_TFIDF,query_vec_TFIDF).reshape((-1,)) # Op -- (n_docs,1) -- Cosine Sim with each doc\n",
        "\n",
        "# Immprimimos a continuación los primeros 10 resultados ordenados por la similitud obtenida\n",
        "for i in results.argsort()[-10:][::-1]:\n",
        "    print(data.iloc[i,2],\"--\",results[i],\"--\",i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "rOKu2It3FRq9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Es tiempo de recogimiento ?. Esta Semana Santa no se sale. #SemanaSanta #YoMeQuedoEnCasa #QuedateEnCasa -- 0.4618900044681435 -- 4863\n",
            "¡Feliz Semana Santa! Aunque sea en casa ?. #AyudaADomicilio #Mayores #QuéDateEnCasa #Santander #Cantabria -- 0.4044128930022516 -- 1967\n",
            "La procesión de carros de compra que estoy viendo desde mi balcón sirve como Semana Santa? #YoMeQuedoEnCasa -- 0.38389199667649315 -- 156\n",
            "Buen dia amigos espero que estén todos bien!!! Feliz inicio de semana arriba ese ánimo todo lo mejor, que sea una Semana Santa de bendiciones!!! A cuidarse . #QuedateEnCasa -- 0.38070852809993044 -- 31\n",
            "Aunque sean unos momentos difíciles, debemos tomarlo con el mayor positivismo posible! La Semana Santa no es sólo salir a beber y a ver procesiones, es mucho más que eso. Disfrutemos de esta semana de una forma verdadera! Feliz Domingo de Ramos y Semana Santa!!! #YoMeQuedoEnCasa -- 0.37318611827279735 -- 804\n",
            "Último día de teletrabajo hasta después de Semana Santa ???. Gracias equipo TeSera por vuestro compromiso y responsabilidad #todosjuntospodemos #yomequedoencasa ?????? -- 0.3505556476212132 -- 1353\n",
            "#QuedateEnCasa #YoMeQuedoEnCasa #COVIDー19 #JuntosNosCuidamos Disfrutando de la Semana Santa atraves de la plataforma @NefliCofrades ¡MUCHAS GRACIAS A TODO EL EQUIPO! ???? -- 0.34283890481106116 -- 813\n",
            "Esta Semana Santa quiero salvar vidas: #YoMeQuedoEnCasa . Si te vas de “vacaciones”, ¿cuántas muertes caerán sobre tus espaldas la semana que viene...?. #QuedateEnTuCasa ?? -- 0.342632848663025 -- 1372\n",
            "¡Feliz Semana Santa en casa!. Laboratorios Mahen, ¡siente la conexión con la naturaleza! ?. #SemanaSanta #YoMeQuedoEnCasa #LaboratoriosMahen -- 0.33651344818376056 -- 1327\n",
            "A este paso no creo que las escuelas abran hasta después de Semana Santa y ya veremos si más allá. #coronavirus #COVID19 #quedatencasa #estadodealarma -- 0.33605409906504957 -- 4170\n"
          ]
        }
      ],
      "source": [
        "# Obtenemos ahora los resultados usando el BM25\n",
        "query_vec = count_vect.transform([query]) # Ip -- (n_docs,x), Op -- (n_docs,n_Feats)\n",
        "query_vec_BM25 = bm25_transformer.fit_transform(query_vec)\n",
        "results = cosine_similarity(X_BM25,query_vec_BM25).reshape((-1,)) # Op -- (n_docs,1) -- Cosine Sim with each doc\n",
        "\n",
        "# Immprimimos a continuación los primeros 10 resultados ordenados por la similitud obtenida\n",
        "for i in results.argsort()[-10:][::-1]:\n",
        "    print(data.iloc[i,2],\"--\",results[i],\"--\",i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHhl2wy4w8Ka"
      },
      "source": [
        "#Apartado 1.5 Modificamos el buscador anterior para trabajar con bigramas o trigramas\n",
        "\n",
        "Configuraremos el CountVectorizer para trabajar con unigramas, bigramas y trigramas y probamos distintas consultas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "U6wNn3uKw7qa"
      },
      "outputs": [],
      "source": [
        "# Calculamos la matriz de TF usando la función fit_transform\n",
        "###########################################################\n",
        "\n",
        "# Calculamos ahora el TFIDF\n",
        "X_TFIDF = tfidf_transformer.fit_transform(X_counts)\n",
        "\n",
        "# Calculamos también el BM25\n",
        "X_BM25 = bm25_transformer.fit_transform(X_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "ExmVzvL-xcVJ"
      },
      "outputs": [],
      "source": [
        "query = \"semana santa\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "M5-TKMQBxc68"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Aunque sean unos momentos difíciles, debemos tomarlo con el mayor positivismo posible! La Semana Santa no es sólo salir a beber y a ver procesiones, es mucho más que eso. Disfrutemos de esta semana de una forma verdadera! Feliz Domingo de Ramos y Semana Santa!!! #YoMeQuedoEnCasa -- 0.5341953468689503 -- 804\n",
            "Es tiempo de recogimiento ?. Esta Semana Santa no se sale. #SemanaSanta #YoMeQuedoEnCasa #QuedateEnCasa -- 0.4618900044681435 -- 4863\n",
            "Buen dia amigos espero que estén todos bien!!! Feliz inicio de semana arriba ese ánimo todo lo mejor, que sea una Semana Santa de bendiciones!!! A cuidarse . #QuedateEnCasa -- 0.450087834395828 -- 31\n",
            "Esta Semana Santa quiero salvar vidas: #YoMeQuedoEnCasa . Si te vas de “vacaciones”, ¿cuántas muertes caerán sobre tus espaldas la semana que viene...?. #QuedateEnTuCasa ?? -- 0.40809989875165065 -- 1372\n",
            "¡Feliz Semana Santa! Aunque sea en casa ?. #AyudaADomicilio #Mayores #QuéDateEnCasa #Santander #Cantabria -- 0.4044128930022516 -- 1967\n",
            "La procesión de carros de compra que estoy viendo desde mi balcón sirve como Semana Santa? #YoMeQuedoEnCasa -- 0.38389199667649315 -- 156\n",
            "Último día de teletrabajo hasta después de Semana Santa ???. Gracias equipo TeSera por vuestro compromiso y responsabilidad #todosjuntospodemos #yomequedoencasa ?????? -- 0.3505556476212133 -- 1353\n",
            "#QuedateEnCasa #YoMeQuedoEnCasa #COVIDー19 #JuntosNosCuidamos Disfrutando de la Semana Santa atraves de la plataforma @NefliCofrades ¡MUCHAS GRACIAS A TODO EL EQUIPO! ???? -- 0.34283890481106116 -- 813\n",
            "¡Feliz Semana Santa en casa!. Laboratorios Mahen, ¡siente la conexión con la naturaleza! ?. #SemanaSanta #YoMeQuedoEnCasa #LaboratoriosMahen -- 0.33651344818376056 -- 1327\n",
            "A este paso no creo que las escuelas abran hasta después de Semana Santa y ya veremos si más allá. #coronavirus #COVID19 #quedatencasa #estadodealarma -- 0.33605409906504957 -- 4170\n"
          ]
        }
      ],
      "source": [
        "# Transformamos la query a TF-ID y sacamos los resultados de la comparación con la función del coseno\n",
        "# cosine_similarity\n",
        "query_vec = count_vect.transform([query]) # Ip -- (n_docs,x), Op -- (n_docs,n_Feats)\n",
        "query_vec_TFIDF = tfidf_transformer.fit_transform(query_vec)\n",
        "results = cosine_similarity(X_TFIDF,query_vec_TFIDF).reshape((-1,)) # Op -- (n_docs,1) -- Cosine Sim with each doc\n",
        "\n",
        "# Immprimimos a continuación los primeros 10 resultados ordenados por la similitud obtenida\n",
        "for i in results.argsort()[-10:][::-1]:\n",
        "    print(data.iloc[i,2],\"--\",results[i],\"--\",i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "gNTwbYEpxfbc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Es tiempo de recogimiento ?. Esta Semana Santa no se sale. #SemanaSanta #YoMeQuedoEnCasa #QuedateEnCasa -- 0.4618900044681435 -- 4863\n",
            "¡Feliz Semana Santa! Aunque sea en casa ?. #AyudaADomicilio #Mayores #QuéDateEnCasa #Santander #Cantabria -- 0.4044128930022516 -- 1967\n",
            "La procesión de carros de compra que estoy viendo desde mi balcón sirve como Semana Santa? #YoMeQuedoEnCasa -- 0.38389199667649315 -- 156\n",
            "Buen dia amigos espero que estén todos bien!!! Feliz inicio de semana arriba ese ánimo todo lo mejor, que sea una Semana Santa de bendiciones!!! A cuidarse . #QuedateEnCasa -- 0.38070852809993044 -- 31\n",
            "Aunque sean unos momentos difíciles, debemos tomarlo con el mayor positivismo posible! La Semana Santa no es sólo salir a beber y a ver procesiones, es mucho más que eso. Disfrutemos de esta semana de una forma verdadera! Feliz Domingo de Ramos y Semana Santa!!! #YoMeQuedoEnCasa -- 0.37318611827279735 -- 804\n",
            "Último día de teletrabajo hasta después de Semana Santa ???. Gracias equipo TeSera por vuestro compromiso y responsabilidad #todosjuntospodemos #yomequedoencasa ?????? -- 0.3505556476212132 -- 1353\n",
            "#QuedateEnCasa #YoMeQuedoEnCasa #COVIDー19 #JuntosNosCuidamos Disfrutando de la Semana Santa atraves de la plataforma @NefliCofrades ¡MUCHAS GRACIAS A TODO EL EQUIPO! ???? -- 0.34283890481106116 -- 813\n",
            "Esta Semana Santa quiero salvar vidas: #YoMeQuedoEnCasa . Si te vas de “vacaciones”, ¿cuántas muertes caerán sobre tus espaldas la semana que viene...?. #QuedateEnTuCasa ?? -- 0.342632848663025 -- 1372\n",
            "¡Feliz Semana Santa en casa!. Laboratorios Mahen, ¡siente la conexión con la naturaleza! ?. #SemanaSanta #YoMeQuedoEnCasa #LaboratoriosMahen -- 0.33651344818376056 -- 1327\n",
            "A este paso no creo que las escuelas abran hasta después de Semana Santa y ya veremos si más allá. #coronavirus #COVID19 #quedatencasa #estadodealarma -- 0.33605409906504957 -- 4170\n"
          ]
        }
      ],
      "source": [
        "# Obtenemos ahora los resultados usando el BM25\n",
        "query_vec = count_vect.transform([query]) # Ip -- (n_docs,x), Op -- (n_docs,n_Feats)\n",
        "query_vec_BM25 = bm25_transformer.fit_transform(query_vec)\n",
        "results = cosine_similarity(X_BM25,query_vec_BM25).reshape((-1,)) # Op -- (n_docs,1) -- Cosine Sim with each doc\n",
        "\n",
        "# Immprimimos a continuación los primeros 10 resultados ordenados por la similitud obtenida\n",
        "for i in results.argsort()[-10:][::-1]:\n",
        "    print(data.iloc[i,2],\"--\",results[i],\"--\",i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5kye10CTYDn"
      },
      "source": [
        "## Apartado 1.6 Calculamos la similitud de varios textos con TF-IDF y BM25 (Resuelto)\n",
        "\n",
        "Vamos a calcular la similitud de el primer texto con respecto a los demás usando la similitud del coseno."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "9sBC9HP8TYDn"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Similitud de textos con TFIDF: [0.82218193 0.16800859 0.03486424 0.15382285]\n",
            "Similitud de textos con TFIDF: [0.7465174  0.1372938  0.03771061 0.1419987 ]\n"
          ]
        }
      ],
      "source": [
        "# Definimos un conjunto de textos\n",
        "textos=['El procesamiento del lenguaje natural (PLN o NLP) es un campo dentro de la inteligencia artificial y la lingüística aplicada que estudia las interacciones mediante uso del lenguaje natural entre los seres humanos y las máquinas. \\\n",
        "Más concretamente se centra en el procesamiento de las comunicaciones humanas, dividiéndolas en partes, e identificando los elementos más relevantes del mensaje.\\\n",
        "Con la Comprensión y Generación de Lenguaje Natural, busca que las máquinas consigan entender, interpretar y manipular el lenguaje humano.'\n",
        ", 'El procesamiento del lenguaje natural (NLP, por sus siglas en inglés) es una rama de la inteligencia artificial que ayuda a las computadoras a entender, interpretar y manipular el lenguaje humano. \\\n",
        "NLP toma elementos prestados de muchas disciplinas, incluyendo la ciencia de la computación y la lingüística computacional, en su afán por cerrar la brecha entre la comunicación humana y el entendimiento de las computadoras.\"\"\"], \"\"\"El procesamiento del lenguaje natural (PLN o NLP) es un campo dentro de la inteligencia artificial y la lingüística aplicada que estudia las interacciones mediante uso del lenguaje natural entre los seres humanos y las máquinas. Más concretamente se centra en el procesamiento de las comunicaciones humanas, dividiéndolas en partes, e identificando los elementos más relevantes del mensaje. Con la Comprensión y Generación de Lenguaje Natural, busca que las máquinas consigan entender, interpretar y manipular el lenguaje humano.'\n",
        ", 'La lingüística computacional es un campo interdisciplinario que se ocupa del desarrollo de formalismos del funcionamiento del lenguaje natural, tales que puedan ser transformados en programas ejecutables para un ordenador. \\\n",
        "Dicho desarrollo se sitúa entre el modelado basado en reglas y el modelado estadístico del lenguaje natural desde una perspectiva computacional, y en él participan lingüistas e informáticos especializados en inteligencia artificial, psicólogos cognoscitivos y expertos en lógica, entre otros.'\n",
        ", 'El aprendizaje automático es un tipo de inteligencia artificial (AI) que proporciona a las computadoras la capacidad de aprender, sin ser programadas explícitamente. El aprendizaje automático se centra en el desarrollo de programas informáticos que pueden cambiar cuando se exponen a nuevos datos.'\n",
        ", 'El  aprendizaje profundo es un tema que cada vez adquiere mayor relevancia en el campo de la inteligencia artificial (IA). Siendo una subcategoría del aprendizaje automático, el aprendizaje profundo trata del uso de redes neuronales para mejorar cosas tales como el reconocimiento de voz, la visión por ordenador y el procesamiento del lenguaje natural. \\\n",
        "Rápidamente se está convirtiendo en uno de los campos más solicitados en informática. \\\n",
        "En los últimos años, el aprendizaje profundo ha ayudado a lograr avances en áreas tan diversas como la percepción de objetos, el procesamiento del lenguaje natural y el reconocimiento de voz (todas ellas áreas especialmente complejas para los investigadores en IA).'\n",
        "]\n",
        "\n",
        "# Calculamos la similitud usando TFIDF\n",
        "count_vect = CountVectorizer(stop_words=stopwords_sp)\n",
        "X_counts = count_vect.fit_transform(textos)\n",
        "\n",
        "# Calculamos ahora el TFIDF\n",
        "tfidf_transformer = TfidfTransformer()\n",
        "X_TFIDF = tfidf_transformer.fit_transform(X_counts)\n",
        "\n",
        "# Calculamos también el BM25\n",
        "BM25_transformer = BM25Transformer(k=1.2,b=0.75)\n",
        "X_BM25 = BM25_transformer.fit_transform(X_counts)\n",
        "\n",
        "# Calculamos la similitud de los documentos con el coseno para TFIDF\n",
        "results = cosine_similarity(X_TFIDF[1::],X_TFIDF[0]).reshape((-1,)) # Op -- (n_docs,1) -- Cosine Sim with each doc\n",
        "print(\"Similitud de textos con TFIDF:\", results)\n",
        "\n",
        "# Calculamos la similitud de los textos con el coseno para BM25\n",
        "results = cosine_similarity(X_BM25[1::],X_TFIDF[0]).reshape((-1,)) # Op -- (n_docs,1) -- Cosine Sim with each doc\n",
        "print(\"Similitud de textos con TFIDF:\",results)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxdP7JACcBqH"
      },
      "source": [
        "## 1.7 Cambiando a character n-grams\n",
        "Podemos usar en vez de palabras (words), character n-grams para crear el vocabulario. Lo vemos con el primer ejemplo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "KtOJF8z9TYDn"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(3, 185)\n",
            "La asignatura de TGINE es una asignatura del máster de BigData que se estudia en la Universidad de Murcia.\n",
            "En la asignatura de máster vemos una introducción al procesamiento del lenguaje natural y tecnologías de procesamiento\n",
            "de información no estructurada.\n",
            "\n",
            "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
            "\twith 140 stored elements and shape (1, 185)>\n",
            "  Coords\tValues\n",
            "  (0, 12)\t3\n",
            "  (0, 100)\t3\n",
            "  (0, 1)\t3\n",
            "  (0, 42)\t3\n",
            "  (0, 152)\t3\n",
            "  (0, 93)\t3\n",
            "  (0, 83)\t3\n",
            "  (0, 112)\t4\n",
            "  (0, 45)\t4\n",
            "  (0, 167)\t5\n",
            "  (0, 175)\t5\n",
            "  (0, 135)\t3\n",
            "  (0, 4)\t8\n",
            "  (0, 59)\t6\n",
            "  (0, 25)\t1\n",
            "  (0, 162)\t1\n",
            "  (0, 82)\t1\n",
            "  (0, 94)\t1\n",
            "  (0, 114)\t1\n",
            "  (0, 7)\t3\n",
            "  (0, 72)\t1\n",
            "  (0, 26)\t3\n",
            "  (0, 173)\t2\n",
            "  (0, 111)\t2\n",
            "  (0, 60)\t2\n",
            "  :\t:\n",
            "  (0, 52)\t1\n",
            "  (0, 120)\t1\n",
            "  (0, 129)\t1\n",
            "  (0, 104)\t1\n",
            "  (0, 127)\t1\n",
            "  (0, 86)\t1\n",
            "  (0, 183)\t1\n",
            "  (0, 41)\t1\n",
            "  (0, 95)\t1\n",
            "  (0, 115)\t1\n",
            "  (0, 78)\t1\n",
            "  (0, 131)\t1\n",
            "  (0, 142)\t1\n",
            "  (0, 105)\t1\n",
            "  (0, 31)\t1\n",
            "  (0, 19)\t1\n",
            "  (0, 119)\t1\n",
            "  (0, 156)\t1\n",
            "  (0, 165)\t1\n",
            "  (0, 147)\t1\n",
            "  (0, 170)\t1\n",
            "  (0, 54)\t1\n",
            "  (0, 136)\t1\n",
            "  (0, 33)\t1\n",
            "  (0, 56)\t1\n",
            "No me gusta el chocolate ni las fresas\n",
            "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
            "\twith 30 stored elements and shape (1, 185)>\n",
            "  Coords\tValues\n",
            "  (0, 12)\t1\n",
            "  (0, 64)\t1\n",
            "  (0, 158)\t1\n",
            "  (0, 73)\t1\n",
            "  (0, 41)\t2\n",
            "  (0, 19)\t1\n",
            "  (0, 119)\t1\n",
            "  (0, 14)\t1\n",
            "  (0, 106)\t1\n",
            "  (0, 10)\t1\n",
            "  (0, 85)\t1\n",
            "  (0, 177)\t1\n",
            "  (0, 154)\t1\n",
            "  (0, 5)\t1\n",
            "  (0, 3)\t1\n",
            "  (0, 49)\t1\n",
            "  (0, 87)\t1\n",
            "  (0, 124)\t1\n",
            "  (0, 53)\t1\n",
            "  (0, 128)\t1\n",
            "  (0, 102)\t1\n",
            "  (0, 44)\t1\n",
            "  (0, 159)\t1\n",
            "  (0, 18)\t1\n",
            "  (0, 117)\t1\n",
            "  (0, 101)\t1\n",
            "  (0, 8)\t1\n",
            "  (0, 79)\t1\n",
            "  (0, 141)\t1\n",
            "  (0, 149)\t1\n",
            "El profesor de la asignatura TGINE es Rafael Valencia García.\n",
            "\n",
            "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
            "\twith 51 stored elements and shape (1, 185)>\n",
            "  Coords\tValues\n",
            "  (0, 12)\t1\n",
            "  (0, 100)\t1\n",
            "  (0, 1)\t1\n",
            "  (0, 42)\t1\n",
            "  (0, 152)\t1\n",
            "  (0, 93)\t1\n",
            "  (0, 83)\t1\n",
            "  (0, 112)\t1\n",
            "  (0, 45)\t1\n",
            "  (0, 167)\t1\n",
            "  (0, 175)\t1\n",
            "  (0, 135)\t1\n",
            "  (0, 4)\t1\n",
            "  (0, 59)\t1\n",
            "  (0, 25)\t1\n",
            "  (0, 162)\t1\n",
            "  (0, 82)\t1\n",
            "  (0, 94)\t1\n",
            "  (0, 114)\t1\n",
            "  (0, 7)\t1\n",
            "  (0, 72)\t1\n",
            "  (0, 64)\t2\n",
            "  (0, 88)\t1\n",
            "  (0, 50)\t1\n",
            "  (0, 30)\t1\n",
            "  :\t:\n",
            "  (0, 133)\t1\n",
            "  (0, 103)\t1\n",
            "  (0, 5)\t1\n",
            "  (0, 145)\t1\n",
            "  (0, 126)\t1\n",
            "  (0, 77)\t1\n",
            "  (0, 74)\t1\n",
            "  (0, 153)\t1\n",
            "  (0, 130)\t1\n",
            "  (0, 22)\t1\n",
            "  (0, 137)\t1\n",
            "  (0, 35)\t1\n",
            "  (0, 76)\t1\n",
            "  (0, 34)\t1\n",
            "  (0, 27)\t1\n",
            "  (0, 178)\t1\n",
            "  (0, 38)\t1\n",
            "  (0, 67)\t1\n",
            "  (0, 113)\t1\n",
            "  (0, 9)\t1\n",
            "  (0, 80)\t1\n",
            "  (0, 40)\t1\n",
            "  (0, 140)\t1\n",
            "  (0, 55)\t1\n",
            "  (0, 182)\t1\n",
            "Mostramos los items del diccionario\n",
            "dict_items([(' la', 12), ('la ', 100), (' as', 1), ('asi', 42), ('sig', 152), ('ign', 93), ('gna', 83), ('nat', 112), ('atu', 45), ('tur', 167), ('ura', 175), ('ra ', 135), (' de', 4), ('de ', 59), (' tg', 25), ('tgi', 162), ('gin', 82), ('ine', 94), ('ne ', 114), (' es', 7), ('es ', 72), (' un', 26), ('una', 173), ('na ', 111), ('del', 60), ('el ', 64), (' má', 16), ('más', 110), ('ást', 181), ('ste', 155), ('ter', 161), ('er ', 70), (' bi', 2), ('big', 46), ('igd', 92), ('gda', 81), ('dat', 58), ('ata', 43), ('ta ', 158), (' qu', 21), ('que', 134), ('ue ', 172), (' se', 23), ('se ', 150), ('est', 75), ('stu', 157), ('tud', 166), ('udi', 171), ('dia', 61), ('ia ', 88), (' en', 6), ('en ', 66), ('uni', 174), ('niv', 118), ('ive', 97), ('ver', 180), ('ers', 71), ('rsi', 146), ('sid', 151), ('ida', 90), ('dad', 57), ('ad ', 32), (' mu', 15), ('mur', 109), ('urc', 176), ('rci', 139), ('cia', 50), ('ia.', 89), ('a. ', 30), (' ve', 28), ('vem', 179), ('emo', 65), ('mos', 108), ('os ', 132), (' in', 11), ('int', 96), ('ntr', 122), ('tro', 164), ('rod', 144), ('odu', 125), ('duc', 62), ('ucc', 169), ('cci', 47), ('ció', 51), ('ión', 98), ('ón ', 184), (' al', 0), ('al ', 37), (' pr', 20), ('pro', 133), ('roc', 143), ('oce', 123), ('ces', 48), ('esa', 73), ('sam', 148), ('ami', 39), ('mie', 107), ('ien', 91), ('ent', 69), ('nto', 121), ('to ', 163), (' le', 13), ('len', 103), ('eng', 68), ('ngu', 116), ('gua', 84), ('uaj', 168), ('aje', 36), ('je ', 99), (' na', 17), ('ral', 138), (' y ', 29), (' te', 24), ('tec', 160), ('ecn', 63), ('cno', 52), ('nol', 120), ('olo', 129), ('log', 104), ('ogí', 127), ('gía', 86), ('ías', 183), ('as ', 41), ('inf', 95), ('nfo', 115), ('for', 78), ('orm', 131), ('rma', 142), ('mac', 105), ('aci', 31), (' no', 19), ('no ', 119), ('str', 156), ('tru', 165), ('ruc', 147), ('uct', 170), ('ctu', 54), ('rad', 136), ('ada', 33), ('da.', 56), (' me', 14), ('me ', 106), (' gu', 10), ('gus', 85), ('ust', 177), ('sta', 154), (' el', 5), (' ch', 3), ('cho', 49), ('hoc', 87), ('oco', 124), ('col', 53), ('ola', 128), ('lat', 102), ('ate', 44), ('te ', 159), (' ni', 18), ('ni ', 117), ('las', 101), (' fr', 8), ('fre', 79), ('res', 141), ('sas', 149), ('rof', 145), ('ofe', 126), ('fes', 77), ('eso', 74), ('sor', 153), ('or ', 130), (' ra', 22), ('raf', 137), ('afa', 35), ('fae', 76), ('ael', 34), (' va', 27), ('val', 178), ('ale', 38), ('enc', 67), ('nci', 113), (' ga', 9), ('gar', 80), ('arc', 40), ('rcí', 140), ('cía', 55), ('ía.', 182)])\n",
            "Tamaño vocabulario: 185\n",
            "Código de la palabra TGI es: 162\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\X542UA\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:543: UserWarning: The parameter 'stop_words' will not be used since 'analyzer' != 'word'\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "texto = \"\"\"La asignatura de TGINE es una asignatura del máster de BigData que se estudia en la Universidad de Murcia.\n",
        "En la asignatura de máster vemos una introducción al procesamiento del lenguaje natural y tecnologías de procesamiento\n",
        "de información no estructurada.\n",
        "\"\"\"\n",
        "texto2 = \"No me gusta el chocolate ni las fresas\"\n",
        "\n",
        "texto3 = \"\"\"El profesor de la asignatura TGINE es Rafael Valencia García.\n",
        "\"\"\"\n",
        "# Calculamos la matriz de TF usando la función fit_transform\n",
        "############################################################\n",
        "\n",
        "# Mostramos entonces el número de textos y el número de tokens únicos\n",
        "count_vect = CountVectorizer(analyzer=\"char_wb\", stop_words=stopwords_sp, ngram_range=(3,3))\n",
        "X_counts = count_vect.fit_transform([texto, texto2, texto3])\n",
        "print(X_counts.shape)\n",
        "\n",
        "# X_counts es una matriz dispersa con el TF de cada token en cada texto\n",
        "# Imprimimos los textos y su correspondientes TF\n",
        "print(texto)\n",
        "print(X_counts[0])\n",
        "print(texto2)\n",
        "print(X_counts[1])\n",
        "print(texto3)\n",
        "print(X_counts[2])\n",
        "\n",
        "#Los tokens de todo el vocabulario se representan con ids que hacen referencia a cada token.\n",
        "print(\"Mostramos los items del diccionario\")\n",
        "print(count_vect.vocabulary_.items())\n",
        "print(\"Tamaño vocabulario:\", str(len(count_vect.vocabulary_.items())))\n",
        "\n",
        "#Mostramos el código de una palabra determinada\n",
        "#hay que tener en cuenta que todos los tokens se guardan en minúsculas\n",
        "palabra_a_buscar=\"TGI\"\n",
        "print(\"Código de la palabra\", palabra_a_buscar, \"es:\", count_vect.vocabulary_.get(palabra_a_buscar.lower()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
