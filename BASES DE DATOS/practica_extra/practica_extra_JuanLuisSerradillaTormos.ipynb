{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elasticsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ACLARACIÓN:**\n",
    "\n",
    "El cuaderno no está preparado para ser ejecutado dentro de Colab. En su lugar, hay que seguir las instrucciones que se detallan en los respectivos apartados para instalar y ejecutar Elasticsearch en un entorno local."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementación de ElasticSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la instalación de Elasticsearch se ha usdo el contenido del resopositorio [docker-elk](https://github.com/deviantony/docker-elk?tab=readme-ov-file). Cualquier duda sobre la instalación que no aparezca en este documento se puede consultar en el repositorio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instalación del repositorio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo primero de todo es clonar el repositorio en el directorio de trabajo:\n",
    "\n",
    "```bash\n",
    "git clone https://github.com/deviantony/docker-elk?tab=readme-ov-file\n",
    "```\n",
    "\n",
    "Una vez tenemos el direcotorio clonado, nos movemos a la carpeta `docker-elk`. Podemos ver en esta carpeta dos archivos importantes:\n",
    "- `docker-compose.yml`: Archivo de configuración de docker-compose.\n",
    "- `.env`: Archivo de configuración del cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modificación de credenciales <a id=\"modificacion-de-credenciales\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para modificar las credenciales de acceso a la interfaz web, debemos modificar el archivo `.env`. En este archivo podemos encontrar las siguientes líneas:\n",
    "\n",
    "```bash\n",
    "## Passwords for stack users\n",
    "#\n",
    "\n",
    "# User 'elastic' (built-in)\n",
    "#\n",
    "# Superuser role, full access to cluster management and data indices.\n",
    "# https://www.elastic.co/guide/en/elasticsearch/reference/current/built-in-users.html\n",
    "ELASTIC_PASSWORD='changeme'\n",
    "\n",
    "# User 'logstash_internal' (custom)\n",
    "#\n",
    "# The user Logstash uses to connect and send data to Elasticsearch.\n",
    "# https://www.elastic.co/guide/en/logstash/current/ls-security.html\n",
    "LOGSTASH_INTERNAL_PASSWORD='changeme'\n",
    "\n",
    "# User 'kibana_system' (built-in)\n",
    "#\n",
    "# The user Kibana uses to connect and communicate with Elasticsearch.\n",
    "# https://www.elastic.co/guide/en/elasticsearch/reference/current/built-in-users.html\n",
    "KIBANA_SYSTEM_PASSWORD='changeme'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De esta forma se pueden modificar los valores de las contraseñas antes de instalar e iniciar el cluster. Sin embargo, los usuarios son los que hay por defecto y no se pueden cambiar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instalación de Elasticsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para iniciar la instlación de Elasticsearch, simplemente ejecutamos el siguiente comando dentro de la carpeta `docker-elk`:\n",
    "\n",
    "```bash\n",
    "docker-compose up setup\n",
    "```\n",
    "\n",
    "Luego, una vez se haya terminado de instalar, iniciamos el docker-compose de forma normal:\n",
    "\n",
    "```bash\n",
    "docker-compose up\n",
    "```\n",
    "\n",
    "Para comprobar que todo ha ido bien y está el cluster funcionando, podemos acceder a la dirección `http://localhost:9200` y veremos la información del cluster (hay que meterse con las credenciales: `elastic` y `changeme`, en el caso de no haber cambiaddo la contraseña como se indica enel [apartado anterior](#modificacion-de-credenciales)). Se puede ejeccutar en Bash como:\n",
    "\n",
    "```bash\n",
    "# En el caso de Powershell \"curl.exe\" en vez de solo \"curl\"\n",
    "curl -u elastic:changeme http://localhost:9200\n",
    "```\n",
    "\n",
    "Debería devolver algo similar a lo siguiente:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"name\": \"elasticsearch\",\n",
    "  \"cluster_name\": \"docker-cluster\",\n",
    "  \"cluster_uuid\": \"CeqMQi1oRvqLxuKa9Lejlg\",\n",
    "  \"version\": {\n",
    "    \"number\": \"8.17.0\",\n",
    "    \"build_flavor\": \"default\",\n",
    "    \"build_type\": \"docker\",\n",
    "    \"build_hash\": \"2b6a7fed44faa321997703718f07ee0420804b41\",\n",
    "    \"build_date\": \"2024-12-11T12:08:05.663969764Z\",\n",
    "    \"build_snapshot\": false,\n",
    "    \"lucene_version\": \"9.12.0\",\n",
    "    \"minimum_wire_compatibility_version\": \"7.17.0\",\n",
    "    \"minimum_index_compatibility_version\": \"7.0.0\"\n",
    "  },\n",
    "  \"tagline\": \"You Know, for Search\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descripción de Elasticsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elasticsearch es una base de datos NoSQL orientada a documentos que utiliza un motor de búsqueda basado en Apache Lucene. Está diseñada para proporcionar un análisis rápido y eficiente de grandes volúmenes de datos y se utiliza comúnmente en aplicaciones de búsqueda y análisis de logs, métricas, contenido en tiempo real, entre otros.\n",
    "\n",
    "### Modo de funcionamiento:\n",
    "Elasticsearch funciona de manera distribuida, lo que significa que puede escalar horizontalmente agregando más nodos a un clúster. Los datos en Elasticsearch se almacenan en índices, que se dividen en shards (fragmentos) y pueden ser replicados para asegurar la disponibilidad y la durabilidad.\n",
    "\n",
    "### Posibilidades de modelado de datos:\n",
    "- **Índices**: Los datos se organizan en índices, y cada índice puede tener múltiples tipos de documentos. Un documento es una unidad de datos (por ejemplo, un registro) que se almacena en formato JSON.\n",
    "- **Campos**: Los documentos se componen de campos (pares clave-valor). Los tipos de datos en Elasticsearch incluyen cadenas, enteros, fechas, booleanos, entre otros.\n",
    "- **Mapping**: Elasticsearch permite definir el tipo de datos y el comportamiento de los campos a través del \"mapping\", lo que permite optimizar las búsquedas y el almacenamiento de los datos.\n",
    "\n",
    "### Características principales:\n",
    "- **Transacciones**: Elasticsearch no está diseñado para manejar transacciones tradicionales con ACID (Atomicidad, Consistencia, Aislamiento, Durabilidad). Aunque ofrece operaciones de escritura, no garantiza la consistencia inmediata entre nodos, ya que su enfoque está en la alta disponibilidad y la escalabilidad. En su lugar, ofrece mecanismos de consistencia eventual.\n",
    "\n",
    "- **Organización en etiquetas**: Elasticsearch no organiza los datos de manera jerárquica en términos de etiquetas, pero se pueden usar filtros y facetas para organizar y agrupar la información durante la búsqueda. Además, se pueden usar etiquetas como valores en los documentos para facilitar su clasificación.\n",
    "\n",
    "- **Búsquedas complejas**: Elasticsearch es extremadamente potente para realizar búsquedas complejas. Ofrece una amplia gama de consultas como:\n",
    "  - **Búsquedas de texto completo**: búsqueda por coincidencia, búsqueda booleana, búsqueda de frases, etc.\n",
    "  - **Filtros**: para limitar los resultados de las búsquedas.\n",
    "  - **Agregaciones**: para análisis de datos como sumas, promedios, máximos, mínimos, y más.\n",
    "  - **Búsqueda fuzzy**: que permite encontrar coincidencias aproximadas.\n",
    "  - **Consultas geoespaciales**: para realizar búsquedas basadas en la ubicación geográfica.\n",
    "\n",
    "- **Replicación multiservidor**: Elasticsearch permite replicar los datos entre múltiples nodos en un clúster. Esto proporciona redundancia, alta disponibilidad y equilibrio de carga. Cada shard puede tener réplicas en otros nodos, lo que asegura que los datos sigan disponibles incluso si un nodo falla.\n",
    "\n",
    "- **Lenguaje de consultas**: Elasticsearch utiliza su propio **DSL (Domain Specific Language)** para consultas, que es una forma declarativa basada en JSON. Con este lenguaje se pueden construir consultas complejas, agregaciones, filtrados y ordenamientos. Además, es posible interactuar con Elasticsearch a través de su API RESTful, que facilita su integración con otros sistemas.\n",
    "\n",
    "### Otras características:\n",
    "- **Escalabilidad horizontal**: Elasticsearch está diseñado para escalar horizontalmente, agregando más nodos al clúster para manejar grandes volúmenes de datos y consultas.\n",
    "- **Alta disponibilidad**: A través de la replicación y el control de fallos, Elasticsearch garantiza que el sistema continúe funcionando correctamente incluso si algunos nodos fallan.\n",
    "- **Tiempo real**: Aunque Elasticsearch no es estrictamente \"en tiempo real\", tiene una latencia baja y permite búsquedas en casi tiempo real, lo que lo hace ideal para sistemas de análisis de logs, monitoreo de eventos y otros escenarios similares.\n",
    "\n",
    "En resumen, Elasticsearch es una base de datos de búsqueda potente, distribuida y escalable, que permite realizar búsquedas complejas, realizar análisis de datos y manejar grandes volúmenes de información con alta disponibilidad y replicación entre servidores. Sin embargo, no está diseñado para gestionar transacciones tradicionales con consistencia fuerte.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargas de datos a Elasticseatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instalación de librerías"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para poder usar Elasticsearch necesitaremos instalar las librerías:\n",
    "- `elasticsearch`\n",
    "- `pandas`\n",
    "\n",
    "Para ello, simplemente ejecutamos el siguiente comando:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: elasticsearch in d:\\juanluhijo\\.venv\\lib\\site-packages (8.17.0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: pandas in d:\\juanluhijo\\.venv\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: elasticsearch_dsl in d:\\juanluhijo\\.venv\\lib\\site-packages (8.17.1)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.0-cp310-cp310-win_amd64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: elastic-transport<9,>=8.15.1 in d:\\juanluhijo\\.venv\\lib\\site-packages (from elasticsearch) (8.15.1)\n",
      "Requirement already satisfied: numpy>=1.22.4 in d:\\juanluhijo\\.venv\\lib\\site-packages (from pandas) (2.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\juanluhijo\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\juanluhijo\\.venv\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\juanluhijo\\.venv\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: typing-extensions in d:\\juanluhijo\\.venv\\lib\\site-packages (from elasticsearch_dsl) (4.12.2)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.1-cp310-cp310-win_amd64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.55.3-cp310-cp310-win_amd64.whl.metadata (168 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.8-cp310-cp310-win_amd64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\juanluhijo\\.venv\\lib\\site-packages (from matplotlib) (24.2)\n",
      "Collecting pillow>=8 (from matplotlib)\n",
      "  Downloading pillow-11.1.0-cp310-cp310-win_amd64.whl.metadata (9.3 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Downloading pyparsing-3.2.1-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.26.2 in d:\\juanluhijo\\.venv\\lib\\site-packages (from elastic-transport<9,>=8.15.1->elasticsearch) (2.3.0)\n",
      "Requirement already satisfied: certifi in d:\\juanluhijo\\.venv\\lib\\site-packages (from elastic-transport<9,>=8.15.1->elasticsearch) (2024.12.14)\n",
      "Requirement already satisfied: six>=1.5 in d:\\juanluhijo\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading matplotlib-3.10.0-cp310-cp310-win_amd64.whl (8.0 MB)\n",
      "   ---------------------------------------- 0.0/8.0 MB ? eta -:--:--\n",
      "   ----------------------------------- ---- 7.1/8.0 MB 33.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.0/8.0 MB 33.0 MB/s eta 0:00:00\n",
      "Downloading contourpy-1.3.1-cp310-cp310-win_amd64.whl (218 kB)\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.55.3-cp310-cp310-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.2/2.2 MB 17.8 MB/s eta 0:00:00\n",
      "Downloading kiwisolver-1.4.8-cp310-cp310-win_amd64.whl (71 kB)\n",
      "Downloading pillow-11.1.0-cp310-cp310-win_amd64.whl (2.6 MB)\n",
      "   ---------------------------------------- 0.0/2.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.6/2.6 MB 50.3 MB/s eta 0:00:00\n",
      "Downloading pyparsing-3.2.1-py3-none-any.whl (107 kB)\n",
      "Installing collected packages: pyparsing, pillow, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.3.1 cycler-0.12.1 fonttools-4.55.3 kiwisolver-1.4.8 matplotlib-3.10.0 pillow-11.1.0 pyparsing-3.2.1\n"
     ]
    }
   ],
   "source": [
    "%pip install elasticsearch pandas elasticsearch_dsl matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprobamos que Elasticseatch se está ejecutándose correctamente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Información del clúster:\n",
      "\t- cluster_name : docker-cluster\n",
      "\t- status : green\n",
      "\t- timed_out : False\n",
      "\t- number_of_nodes : 1\n",
      "\t- number_of_data_nodes : 1\n",
      "\t- active_primary_shards : 33\n",
      "\t- active_shards : 33\n",
      "\t- relocating_shards : 0\n",
      "\t- initializing_shards : 0\n",
      "\t- unassigned_shards : 0\n",
      "\t- unassigned_primary_shards : 0\n",
      "\t- delayed_unassigned_shards : 0\n",
      "\t- number_of_pending_tasks : 0\n",
      "\t- number_of_in_flight_fetch : 0\n",
      "\t- task_max_waiting_in_queue_millis : 0\n",
      "\t- active_shards_percent_as_number : 100.0\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch, helpers\n",
    "\n",
    "# Conexión a Elasticsearch con autenticación\n",
    "client = Elasticsearch(\n",
    "    ['http://localhost:9200'],\n",
    "    basic_auth=('elastic', 'changeme')\n",
    ")\n",
    "\n",
    "def obtener_informacion_cluster():\n",
    "    try:\n",
    "        # Obtener información sobre el clúster de Elasticsearch\n",
    "        info_cluster = client.cluster.health()\n",
    "        print(\"Información del clúster:\")\n",
    "        for element in info_cluster:\n",
    "            print(\"\\t-\", element, \":\", info_cluster[element])\n",
    "    except Exception as e:\n",
    "        print(\"Error al obtener información del clúster:\", e)\n",
    "\n",
    "# Llamar a la función para obtener información del clúster\n",
    "obtener_informacion_cluster();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subida de los documentos CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero de todo tenemos que descargar los documentos CSV de Stack Overflow. Estos documentos se encuentran en el [repositorio de la asignatura de Bases de datos](https://github.com/dsevilla/bd2-data/raw/main/es.stackoverflow/). Los archivos que hay que descargar y descomprimir son:\n",
    "- `es.stackoverflow.csv.7z.001`\n",
    "- `es.stackoverflow.csv.7z.002`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  6  100M    6 6334k    0     0  5046k      0  0:00:20  0:00:01  0:00:19 6284k\n",
      " 29  100M   29 29.1M    0     0  12.9M      0  0:00:07  0:00:02  0:00:05 14.5M\n",
      " 78  100M   78 78.5M    0     0  24.1M      0  0:00:04  0:00:03  0:00:01 26.1M\n",
      "100  100M  100  100M    0     0  26.4M      0  0:00:03  0:00:03 --:--:-- 28.3M\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "  0 91.1M    0  476k    0     0  1265k      0  0:01:13 --:--:--  0:01:13 1265k\n",
      " 52 91.1M   52 48.0M    0     0  34.7M      0  0:00:02  0:00:01  0:00:01 47.3M\n",
      "100 91.1M  100 91.1M    0     0  45.6M      0  0:00:01  0:00:01 --:--:-- 56.0M\n"
     ]
    }
   ],
   "source": [
    "# Descargamos los archivos comprimidos\n",
    "!curl -L -o es.stackoverflow.csv.7z.001 https://github.com/dsevilla/bd2-data/raw/main/es.stackoverflow/es.stackoverflow.csv.7z.001\n",
    "!curl -L -o es.stackoverflow.csv.7z.002 https://github.com/dsevilla/bd2-data/raw/main/es.stackoverflow/es.stackoverflow.csv.7z.002"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez tenemos los archivos descargados, los descomprimimos, de forma que deberíamos tener:\n",
    "- `Posts.csv`\n",
    "- `Users.csv`\n",
    "- `Tags.csv`\n",
    "- `Votes.csv`\n",
    "- `Comments.csv`\n",
    "\n",
    "Para la ejecución de la práctica deben estar en la misma carpeta que este Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos una función para cargar los datos de los CSV a Elasticsearch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos una función para pasar de CSV a Elasticsearch\n",
    "import pandas as pd\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "\n",
    "def csv_to_elastic(client: Elasticsearch, df: pd.DataFrame, index: str) -> None:\n",
    "    # Creamos el índice en Elasticsearch si no existe\n",
    "    if not client.indices.exists(index=index):\n",
    "        client.indices.create(index=index)\n",
    "    \n",
    "    # Creamos una lista vacía para ir guardando las filas del CSV\n",
    "    docs: list = []\n",
    "    \n",
    "    # Cambiamos los NaN por None\n",
    "    df = df.fillna(\"NAN\").replace(\"NAN\", None)\n",
    "    \n",
    "    # Creamos un bucle para recorrer las filas del DataFrame\n",
    "    for _, row in df.iterrows():\n",
    "        # Convertimos la fila a un diccionario\n",
    "        content: dict = row.to_dict()\n",
    "        \n",
    "        # Creamos el documento\n",
    "        doc = {\n",
    "            \"_index\": index,\n",
    "            \"_source\": content\n",
    "        }\n",
    "        \n",
    "        # Añadimos el documento a la lista\n",
    "        docs.append(doc)\n",
    "\n",
    "    # Subimos los documentos a Elasticsearch\n",
    "    helpers.bulk(client, docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pasamos por todos los documentos guardando los datos en un DataFrame de pandas y luego los subimos a Elasticsearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pasando el CSV a Elasticsearch: Posts.csv\n",
      "Pasando el CSV a Elasticsearch: Tags.csv\n",
      "Pasando el CSV a Elasticsearch: Users.csv\n",
      "Pasando el CSV a Elasticsearch: Votes.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Creamos el cliente de Elasticsearch\n",
    "es_client: Elasticsearch = Elasticsearch(\n",
    "    ['http://localhost:9200'],\n",
    "    basic_auth=('elastic', 'changeme')\n",
    ")\n",
    "\n",
    "# Hacemos un bucle para pasar por todos los CSV del directorio actual\n",
    "for file in os.listdir(\".\"):\n",
    "    if file.endswith(\".csv\"):\n",
    "        # Leemos el CSV con Pandas\n",
    "        df: pd.DataFrame = pd.read_csv(file)\n",
    "        index: str = file.split(\".\")[0].lower()\n",
    "        \n",
    "        # Pasamos el CSV a Elasticsearch\n",
    "        print(\"Pasando el CSV a Elasticsearch:\", file)\n",
    "        csv_to_elastic(es_client, df, index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
