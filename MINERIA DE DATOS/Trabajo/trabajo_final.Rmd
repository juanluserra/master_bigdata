# Apartado 2.1

```{r}
# Seleccionamos el "path" de este archivo como el directorio de trabajo
if (interactive()) {
    setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
}

# Cargamos las librerías necesarias
library(summarytools)
library(ggplot2)
library(scales)
library(dplyr)
library(tidyr)
library(caret)
library(gridExtra)
library(VIM)
library(doParallel)
library(smotefamily)
```


```{r}
# Leemos el archivo CSV
data <- read.csv("data.csv", sep = ";", stringsAsFactors = TRUE)

# Inspeccionamos las características del dataframe
dim(data)
str(data)
head(data)
```

Podemos ver que el dataframe tiene 24120 valores y 15 predictores. De todos estos predictores, hay 1 que no nos interesa: `kreis_code`. Este es simplemente un código que identifica la región, la cuAl ya se especifica en `kreis`. Por lo tanto, vamos a eliminar esta columna.

Ahora vamos también a analizar los valores NA y duplicados, eliminándolos si es necesario.
```{r}
# Vemos el porcentaje de filas con valores NA
p.rows.na <- sum(rowSums(is.na(data)) > 0) / nrow(data) * 100
p.rows.na

# Vemos el porcentaje de filas duplicadas
p.rows.duplicated <- sum(duplicated(data)) / nrow(data) * 100
p.rows.duplicated

# Como apenas hay filas con valores NA vamos a eliminarlas directamente, ya que no deberían afectar a la distribución de los resultados
data <- na.omit(data)

# Eliminamos la columna "kreis_code"
data$kreis_code <- NULL

# Pasamos la variable "year" a factor
data$year <- as.factor(data$year)
```

Hemos eliminado las flas con valores `NA` ya que apenas representaban un 0.10% del dataset. Vamos ahora que no hay valores `NA` un resumen estadístico de los datos.
```{r}
# Resumen estadístico de las variables numéricas
print(dfSummary(data, varnumbers = FALSE, style = "grid"))
```

Vemos que hay algunas variables numéricas con valores negativos. Sin embargo, estas variables son porcentajes y concentraciones de partículas, por lo que los valores negativos no tienen sentido dada su definición matemática. Estas variables son: `NO_annualMean`, `PM10_daysOver50`, `PM2.5_annualMean`. Vamos a ver si podemos eliminar estos valores negativos.
```{r}
# Seleccioamos las variables que queremos inspeccionar
data.vars <- data %>% select(NO_annualMean, PM10_daysOver50, PM2.5_annualMean)

# Seleccionamos las filas con valores negativos
data.vars.neg <- data.vars %>% filter(NO_annualMean < 0 | PM10_daysOver50 < 0 | PM2.5_annualMean < 0)

# Vemos el porcentaje de filas con valores negativos
p.rows.neg <- nrow(data.vars.neg) / (nrow(data) - nrow(data.vars.neg)) * 100
p.rows.neg
```

Vemos como el 2.3% de las filas tienen valores negativos. Esto es bastante más que el caso anterior de los `NA`, que suponían solo un 0.19%. Por lo tanto, vamos a ver si podemos hacer algo con estos valores negativos. Los trateremos como valores `NA`, ya que no tienen sentido en el contexto de este dataset. 
```{r}
# Convertimos los valores negativos en NA
data$NO_annualMean[data$NO_annualMean < 0] <- NA
data$PM10_daysOver50[data$PM10_daysOver50 < 0] <- NA
data$PM2.5_annualMean[data$PM2.5_annualMean < 0] <- NA

# Usamos el paquete VIM para ver si se puede hacer algo con los valores NA
data.plot <- data %>% select(NO_annualMean, PM10_daysOver50, PM2.5_annualMean)
aggr(data.plot, numbers = TRUE, prop = FALSE)
```

Vemos que de estos valores negativos, casi el 100% de estos son de la variable a predecir en este práctica (`PM2.5_annualMean`). Por lo tanto, no vamos a imputar estos valores y los eliminaremos directamente, ya que pueden afectar a la predicción. 
```{r}
# Eliminamos las filas con N
data <- na.omit(data)

# Volvemos a ver el resumen estadístico
print(dfSummary(data, varnumbers = FALSE, style = "grid"))
```

Vemos que los valores de las variables categóricas están equilibrados, y que los valores de las variables numéricas en general son razonables. Sin embargo, hay algunos predictores numéricos que parece que tienen poca variabilidad. Vamos a analizar con `nearZeroVar` de caret si hace falta eliminar alguna de estas variables.
```{r}
# Seleccionamos las variables numéricas
data.num <- data %>% select_if(is.numeric)

# Analizamos la variabilidad de las variables numéricas
data.nzv <- nearZeroVar(data.num, saveMetrics = TRUE)
data.nzv
```

La métrica `freqRatio` nos indica la frecuencia de la variable más común sobre la segunda más común, es decir, si la variable más común tiene una frecuencia de 100 y la segunda más común de 25, este ratio será 4. Por otro lado, la métrica `percentUnique` nos indica el porcentaje de valores únicos sobre el total de valores, ayudándonos a ver una variable es continua (porcentaje alto) o es prácticamente constante (porcentaje bajo). 

Podemos ver en los resultados que hay dos variables con un `freqRatio` muy alto: `NO2_hrOver200` y `O3_daysOver120`, aunque la primera es mucho mayor a la segunda. De primeras esto es un mal indicativo, sin embargo, las dos variables tienen un `percentUnique` alto, por lo que el algoritmo no las clasifica con `TRUE` en `nzv`. La segunda variable tiene un `freqRatio` de 11, que junto a su alto `percentUnique` de 62 indica que es una variable válida. Por otro lado, la primera variable tiene un `freqRatio` enorme en comparación con el resto (de 250) y, aunque tenga un `percentUnique` del 48.8 (lo cual no es bajo), sí que es mucho menor que el siguinete valor más pequeño (57.7), de casi 10 puntos menos. Por lo tanto, vamos a eliminar del dataset la variable `NO2_hrOver200`.

```{r}
# Eliminamos la variable "NO2_hrOver200"
data$NO2_hrOver200 <- NULL
```

Vamos a responder a las preguntas: 

- "¿Cuáles son las características de las zonas con alta contaminación por PM2.5?"
- "¿Cuáles son las características de las zonas con baja contaminación por PM2.5?"
```{r}
# Creamos una nueva variable que indique el nivel de contaminación de forma categórica
data$contamination <- cut(
    data$PM2.5_annualMean,
    breaks = c(-Inf, 10, 25, 50, Inf),
    labels = c("Bajo", "Moderada", "Insalubre", "Muy Insalubre")
)

# Vemos la cantidad de filas por cada nivel de contaminación
table(data$contamination)

# Creamos una nueva variable que indique si la contaminacion es alta o baja
data$contamination_2levels <- ifelse(data$PM2.5_annualMean > 25, "Alta", "Baja")
data$contamination_2levels <- as.factor(data$contamination_2levels)

# Creamos una lista con los nombres de las variables numéricas
data.num.vars <- names(data)[sapply(data, is.numeric)]
data.num.vars <- setdiff(data.num.vars, "PM2.5_annualMean")

# Realizamos un histograma para cada variable numérica en el cual se vea su distribución según el nivel de contaminación.
densities <- lapply(data.num.vars, function(var) {
    ggplot(data, aes_string(x = var, fill = "contamination_2levels")) +
        geom_density(
            aes(y = after_stat(..scaled..)), # curva de densidad escalada [0,1]
            alpha = 0.5, position = "identity"
        ) +
        scale_y_continuous(limits = c(0, 1), expand = c(0, 0)) +
        scale_fill_manual(
            name   = "Contaminación",
            values = c("Baja" = "blue", "Alta" = "red")
        ) +
        labs(title = var, x = var, y = "Densidad normalizada") +
        theme_minimal(base_size = 10) +
        theme(
            plot.title   = element_text(size = 10),
            axis.title   = element_text(size = 9),
            axis.text    = element_text(size = 8),
            legend.title = element_text(size = 9),
            legend.text  = element_text(size = 8)
        )
})
do.call(grid.arrange, c(densities, ncol = 2))

# Calculamos las medias y las desviaciones típicas de cada variable según el nivel de contaminación y lo enseñamos con print()
means <- data %>%
    group_by(contamination_2levels) %>%
    summarise(across(all_of(data.num.vars), list(mean = mean, sd = sd), na.rm = TRUE))
sds <- data %>%
    group_by(contamination_2levels) %>%
    summarise(across(all_of(data.num.vars), list(sd = sd), na.rm = TRUE))
print(means, width = Inf)
print(sds, width = Inf)

# Creamos una lista con los nombres de las variables categóricas
data.cat.vars <- c("year", "scenario")

p_year <- ggplot(data, aes(x = factor(year), fill = contamination_2levels)) +
    geom_bar(position = "stack") +
    scale_y_continuous(trans = "log10") +
    scale_fill_manual(values = c("Alta" = "red", "Baja" = "blue")) +
    labs(
        title = "Contaminación PM2.5 por año",
        x = "Año",
        y = "Frecuencia (escala log10)",
        fill = "Contaminación"
    ) +
    theme_minimal() +
    theme(
        plot.title = element_text(hjust = 0.5),
        axis.text.x = element_text(angle = 45, hjust = 1, size = 8)
    )

p_scenario <- ggplot(data, aes(x = scenario, fill = contamination_2levels)) +
    geom_bar(position = "stack") +
    scale_y_continuous(trans = "log10") +
    scale_fill_manual(values = c("Alta" = "red", "Baja" = "blue")) +
    labs(
        title = "Contaminación PM2.5 por escenario",
        x = "Escenario",
        y = "Frecuencia (escala log10)",
        fill = "Contaminación"
    ) +
    theme_minimal() +
    theme(
        plot.title = element_text(hjust = 0.5),
        axis.text.x = element_text(angle = 45, hjust = 1, size = 8)
    )

# Unimos los tres gráficos en un grid de 3 columnas
grid.arrange(p_year, p_scenario, nrow = 2)
```

**__Respecto a las variables numéricas:__**

Con el gráfico anterior hay algunas variables que tienen distribuciones relativamente similares, ya que los valores se agrupan sobre valores parecidos pero la anchura de la distribución es diferente. Luego, tenemos variables con distribuciones diferentes pero solapadas, donde tanto la concentración como la anchura de los valores es distinta. Finalmente, tenemos variables donde las distribuciones son completamente diferentes, solapándose muy poco. Vamos a intentar sacar conclusiones de estas gráficas para definir las características de las zonas con alta contaminación por PM2.5.

- **NO2_annualMean**: En este caso la media tiende a ser mayor y las mediciones se distribuyen en un rango más amplio en la parte superior del espectro de valores. Esto indica que las zonas con alta contaminación por PM2.5 tienden a tener una mayor concentración de NO2.
- **NO_annualMean**: En este caso la media es ligeramente superior en las zonas co alta contaminación, además de que los valores se distribuyen en un rango más amplio en la parte superior del espectro de mediciones.
- **O3_annualMean, O3_daysOver120, O3_dailyMaxAnnualMean, O3_dailyHourlyMax, O3_daily8HrMax**: En ninguno de estos casos se puede decir que la distribución sea tan diferente como para destacar diferencias. Las medias son muy parecidas y en algunos casos la distribución es algo más amplia.
- **PM10_annualMean, PM10_daysOver50**: En los dos casos la media de la alta contaminacion es mayor a la de la baja contaminacion, teniendo distribuciones muy diferentes entre sí.

**__Respecto a las variables categóricas:__**
Podemos ver que la contaminación por PM2.5 es alta casi exclusivamente en el año 2003, con algunos casos en 2007 y 2011. Además, da igual el escenario ("medio", "remoto", "urbano") ya que la cantidad de contaminación es la misma en todos.

Podemos afirmar, por tanto, que las zonas con alta contaminación por PM2.5 suelten tener también una alta contaminación por PM10. Debido a esto, si la zona tiene registros de muchos días con la contaminacion por PM10 mayor a 50, es probable de que la contaminación por PM2.5 sea también alta. El tener valores muy elevados de NO2 y NO también puede indicar que la contaminación por PM2.5 es alta, aunque es menos evidente que en el caso de PM10, ya que las distribuciones se solapan más. El caso de la baja contaminación por PM2.5 cumple las condiciones contrarias. Además, las zonas contaminadas se localizan casi todas en 2003, con algunas excepciones en 2007 y 2011, independientemente del escenario.


# Apartado 2.2

Vamos a ver la distribución de los diferentes valores de la variable a predecir, `contamination`.
```{r}
# Vemos la distribución de los valores de la variable a predecir
table(data$contamination)
```

Vemos que apenas hay datos `Insalubres` y `Muy Insalubres`, por lo que vamos a tener que balancear el dataset. Antes de balancear el dataset dividiremos en train y test, de forma que el conjunto de test no se vea afectado por el balanceo y tenga datos sintéticos. Con esto se quiere mantener la integridad del conjunto de test la máximo.
```{r}
# Definimos conjuntos de train y test
set.seed(123)
trainIndex <- createDataPartition(
    data$contamination,
    p = .8,
    list = FALSE,
    times = 1
)
data.train <- data[trainIndex, ]
data.test <- data[-trainIndex, ]

# Vemos la distribución de los valores en el conjunto de test
table(data.test$contamination)
```

Vemos que apenas tenemos datos de las clases `Insalubre` y `Muy Insalubre` para predecir en el conjunto de test. Pero, como se ha dicho anteriormente, es preferible tener pocos datos a tener un conjunto de test con datos sintéticos, los cuales no representarían la realidad. Ahora vamos a hacer un upsampling del conjutno de train.
```{r}
# Hacemos un upsampling con SMOTE
set.seed(123)
data.train.smote <- SMOTE(
    data.train[, sapply(data.train, is.numeric)],
    data.train$contamination,
    dup_size = 15
)
data.train.balanced <- data.train.smote$data %>% rename(contamination = class)

set.seed(123)
data.train.smote <- SMOTE(
    data.train.balanced[, sapply(data.train.balanced, is.numeric)],
    data.train.balanced$contamination,
    dup_size = 10
)
data.train.balanced <- data.train.smote$data %>% rename(contamination = class)

# Hacemos un downsampling eligiendo una muestra aleatoria de la categoría "Moderada"
set.seed(123)
diff <- nrow(data.train.balanced[data.train.balanced$contamination == "Moderada", ]) - nrow(data.train.balanced[data.train.balanced$contamination == "Bajo", ])
idxs <- sample(
    which(data.train.balanced$contamination == "Moderada"),
    diff
)
data.train.balanced <- data.train.balanced[-idxs, ]

# Vemos la distribución de los valores en el conjunto de train
table(data.train.balanced$contamination)
```

```{r}
# Declaramos el cluster con doParallel dependiendo del número de núcleos de la CPU
cluster <- makePSOCKcluster(2)
registerDoParallel(cluster)

# Seleccionamos las variables que queremos usar para la predicción
df <- data %>% select(
    -c(
        PM2.5_annualMean,
        contamination_2levels,
        year,
        scenario,
        kreis
    )
)

# Definimos el tamaño de los subconjuntos
subsets <- 5:length(df)

# Definimos el control de entrenamiento
set.seed(123)
rfe.ctrl <- rfeControl(
    functions = rfFuncs,
    method = "cv",
    number = 5,
    verbose = TRUE,
    returnResamp = "final",
    allowParallel = TRUE,
)

# Definimos los

# Ejecutamos el algoritmo RFE
set.seed(321)
df.rfe <- rfe(
    contamination ~ .,
    data = df,
    sizes = c(3:ncol(df)),
    rfeControl = rfe.ctrl,
)

# Vemos los resultados
df.rfe

# Seleccionamos las variables óptimas
df <- df %>% select(c(df.rfe$optVariables, "contamination"))
```

Vamos a usar modelos para realizar predicciones.
```{r}
##########
# RPart #
#########

# Vemos la infomración del modelo con modelLookup
modelLookup("rpart")

# Definimos el control de la validación cruzada
set.seed(123)
fit.control.cv.10 <- trainControl(
    method = "cv",
    number = 10,
    returnResamp = "all",
    verboseIter = TRUE,
    allowParallel = TRUE
)

# Realizamos la validación cruzada
set.seed(342)
rpart.cv <- train(contamination ~ .,
    data = data.train.balanced,
    method = "rpart",
    tuneLength = 10,
    trControl = fit.control.cv.10
)
rpart.cv
```