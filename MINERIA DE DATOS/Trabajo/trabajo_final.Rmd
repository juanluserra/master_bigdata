# Apartado 2.1

```{r}
# Seleccionamos el "path" de este archivo como el directorio de trabajo
if (interactive()) {
    setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
}

# Cargamos el workspace.RData
# load("workspace.RData")

# Cargamos las librerías necesarias
library(summarytools)
library(ggplot2)
library(scales)
library(dplyr)
library(tidyr)
library(caret)
library(gridExtra)
library(VIM)
library(doParallel)
library(smotefamily)
library(vip)
library(reticulate)
library(keras3)
library(MLmetrics)
```


```{r}
# Leemos el archivo CSV
data <- read.csv("data.csv", sep = ";", stringsAsFactors = TRUE)

# Inspeccionamos las características del dataframe
dim(data)
str(data)
head(data)
```

Podemos ver que el dataframe tiene 24120 valores y 15 predictores. De todos estos predictores, hay 1 que no nos interesa: `kreis_code`. Este es simplemente un código que identifica la región, la cuAl ya se especifica en `kreis`. Por lo tanto, vamos a eliminar esta columna.

Ahora vamos también a analizar los valores NA y duplicados, eliminándolos si es necesario.
```{r}
# Vemos el porcentaje de filas con valores NA
p.rows.na <- sum(rowSums(is.na(data)) > 0) / nrow(data) * 100
p.rows.na

# Vemos el porcentaje de filas duplicadas
p.rows.duplicated <- sum(duplicated(data)) / nrow(data) * 100
p.rows.duplicated

# Como apenas hay filas con valores NA vamos a eliminarlas directamente, ya que no deberían afectar a la distribución de los resultados
data <- na.omit(data)

# Eliminamos la columna "kreis_code"
data$kreis_code <- NULL

# Pasamos la variable "year" a factor
data$year <- as.factor(data$year)
```

Hemos eliminado las flas con valores `NA` ya que apenas representaban un 0.10% del dataset. Vamos ahora que no hay valores `NA` un resumen estadístico de los datos.
```{r}
# Resumen estadístico de las variables numéricas
print(dfSummary(data, varnumbers = FALSE, style = "grid"))
```

Vemos que hay algunas variables numéricas con valores negativos. Sin embargo, estas variables son porcentajes y concentraciones de partículas, por lo que los valores negativos no tienen sentido dada su definición matemática. Estas variables son: `NO_annualMean`, `PM10_daysOver50`, `PM2.5_annualMean`. Vamos a ver si podemos eliminar estos valores negativos.
```{r}
# Seleccioamos las variables que queremos inspeccionar
data.vars <- data %>% select(NO_annualMean, PM10_daysOver50, PM2.5_annualMean)

# Seleccionamos las filas con valores negativos
data.vars.neg <- data.vars %>% filter(NO_annualMean < 0 | PM10_daysOver50 < 0 | PM2.5_annualMean < 0)

# Vemos el porcentaje de filas con valores negativos
p.rows.neg <- nrow(data.vars.neg) / (nrow(data) - nrow(data.vars.neg)) * 100
p.rows.neg
```

Vemos como el 2.3% de las filas tienen valores negativos. Esto es bastante más que el caso anterior de los `NA`, que suponían solo un 0.19%. Por lo tanto, vamos a ver si podemos hacer algo con estos valores negativos. Los trateremos como valores `NA`, ya que no tienen sentido en el contexto de este dataset. 
```{r}
# Convertimos los valores negativos en NA
data$NO_annualMean[data$NO_annualMean < 0] <- NA
data$PM10_daysOver50[data$PM10_daysOver50 < 0] <- NA
data$PM2.5_annualMean[data$PM2.5_annualMean < 0] <- NA

# Usamos el paquete VIM para ver si se puede hacer algo con los valores NA
data.plot <- data %>% select(NO_annualMean, PM10_daysOver50, PM2.5_annualMean)
aggr(data.plot, numbers = TRUE, prop = FALSE)
```

Vemos que de estos valores negativos, casi el 100% de estos son de la variable a predecir en este práctica (`PM2.5_annualMean`). Por lo tanto, no vamos a imputar estos valores y los eliminaremos directamente, ya que pueden afectar a la predicción. 
```{r}
# Eliminamos las filas con N
data <- na.omit(data)

# Volvemos a ver el resumen estadístico
print(dfSummary(data, varnumbers = FALSE, style = "grid"))
```

Vemos que los valores de las variables categóricas están equilibrados, y que los valores de las variables numéricas en general son razonables. Sin embargo, hay algunos predictores numéricos que parece que tienen poca variabilidad. Vamos a analizar con `nearZeroVar` de caret si hace falta eliminar alguna de estas variables.
```{r}
# Seleccionamos las variables numéricas
data.num <- data %>% select_if(is.numeric)

# Analizamos la variabilidad de las variables numéricas
data.nzv <- nearZeroVar(data.num, saveMetrics = TRUE)
data.nzv
```

La métrica `freqRatio` nos indica la frecuencia de la variable más común sobre la segunda más común, es decir, si la variable más común tiene una frecuencia de 100 y la segunda más común de 25, este ratio será 4. Por otro lado, la métrica `percentUnique` nos indica el porcentaje de valores únicos sobre el total de valores, ayudándonos a ver una variable es continua (porcentaje alto) o es prácticamente constante (porcentaje bajo). 

Podemos ver en los resultados que hay dos variables con un `freqRatio` muy alto: `NO2_hrOver200` y `O3_daysOver120`, aunque la primera es mucho mayor a la segunda. De primeras esto es un mal indicativo, sin embargo, las dos variables tienen un `percentUnique` alto, por lo que el algoritmo no las clasifica con `TRUE` en `nzv`. La segunda variable tiene un `freqRatio` de 11, que junto a su alto `percentUnique` de 62 indica que es una variable válida. Por otro lado, la primera variable tiene un `freqRatio` enorme en comparación con el resto (de 250) y, aunque tenga un `percentUnique` del 48.8 (lo cual no es bajo), sí que es mucho menor que el siguinete valor más pequeño (57.7), de casi 10 puntos menos. Por lo tanto, vamos a eliminar del dataset la variable `NO2_hrOver200`.

```{r}
# Eliminamos la variable "NO2_hrOver200"
data$NO2_hrOver200 <- NULL
```

Vamos a responder a las preguntas: 

- "¿Cuáles son las características de las zonas con alta contaminación por PM2.5?"
- "¿Cuáles son las características de las zonas con baja contaminación por PM2.5?"
```{r}
# Creamos una nueva variable que indique el nivel de contaminación de forma categórica
data$contamination <- cut(
    data$PM2.5_annualMean,
    breaks = c(-Inf, 10, 25, 50, Inf),
    labels = c("Bajo", "Moderada", "Insalubre", "Muy Insalubre")
)

# Vemos la cantidad de filas por cada nivel de contaminación
table(data$contamination)

# Creamos una nueva variable que indique si la contaminacion es alta o baja
data$contamination_2levels <- ifelse(data$PM2.5_annualMean > 25, "Alta", "Baja")
data$contamination_2levels <- as.factor(data$contamination_2levels)

# Creamos una lista con los nombres de las variables numéricas
data.num.vars <- names(data)[sapply(data, is.numeric)]
data.num.vars <- setdiff(data.num.vars, "PM2.5_annualMean")

# Realizamos un histograma para cada variable numérica en el cual se vea su distribución según el nivel de contaminación.
densities <- lapply(data.num.vars, function(var) {
    ggplot(data, aes(
        x     = .data[[var]],
        fill  = contamination_2levels,
        group = contamination_2levels
    )) +
        geom_density(
            aes(y = after_stat(..scaled..)),
            alpha = 0.5,
            position = "identity"
        ) +
        scale_fill_manual(
            name   = "Contaminación",
            values = c("Baja" = "blue", "Alta" = "red")
        ) +
        labs(
            title = var,
            x     = var,
            y     = "Densidad normalizada"
        ) +
        theme_minimal(base_size = 10) +
        theme(
            plot.title   = element_text(size = 10),
            axis.title   = element_text(size = 9),
            axis.text    = element_text(size = 8),
            legend.title = element_text(size = 9),
            legend.text  = element_text(size = 8)
        )
})
do.call(grid.arrange, c(densities, ncol = 2))


# Calculamos las medias y las desviaciones típicas de cada variable según el nivel de contaminación y lo enseñamos con print()
means <- data %>%
    group_by(contamination_2levels) %>%
    summarise(across(all_of(data.num.vars), list(mean = mean, sd = sd), na.rm = TRUE))
sds <- data %>%
    group_by(contamination_2levels) %>%
    summarise(across(all_of(data.num.vars), list(sd = sd), na.rm = TRUE))
print(means, width = Inf)
print(sds, width = Inf)

# Creamos una lista con los nombres de las variables categóricas
data.cat.vars <- c("year", "scenario")

p_year <- ggplot(data, aes(x = factor(year), fill = contamination_2levels)) +
    geom_bar(position = "stack") +
    scale_y_continuous(trans = "log10") +
    scale_fill_manual(values = c("Alta" = "red", "Baja" = "blue")) +
    labs(
        title = "Contaminación PM2.5 por año",
        x = "Año",
        y = "Frecuencia (escala log10)",
        fill = "Contaminación"
    ) +
    theme_minimal() +
    theme(
        plot.title = element_text(hjust = 0.5),
        axis.text.x = element_text(angle = 45, hjust = 1, size = 8)
    )

p_scenario <- ggplot(data, aes(x = scenario, fill = contamination_2levels)) +
    geom_bar(position = "stack") +
    scale_y_continuous(trans = "log10") +
    scale_fill_manual(values = c("Alta" = "red", "Baja" = "blue")) +
    labs(
        title = "Contaminación PM2.5 por escenario",
        x = "Escenario",
        y = "Frecuencia (escala log10)",
        fill = "Contaminación"
    ) +
    theme_minimal() +
    theme(
        plot.title = element_text(hjust = 0.5),
        axis.text.x = element_text(angle = 45, hjust = 1, size = 8)
    )

# Unimos los tres gráficos en un grid de 3 columnas
grid.arrange(p_year, p_scenario, nrow = 2)
```

**__Respecto a las variables numéricas:__**

Con el gráfico anterior hay algunas variables que tienen distribuciones relativamente similares, ya que los valores se agrupan sobre valores parecidos pero la anchura de la distribución es diferente. Luego, tenemos variables con distribuciones diferentes pero solapadas, donde tanto la concentración como la anchura de los valores es distinta. Finalmente, tenemos variables donde las distribuciones son completamente diferentes, solapándose muy poco. Vamos a intentar sacar conclusiones de estas gráficas para definir las características de las zonas con alta contaminación por PM2.5.

- **NO2_annualMean**: En este caso la media tiende a ser mayor y las mediciones se distribuyen en un rango más amplio en la parte superior del espectro de valores. Esto indica que las zonas con alta contaminación por PM2.5 tienden a tener una mayor concentración de NO2.
- **NO_annualMean**: En este caso la media es ligeramente superior en las zonas co alta contaminación, además de que los valores se distribuyen en un rango más amplio en la parte superior del espectro de mediciones.
- **O3_annualMean, O3_daysOver120, O3_dailyMaxAnnualMean, O3_dailyHourlyMax, O3_daily8HrMax**: En ninguno de estos casos se puede decir que la distribución sea tan diferente como para destacar diferencias. Las medias son muy parecidas y en algunos casos la distribución es algo más amplia.
- **PM10_annualMean, PM10_daysOver50**: En los dos casos la media de la alta contaminacion es mayor a la de la baja contaminacion, teniendo distribuciones muy diferentes entre sí.

**__Respecto a las variables categóricas:__**
Podemos ver que la contaminación por PM2.5 es alta casi exclusivamente en el año 2003, con algunos casos en 2007 y 2011. Además, da igual el escenario ("medio", "remoto", "urbano") ya que la cantidad de contaminación es la misma en todos.

Podemos afirmar, por tanto, que las zonas con alta contaminación por PM2.5 suelten tener también una alta contaminación por PM10. Debido a esto, si la zona tiene registros de muchos días con la contaminacion por PM10 mayor a 50, es probable de que la contaminación por PM2.5 sea también alta. El tener valores muy elevados de NO2 y NO también puede indicar que la contaminación por PM2.5 es alta, aunque es menos evidente que en el caso de PM10, ya que las distribuciones se solapan más. El caso de la baja contaminación por PM2.5 cumple las condiciones contrarias. Además, las zonas contaminadas se localizan casi todas en 2003, con algunas excepciones en 2007 y 2011, independientemente del escenario.


# Apartado 2.2

Vamos a ver la distribución de los diferentes valores de la variable a predecir, `contamination`.
```{r}
# Vemos la distribución de los valores de la variable a predecir
table(data$contamination)
```

Vemos que apenas hay datos `Insalubres` y `Muy Insalubres`, por lo que vamos a tener que balancear el dataset. Antes de balancear el dataset dividiremos en train y test, de forma que el conjunto de test no se vea afectado por el balanceo y tenga datos sintéticos. Con esto se quiere mantener la integridad del conjunto de test la máximo.
```{r}
# Definimos conjuntos de train y test
set.seed(123)
trainIndex <- createDataPartition(
    data$contamination,
    p = .8,
    list = FALSE,
    times = 1
)
data.train <- data[trainIndex, ]
data.test <- data[-trainIndex, ]

# Vemos la distribución de los valores en el conjunto de test
table(data.test$contamination)
```

Vemos que apenas tenemos datos de las clases `Insalubre` y `Muy Insalubre` para predecir en el conjunto de test. Pero, como se ha dicho anteriormente, es preferible tener pocos datos a tener un conjunto de test con datos sintéticos, los cuales no representarían la realidad. Ahora vamos a hacer un upsampling y downsampling del conjunto de train.
```{r}
# Hacemos un upsampling con SMOTE
set.seed(123)
data.train.smote <- SMOTE(
    data.train[, sapply(data.train, is.numeric)],
    data.train$contamination,
    dup_size = 2
)
data.train.balanced <- data.train.smote$data %>% rename(contamination = class)

set.seed(123)
data.train.smote <- SMOTE(
    data.train.balanced[, sapply(data.train.balanced, is.numeric)],
    data.train.balanced$contamination,
    dup_size = 2
)
data.train.balanced <- data.train.smote$data %>% rename(contamination = class)

# Hacemos un downsampling eligiendo una muestra aleatoria de la categoría "Moderada"
set.seed(123)
diff <- nrow(data.train.balanced[data.train.balanced$contamination == "Moderada", ]) - nrow(data.train.balanced[data.train.balanced$contamination == "Bajo", ])
idxs <- sample(
    which(data.train.balanced$contamination == "Moderada"),
    diff
)
data.train.balanced <- data.train.balanced[-idxs, ]

# Reordenamos las categorías de la variable a predecir
data.train.balanced$contamination <- factor(data.train.balanced$contamination, levels = c("Bajo", "Moderada", "Insalubre", "Muy Insalubre"))

# Vemos la distribución de los valores en el conjunto de train
table(data.train.balanced$contamination)
```

Hemos aumentado un 200% la cantidad de datos de las clases `Insalubre` y `Muy Insalubre`, y hemos reducido la cantidad de datos de la clase `Moderada` a la misma que `Bajo`. De esta forma, aunque no tenemos un dataset balanceado, sí que tenemos un dataset donde las clases `Insalubre` y `Muy Insalubre` tienen una cantidad de datos razonable para poder predecir. No se ha aumentado más el upsampling ya que si no estaríamos generando una cantidad demasiado grande de datos sintéticos, que pueden generar overfiting y otros problemas. Tampoco hemos reducido más la clase `Moderada` ni hemos reducido la clase `Bajo` ya que perderíamos casi toda nuestra información del dataset.

Vamos a realizar ahora una eliminación de variables. Para ello usaremos el
```{r}
# Modificamos las variables del dataset
data.train.balanced$contamination <- as.factor(data.train.balanced$contamination)
data.train.balanced$PM2.5_annualMean <- NULL

# Declaramos el cluster con doParallel dependiendo del número de núcleos de la CPU
cluster <- makePSOCKcluster(detectCores() - 1)
registerDoParallel(cluster)

# Definimos el tamaño de los subconjuntos
subsets <- 3:ncol(data.train.balanced)

# Definimos el control de entrenamiento
set.seed(123)
rfe.ctrl <- rfeControl(
    functions = rfFuncs,
    method = "cv",
    number = 5,
    verbose = TRUE,
    returnResamp = "final",
    allowParallel = TRUE,
)

# Ejecutamos el algoritmo RFE
set.seed(321)
data.rfe <- rfe(
    contamination ~ .,
    data = data.train.balanced,
    sizes = subsets,
    rfeControl = rfe.ctrl,
)

# Vemos los resultados
data.rfe

# Seleccionamos las variables óptimas
data.train.balanced <- data.train.balanced %>% select(c(data.rfe$optVariables, "contamination"))
```

Vamos a usar modelos para realizar predicciones.
```{r}
#########
# RPart #
#########

# Vemos la infomración del modelo con modelLookup
modelLookup("rpart")

# Definimos el control de la validación cruzada
set.seed(123)
rpart.ctrl <- trainControl(
    method = "cv",
    number = 10,
    returnResamp = "all",
    verboseIter = TRUE,
    allowParallel = TRUE
)

# Realizamos el entrenamiento del modelo
set.seed(342)
rpart.cv <- train(contamination ~ .,
    data = data.train.balanced,
    method = "rpart",
    tuneLength = 20,
    trControl = rpart.ctrl
)
rpart.cv
```

```{r}
################
# RandomForest #
################

# Vemos la infomración del modelo con modelLookup
modelLookup("rf")

# Definimos el control de la validación cruzada
set.seed(123)
rf.ctrl <- trainControl(
    method = "cv",
    number = 10,
    returnResamp = "all",
    verboseIter = TRUE,
    allowParallel = TRUE
)

# Realizamos el entrenamiento del modelo
set.seed(342)
rf.cv <- train(contamination ~ .,
    data = data.train.balanced,
    method = "rf",
    tuneLength = 20,
    trControl = rf.ctrl
)
rf.cv
```

```{r}
#######
# GBM #
#######

# Vemos la infomración del modelo con modelLookup
modelLookup("gbm")

# Definimos el control de la validación cruzada
set.seed(123)
gbm.ctrl <- trainControl(
    method = "cv",
    number = 10,
    returnResamp = "all",
    verboseIter = TRUE,
    allowParallel = TRUE
)

# Realizamos el entrenamiento del modelo
set.seed(342)
gbm.cv <- train(contamination ~ .,
    data = data.train.balanced,
    method = "gbm",
    tuneLength = 20,
    trControl = gbm.ctrl
)
gbm.cv
```

```{r}
#######
# SVM #
#######

# Vemos la infomración del modelo con modelLookup
modelLookup("svmRadial")

model_info <- getModelInfo("svmRadial", regex = FALSE)[[1]]
auto_grid <- model_info$grid(
    x = model.matrix(contamination ~ . - 1, data.train.balanced),
    y = data.train.balanced$contamination,
    len = 20
)

# Definimos el control de la validación cruzada
set.seed(123)
svm.ctrl <- trainControl(
    method = "cv",
    number = 10,
    search = "random",
    returnResamp = "all",
    verboseIter = TRUE,
    allowParallel = TRUE,
)

# Realizamos el entrenamiento del modelo
set.seed(342)
svm.cv <- train(contamination ~ .,
    data = data.train.balanced,
    method = "svmRadial",
    preProcess = c("center", "scale"),
    tuneLength = 20,
    trControl = svm.ctrl
)
svm.cv
```

Ahora vamos a comparar los modelos viendo sus resultados en el conjunto de test.
```{r}
# Convertimos la variable de predicción a factor
data.test$contamination <- as.factor(data.test$contamination)

# Predecimos el conjunto de test
rpart.pred <- predict(rpart.cv, newdata = data.test)
rf.pred <- predict(rf.cv, newdata = data.test)
gbm.pred <- predict(gbm.cv, newdata = data.test)
svm.pred <- predict(svm.cv, newdata = data.test)

# Creamos matrices de confusión para cada modelo
rpart.conf <- confusionMatrix(rpart.pred, data.test$contamination)
rpart.conf
rf.conf <- confusionMatrix(rf.pred, data.test$contamination)
rf.conf
gbm.conf <- confusionMatrix(gbm.pred, data.test$contamination)
gbm.conf
svm.conf <- confusionMatrix(svm.pred, data.test$contamination)
svm.conf
```

De primeras vemos que el modelo `rpart` es el menos preciso, lo cual tiene sentdio ya que es un modelo muy simple. Los otros son más parecidos entre sí. Además, un detalle muy importante es que todos tienen un buen desempeño en predecir las clases más desbalanceadas (`Insalubre` y `Muy Insalubre`), lo cual es un buen indicativo de que el balanceo ha funcionado. 

Con cualquiera de los modelos de `rf`, `gbm` o `svm`, podemos afirmar que es posible predecir correctamente si jabrá contaminación por PM2.5 en una zona.

Vamos a comparar estos tres modelos más en detalle.
```{r}
# Hacemos un resamp de los modelos
resamps <- resamples(list(
    GBM = gbm.cv,
    SVM = svm.cv,
    RF = rf.cv
))
summary(resamps)

# Vemos la diferencia entre las métricas de evaluación de modelos
diffs <- diff(resamps)
summary(diffs)

# Representamos el accuracy y el Kappa de los modelos dentro de un mismo grid
grid.arrange(
    bwplot(resamps, metric = "Kappa"),
    bwplot(resamps, metric = "Accuracy"),
    nrow = 2
)
```

Las tablas que genera `summary(diffs)` nos ayudan a ver sin los modelos tienen desempeños similares o no. En la diagonal superior tenemos lo mejor que es una métrica de un modelo respecto al otro, mientrsa que en la diagonal inferior tenemos la significancia estadística de esa diferencia (el p-value). Se puede observar que las diferencias son muy pequeñas y los p-values muy altos, por lo que no hay una diferencia estadísticamente significativa entre los modelos. 

Si ahora vemos las gráficas de las métricas de Kappa y Accuracy, vemos que efectivamente el desempeño es muy similar. Eso sí, el RandomForest está un poco por encima en sus valores mínimos y máximos. Por lo tanto, aunque no haya mucha diferencia entre los modelos, el RandomForest el el que mejor desempeño tiene. Además, este es un modelo muy simple y fácil de interpretar, que nos permite ver la importancia de las variables, lo que lo hace muy útil.

Vamos a ver la importancia de las variables en el modelo RandomForest.
```{r}
# Vemos la importancia de las variables
rf.imp <- varImp(rf.cv, scale = FALSE)
rf.imp
plot(rf.imp, top = 20)
```

Con este gráfico de imortancia vemos que las variables más importantes son las que tienen que ver con las partículas PM10, luego con el NO2, luego con el O3 y finalmente con el NO. Esto nos da una idea de la importancia de cada partícula a la hora de predecir la contaminación por PM2.5.

# Apartado 2.3

```{r}
set.seed(123)

# Cremaos los datos de entrada para la red neuronal
x <- scale(as.matrix(data.train.balanced %>% select(-contamination)))
y <- as.integer(as.factor(data.train.balanced$contamination)) - 1L
n <- ncol(x)
K <- 4

# Creamos los folds para la validación cruzada
folds <- createFolds(y, k = 5, returnTrain = FALSE)

# Creamos el grid de hiperparámetros
grid <- expand.grid(
    n_layers = c(1, 2, 3),
    units = c(32, 64, 128),
    lr = c(1e-2, 1e-3, 1e-4),
    epochs = c(30, 50, 100)
)

# Realizamos la validación cruzada con el grid de hiperparámetros usando el paquete doParallel
cl <- makeCluster(detectCores() - 2)
registerDoParallel(cl)
results <- foreach(i = 1:nrow(grid), .combine = rbind, .packages = c("keras3", "MLmetrics")) %dopar% {
    # Cogemos los hiperparámetros
    p <- grid[i, ]
    # Creamos listas para guardar las métricas de evaluación
    accs <- numeric(length(folds))
    f1s <- numeric(length(folds))

    # Realizamos un bucle para los folds
    for (j in seq_along(folds)) {
        idx_val <- folds[[j]]
        idx_train <- setdiff(1:length(y), idx_val)

        # Calculamos el class_weight
        class_weights <- table(y[idx_train])
        class_weights <- class_weights / sum(class_weights)
        class_weights <- 1 / class_weights
        class_weights <- class_weights / sum(class_weights)
        class_weights <- as.list(class_weights)
        class_weights <- setNames(class_weights, 0:(K - 1))

        # Creamos el modelo
        model <- keras_model_sequential()
        model %>% layer_dense(units = p$units, activation = "relu", input_shape = n)
        if (p$n_layers > 1) {
            for (k in seq_len(p$n_layers - 1)) {
                model %>% layer_dense(units = p$units, activation = "relu")
            }
        }
        model %>% layer_dense(units = K, activation = "softmax")
        model %>% compile(
            optimizer = optimizer_adam(learning_rate = p$lr),
            loss = "sparse_categorical_crossentropy",
            metrics = "accuracy"
        )

        # Ajustamos el modelo
        history <- model %>% fit(
            x[idx_train, ], y[idx_train],
            epochs = p$epochs, batch_size = 64, verbose = 0
        )

        # Predicciones
        probs <- model %>% predict(x[idx_val, ], verbose = 0)
        pred_cls <- apply(probs, 1, which.max) - 1L
        true_cls <- y[idx_val]

        # Calculamos el accuracy
        ev <- model %>% evaluate(x[idx_val, ], y[idx_val], verbose = 0)
        accs[j] <- ev["accuracy"]

        # Calculamos el F1-score
        f1.perr_class <- sapply(0:(K - 1), function(cls) {
            y_pred_bin <- ifelse(pred_cls == cls, 1, 0)
            y_true_bin <- ifelse(true_cls == cls, 1, 0)
            # evita NA si no hay casos de la clase en el fold
            if (all(y_true_bin == 0)) {
                return(NA_real_)
            }
            F1_Score(y_pred = y_pred_bin, y_true = y_true_bin, positive = "1")
        })
        f1s[j] <- mean(f1.perr_class, na.rm = TRUE)
    }

    # Devolvemos los resultados
    c(grid[i, ], mean_acc = mean(unlist(accs)), mean_f1 = mean(unlist(f1s)))
}
stopCluster(cl)

# Convertimos los resultados a un dataframe
results <- as.data.frame(results)
# Mostrmoas los resultados
print(results)
# Escogemos el mejor modelo y lo mostramos
best <- results[which.max(results$mean_f1), ]
best
```

```{r}
# Guardamos los hiperparámetros en variables de los tipos correspondientes
best$n_layers <- as.integer(best$n_layers)
best$units <- as.integer(best$units)
best$lr <- as.numeric(best$lr)
best$epochs <- as.integer(best$epochs)

# Calculamos los pesos de las clases
class_weights <- table(data.train.balanced$contamination)
class_weights <- class_weights / sum(class_weights)
class_weights <- 1 / class_weights
class_weights <- class_weights / sum(class_weights)
class_weights <- as.list(class_weights)
class_weights <- setNames(class_weights, 0:(K - 1))

# Cremos el modelo con los mejores hiperparámetros
model <- keras_model_sequential()
model %>% layer_dense(units = best$units, activation = "relu", input_shape = n)
if (best$n_layers > 1) {
    for (k in seq_len(best$n_layers - 1)) {
        model %>% layer_dense(units = best$units, activation = "relu")
    }
}
model %>% layer_dense(units = K, activation = "softmax")
model %>% compile(
    optimizer = optimizer_adam(learning_rate = best$lr),
    loss = "sparse_categorical_crossentropy",
    metrics = "accuracy"
)

# Ajustamos el modelo
history <- model %>% fit(
    x, y,
    epochs = best$epochs, batch_size = 64, verbose = 1,
    class_weight = class_weights
)

# Conjunto de test
x.test <- data.test %>% select(-contamination)
x.test <- x.test[, colnames(x.test) %in% colnames(data.train.balanced)]
x.test <- scale(x.test)
x.test <- as.matrix(x.test)
y.test <- as.integer(as.factor(data.test$contamination)) - 1L

# Predecimos el conjunto de test
probs <- model %>% predict(x.test, verbose = 0)
pred_cls <- apply(probs, 1, which.max) - 1L
true_cls <- y.test

# Calculamos el accuracy
ev <- model %>% evaluate(x.test, y.test, verbose = 0)
acc <- ev["accuracy"]
f1.perr_class <- sapply(0:(K - 1), function(cls) {
    y_pred_bin <- ifelse(pred_cls == cls, 1, 0)
    y_true_bin <- ifelse(true_cls == cls, 1, 0)
    # evita NA si no hay casos de la clase en el fold
    if (all(y_true_bin == 0)) {
        return(NA_real_)
    }
    F1_Score(y_pred = y_pred_bin, y_true = y_true_bin, positive = "1")
})
f1 <- mean(f1.perr_class, na.rm = TRUE)

# Mostramos los resultados
acc
f1

# Pasamos las predicciones y los test a factor
pred_cls <- factor(pred_cls, levels = 0:(K - 1), labels = levels(data.test$contamination))
true_cls <- factor(true_cls, levels = 0:(K - 1), labels = levels(data.test$contamination))
cm <- confusionMatrix(pred_cls, true_cls)
cm
```
