% Clase de documento
\documentclass[12pt, letterpaper]{article}

% Paquetes
\usepackage[utf8]{inputenc}
\usepackage[spanish]{babel}
\usepackage{biblatex}
\usepackage{csquotes}
\usepackage{datetime}
\usepackage{amsmath}
\usepackage{lipsum}
\usepackage{hyperref}
\usepackage{fancyhdr}
\usepackage{parskip}

%------------ 
% Decoración
%------------
\fancyhf{}
\setlength{\headheight}{15.71667pt}
\addtolength{\topmargin}{-3.71667pt}
\fancyhf{}

% Header
\fancyhead[L]{\textsc{\doctitle}}
\renewcommand{\sectionmark}[1]{\markright{#1}}
\fancyhead[R]{\textit{\nouppercase{\rightmark}}}

% Footer
\renewcommand{\footrulewidth}{0.4pt}
\fancyfoot[C]{Página \thepage}

% Título
\newcommand{\doctitle}{Apuntes de Minería de Datos}
\title{\doctitle}
\author{Juan Luis Serradilla Tormos}
\date{\monthname[\month] de \the\year}

% Bibliografía
\addbibresource{test.bib}

% Eliminar sangría
\setlength{\parindent}{0pt}

% Aumentar la separación entre párrafos
\setlength{\parskip}{1em plus 0.5em minus 0.2em}

%-----------
% Documento
%-----------
\begin{document}

% Mostrar header y footer
\pagestyle{fancy}

% Mostrar el título
\maketitle

% Índice
\newpage
\tableofcontents

% Contenido
\newpage
\section{Preprocesamiento de datos}

\subsection{Introducción}
\begin{itemize}
    \item El resultado de la minería de datos depende en gran medida de la calidad de los datos.
    \item El conjunto de datos estará formado por objetos
    \item Los objetos se describen por medio de atributos
    \item Un atributo tiene asociado un tipo que define de los valores que puede tomar
\end{itemize}

\subsection{Limpieza de datos}
Los errores en los datos pueden deberse a diferentes causas:
\begin{itemize}
    \item \textbf{Datos incompletos}: Pueden faltar atributos de interés, valores de los propios\ldots
    \item \textbf{Datos ruidosos}: Datos con ruido o errores, valores duplicados\ldots
    \item \textbf{Datos inconsistentes}: Datos que discrepan en códigos y nombres, en valores duplicados, etc. Por ejemplo.
    \begin{itemize}
        \item Edad = ''42'', Fecha de nacimiento = ''12/07/2015''
        \item Objetos con escala ``1,2,3'' y otros con escala ``A,B,C''
    \end{itemize}
    \item \textbf{Errores intencionados:} Datos que se introducen para encubrir falta de datos. Por ejemplo, encontrar la misma fecha de nacimiento para un gran grupo de personas ya que no tenían fecha 
\end{itemize}

\subsubsection{Datos ausentes}
Los datos ausentes pueden producir varios errores:
\begin{itemize}
    \item \textbf{Pérdida de eficacia:} Se extraen menos patronas y las conclusiones son menos concluyentes.
    \item \textbf{Complicaciones al analizar:} Surgen complicaciones debido a que hay técnicas que no están preparadas para gestionarlos.
    \item \textbf{Sesgo:} Puede haber sesgo en los datos que se extraen.
\end{itemize}

Por todo esto, al limpiar un dataset es necesario tener en cuenta los datos ausentes. Hay varias formas de detectarlos:
\begin{itemize}
    \item Generalmente se representan como valores nulos.
    \item Puede haber nulos camuflados, es decir, valores que no son nulos pero los representan. Esto se debe a que la integridad del sistema no permite introducir nulos en campos con ciertos formatos (direcciones, teléfonos, códigos postales\ldots)
\end{itemize}

\vspace{1em}
\underline{\textbf{Soluciones}}

Vamos a ver soluciones al tratamiento de los datos ausentes:
\begin{itemize}
    \item \textbf{No hacer nada:} Hay métodos (como los árboles de decisión) que son robustos a los datos ausentes.
    \item \textbf{Eliminar los atributos:} Esta es una solución extrema que es necesaria en el alto porcentaje de nulos. En otros casos podemos encontrar un atributo dependiente de mayor calidad.
    \item \textbf{Eliminar el objeto:} Suele hacerse cuando en un problema de clasificación al clase está ausente, pero no es efectivo si el porcentaje de ausentes varía mucho entre atributos. Además, puede introducir sesgo.
    \item \textbf{Reemplazar:} Se puede reemplazar el hueco por un valor. Hay varias formas de hacerlo:
        \begin{itemize}
            \item Manualmente si no hay muchos valores ausentes.
            \item Por un valor que preserve la media o la varianza en datos numéricos, o la moda en datos nominales.
            \item Imputación: consiste en reemplazar estos valores faltantes con estimaciones o valores plausibles, permitiendo que el dataset esté completo y sea útil para el análisis. Hay varias formas de usarla:
                \begin{itemize}
                    \item Usar el valor medio (de todos los valores de los atributos o de solo los que pertenecen a la misma clase).
                    \item Usar el valor más probable.
                    \item Predecir el valor mediante alguna técnica (regresión, KNN, etc).
                \end{itemize}
            \item Mediante técnicas específicas. Por ejemplo, la detección del sexo a través del nombre.
        \end{itemize}
\end{itemize}

A pesar de que soluciones como la imputación sean técnicas muy útiles y frecuentes, hay que tener en cuenta que se sigue perdiendo información, e incluso que el dato que introducimos sea erróneo.

\subsubsection{Datos ruidosos}
Entendemos el ruido como un error o varianza aleatoria en una medición de una variable.

Existen varios métodos para eliminar el ruido:
\begin{itemize}
    \item \textbf{Discretización:} Este método suaviza un conjunto de valores consultando los vecinos. 
    \begin{itemize}
        \item Los valores ordenados se distribuyen en categorías con el mismo número de elementos (igual frecuencia, igual anchura\dots).
        \item Se sustotuyen los valores de cada categoría por otros: media, mediana, extremo más cercano\dots. 
    \end{itemize}
    \item \textbf{Regresión:} Se ajustan los valores a una función.
    \item \textbf{Clustering:} Se agrupan los valores para identificar los outliers (valores atípicos).
\end{itemize}

\subsubsection{Datos inconsistentes y discrepancias}
Antes de empezar a solucionar los datos ausentes y ruidosos hayq ue detectar las discrepancias en los datos. Las inconsistencias se deben a:
\begin{itemize}
    \item Formularios de entrada mal diseñados.
    \item Errores en los dispositivos de entrada.
    \item Error humano al introducir datos.
    \item Obsolescencia de los datos.
    \item Datos recogidos para otros usos.
    \item Formato inconsistente.
\end{itemize}

Hay dos tipos de herramientas para solucionar estos problemas:
\begin{itemize}
    \item \textbf{Depuración de datos:} Estas herramientas utilizan conocimiento en el dominio para detectar y corregir errores.
    \item \textbf{Auditoría de datos:} Encuentran discrepancias mediante un análisis que permite descubrir reglas y relaciones en los datos, detectando violaciones de estas mismas reglas.
\end{itemize} 

\subsubsection{Variables con varianza cercana a cero}
En muchos casos tendremos variables con varianza cero, es decir, de un solo valor. En otros casos, existirán variables que presentan muy poca variedad en sus resultados, es decir, con varianza cercana a cero o muy desbalanceadas. 

Para detectar estas variables se utilizan dos métricas:
\begin{itemize}
    \item \textbf{Ratio entre frecuencia y valor más frecuencte:} Este valor es 1 para variables balanceadas y crece con variables desbalanceadas.
    \item \textbf{Porcentaje de valores únicos:} Este valor es 0 para variables con un solo valor y 1 para variables con todos los valores únicos.
\end{itemize}

Con estas técnicas, si el ratio de frecuencia supera un límite establecido y el porcentaje de valores únicos cae por debajo de este, podemos considerar que la variable posee varianza cercana a cero y eliminarla.


\subsection{Transformaciones de datos}
Las técnicas de transformación nos permiten preparar los datos de forma apropiada para aplicar las técnicas de minería. La mayoría de las técnicas son \textbf{sobreyectivas} es decir, a cada valor original le corresponde un valor transformado.

Entre las técnicas de transformación tenemos:
\begin{itemize}
    \item \textbf{Suavizado:} Elimina el ruido.
    \item \textbf{Agregación:} Resume o agrega datos, usual en cubos de datos. Por ejemplo, acumular las ventanas mensuales en anuales.
    \item \textbf{Generalización:} Convierte datos de bajo a alto nivel.
    \item \textbf{Creación de atributos:} Se crean nuevos atributos a partir de los ya existentes.
    \item \textbf{Normalización:} Escala los datos a un determinado rango, usualmente $\{0,1\}$ o $\{-1,1\}$.
\end{itemize}

\subsubsection{Normalización}
Consiste en escalar los valores a un determinado rango. Es necesario ya que algunas técnicas de minería requieren que los datos estén normalizados, sobre todo en las basadas en concepto de distancia.

Destacan varios métodos de normalización:
\begin{itemize}
    \item \textbf{Normalización min-max:} Se realiza una transformación lineal sobre los datos originales.
    \[
        x' = \frac{x - \min(x)}{\max(x) - \min(x)}(\max(x') - \min(x')) + \min(x')
    \]
    \item \textbf{Normalización por transformada $z$ (z-score):} Los valores de una variable $A$ son normalizados en función de su media $\bar{A}$ y su desviación típica $\sigma_A$.
    \[
        x' = \frac{x - \bar{A}}{\sigma_A}
    \]

    Este método es útil cuando los rangos de las variables son desconocidos o existen valores anormales que dominan en la normalización.

    La desviación y la media resultantes de la nueva variable serán $1$ y $0$ respectivamente.

    \item \textbf{Normalización por escala decimal:} Este tipo de normalización se basa en el desplazamiento del punto decimal en los valores dle atributo. El número de posiciones que se desplaza el punto decimal depende del valor aboluto máximo de la variable $A$.
    \[
        x' = \frac{x}{10^j}
    \]
    donde $j$ es el entero más pequeño que hace que $\lvert x \rvert < 1$.

\end{itemize}

\subsubsection{Discretización}
La discretización es la conversión de un valor numérico a un valor nominal ordenado, que representa un intervalo o ``bin''. Un ejemplo sería al conversión de la nota en escala $\{1,10\}$ a una escala $\{A,B,C,D,E,F\}$.

Razones para discretizar:
\begin{itemize}
    \item Algunas técnicas de minería solo aceptan atributos discretos.
    \item Cuando existen ciertos umbrales significativos.
    \item Para integrar escalas diferentes.
    \item Cuando la interpretación de la escala no es lineal.
\end{itemize}

Tipos de discretización:
\begin{itemize}
    \item \textbf{Supervisada y no supervisada:} Si la técnica utiliza información sobre la clase será \textbf{supervisada}. En caso contrario diremos que es \textbf{no supervisada}.
    \item \textbf{Local o global:} Los métodos \textbf{gloales} aplican los mismos puntos de corte a todas las instancias. Por otro lado, los \textbf{locales} utilizan diferentes puntos de corte a diferentes ocnjuntos de instancias.
    \item \textbf{Ascendente (bottom-up) o descendente (top-down):}
    \begin{itemize}
        \item \textbf{Top-sown (splitting):} Se comienza seleccionando uno o más puntos para dividir el rango del atributo. Se repite el proceso con cada nuevo intervalo hasta que no se pueda dividir más.
        \item \textbf{Bottom-up (merging):} Se van fusionando puntos cercanos entre sí para formar intervalores y repetir el proceso con nuevos intervalos.
    \end{itemize}
\end{itemize}

Las ténicas más comunes en la discretización son las siguientes:
\begin{itemize}
    \item \textbf{Binning:} Es una técnica descente y no supervisada. 
    \item \textbf{Análisis del histograma:} Es una técnica descendente y no supervisada.
    \item \textbf{Discretización basada en la entropía:} Es una técnica descendente y supervisada.
    \item \textbf{Fusión de intervalos mediante análisis $\chi^2$:} Es una técnica ascendente y supervisada.
    \item \textbf{Análisis de cluster:} Es una técnica ascendente y no supervisada.
\end{itemize}

\vspace{1 em}
\underline{\textbf{Binning}}

Vamos a ver las diferentes técnicas de \textit{Binning} que hay.

\begin{itemize}
    \item \textbf{Binning con intervalos de la misma longitud (equal-width):} Se divide el rango de valores en intervalos de misma longitud. Para determinal la longitud de los intervalos se usa:
    \[
        w = \frac{\max(x) - \min(x)}{N}
    \]
    donde $N$ es el número de intervalos y $x$ es el atributo a discretizar. Los límites de los intervalos son $[\min(x) + iw, \min(x) + (i+1)w]$ con $i = 0,1,\ldots,N-1$. 

    Esta técnica puede verse alterada por la presencia de outliers y datos sesgados.

    \item \textbf{Binning por intervalos de la misma amplitud (equal-depth, frequency):} Se divide el rango de valores en intervalos que contengan aproximadamente el mismo número de elementos. 
    
    Para saver cuántos elementos debee tener cada intervalo se divide el número total de instancias por el número de intervalos. Para determinal cuáles son los valores en los que realizar la partición se suele utulizar el punto medio entre los dos extremos de los intervalos. En el caso de que valores repetidos caigan en intervalos diferentes, habrá que tomar la decisión de a qué intervalo se asignan dichos valores, permitiendo que existan invervalos con un número de valores alejados de la media.
\end{itemize}

\vspace{1 em}
\underline{\textbf{Discretización basada en histograma}}

Un \textbf{histograma} nos muestra la frecuencia de cada uno de los posibles valores del atributo, agrupando en un mismo \textit{balde} pares valor-frecuencia. Podemos discretizar el rango de valores de un atributo agrupando baldes:
\begin{itemize}
    \item \textbf{Intervalos de la misma longitud}
    \item \textbf{Intervalos de la misma frecuencia}
    \item \textbf{Varianza óptima.} Se consideran todas las posibilidades de agrupación de baldes y se selecciona la de menor varianza.
    \item \textbf{Máxima diferencia.} Los límites de los baldes se establecen entre los valores consecutivos con $\beta - 1$ mayores distancias, siendo $\beta$ el número de baldes deseado.
\end{itemize}

Los histogramas son muy efectivos tanto en datos densos como en dispersos, al igual que tanto para datos uniformes como para altamente sesgados. Además, las particiones basadas en varianza y la diferencia suelen ser más precisas y prácticas.

Hay varios criterios a la hora de elegir el número de intervalos que podemos destacar:
\begin{itemize}
    \item \textbf{Raíz cuadrada:}
    \[
        n_{intervalos} = \sqrt{n_{muestras}}
    \]
    \[
        ancho = \frac{\max(x) - \min(x)}{\sqrt{n_{muestras}}}
    \]

    \item \textbf{Sturges:}
    \[
        n_{intervalos} = 1 + \log_2(n_{muestras})
    \]
    \[
        ancho = \frac{\max(x) - \min(x)}{1 + \log_2(n_{muestras})}
    \]

    \item \textbf{Rice:}
    \[
        n_{intervalos} = \lceil 2n^{1/3} \rceil   \]
    \[
        ancho = \frac{\max(x) - \min(x)}{\lceil 2\sqrt[3]{n} \rceil}
    \]

    \item \textbf{Scott:}
    \[
        n_{intervalos} = \frac{\max(x) - \min(x)}{\frac{3.5\sigma}{n^{1/3}}}
    \]
    \[
        ancho = \frac{3.5\sigma}{n^{1/3}}
    \]

    \item \textbf{Freedman-Diaconis:}
    \[
        n_{intervalos} = \frac{\max(x) - \min(x)}{\frac{2 \cdot IQR(x)}{n^{1/3}}}
    \]
    \[
        ancho = \frac{2 \cdot IQR(x)}{n^{1/3}}
    \]
\end{itemize}

\vspace{1 em}
\underline{\textbf{Discretización basada en la entropía}}

Es una técnica descendente y supervisada que utiliza el concepto que gannacia de información, utilizando la entropía como variable objetivo para determinar los puntos de corte, muy parecido a lo que hacen los árboles de decisión.

El proceso es el siguiente:
\begin{itemize}
    \item \textbf{Cálculo de la entropía:} Se calcula la entropía inicial con la fórmula.
    \[
        H(S) = -\sum_{i=1}^{c} p_i \log_2 p_i
    \]
    donde $p_i$ es la probabilidad de que un objeto pertenezca a la clase $i$.
    \item \textbf{Selección de puntos de corte:} Se prueban diferente puntos de corte dentro del atributo continuo, quedando en conjunto izquierdo y derecho.
    \item \textbf{Cálculo de la ganancia de información:} Se calcula la ganancia de información.
    \[
        IG(T) = H(S) - \bigg( \frac{|S_{izq}|}{|S|}H(S_{izq}) + \frac{|S_{der}|}{|S|}H(S_{der}) \bigg)
    \]

    \item Se elige el punto que maximiza la ganancia de información.
    \item \textbf{División recursiva:} Se repite el proceso hasta cumplir el criterio de parada. Este puede ser:
    \begin{itemize}
        \item La ganancia de información es menor que un umbral.
        \item Se alcanza el número mínimo de insancias por intervalo.
    \end{itemize}
\end{itemize}

\vspace{1 em}
\underline{\textbf{Fusión de intervalos mediante análisis $\chi^2$}}

Es una técnica ascendente y supervisada que fusiona intervalos adyacentes que presenten una distribución de clases parecida. Preserva la relación entre la característica y la variable objetivo.

El proceso es el siguiente:
\begin{itemize}
    \item \textbf{Inicialización:} Ordenar los valores de la característica continua.
    \item \textbf{Binning inicial:} Cada valor es un \textit{bin} (intervalo) separado.
    \item \textbf{Cálculo de $\chi^2$:} Se calcula el estadístico $\chi^2$ para cada par de bins adyacentes.
    \item \textbf{Fusión de bins:} Se fusionan los bins con menor $\chi^2$.
    \item \textbf{Condición de parada:} Se repite el proceso hasta que se cumpla una condición de parada (número de bins, $\chi^2$ mínimo\dots).
    \item \textbf{Bins finales:} Los bins finales son los que mejor preservan la relación con la variable objetibo.
\end{itemize}

\vspace{1 em}
\underline{\textbf{Análisis de clusters}}

Se pueden usar algoritmos de clustering para discretizar atributos numéricos. Para ello hay que asociar una categoría a cada gurpo o cluster. Pueden generar discretizaciones de alta calidad, ya que tienen en cuentra la distrivución del atributo a discretizar además de la distnacia de los datos. Además, las técnicas de clustering jerárquico nos permiten obtener una jerarquía conceptual.

\subsubsection{De variables categóricas a numéricas}
Las variables categóricas son variables cuyo dominio lo forman un número finito de etiquetas o categorías. Puedes ser nominales (sin orden) o ordinales (con orden).

Algunas técnicas son:
\begin{itemize}
    \item \textbf{Codificación ordinal:} Se aplica a variables categóricas ordinales. La idea es mantener el orden de categorías asignando un número entero a cada categoría (por ejemplo pasar de $\{bajo, medio, alto\}$ a $\{1,2,3\}$).
    \item \textbf{Codificación one-hot:} Se aplica a categorías nominales. Se crea una nueva variable binaria para cada categoría, asignando un 1 si la categoría está presente y un 0 si no lo está.
    \item \textbf{Codificación por variables \textit{dummy}:} La codificación One-Hot tiene el problema de introducir información redundante. Por ello, la solución es crear $N-1$ variables para $N$ categorías, donde la categoría $N$ se representa con todas las variables a 0.
\end{itemize}


\subsection{Datos desbalanceados}

Se dice que en un dataset los datos están desbalancedaos cuando una o más clases presentan proporciones muy bajas respecto a las otras clases en el conjunto de entrenamiento. Un ejemplo sería el tener una clase $A$ con un 94\% y una clase $B$ co un 6\%.

En el caso de los datos desbalanceados, se espera que un modelo prediga la clase mayoritaria mejor que lo haría un clasificador que elija siempre la clase mayoritaria. Por ejemplo, en el ejemplo anterior un clasificador que prediga la clase $A$ con un 93\% sería un mal clasificador, pero si predice con un 95\% de precisión sería un buen clasificador.

Soluciones:
\begin{itemize}
    \item Utilizar técnicas de muestreo para mitigar el desbalanceo de clases.
    \item Utilizar otras medidas de rendimiento a la hora de evaluar modelos.
    \item Utilizar modelo que permitan mitigar esta problemática.
\end{itemize}

\subsubsection{Técnicas de muestreo}
Vamos a explicar las técnicas de muestreo que sirven para mitigar el desbalanceo de clases.
\begin{itemize}
    \item \textbf{Técnicas básicas:}
    \begin{itemize}
        \item \textbf{Downsampling:} Seleccionar aleatoriamente un subconjunto de todas las clases para que sus frecuencias se ajusten a la de la clase minoritaria.
        \item \textbf{Upsampling:} Realizar un muestreo aleatorio con reemplazo para que sus frecuencias se adapten a las de la clase mayoritaria.
    \end{itemize}

    \item \textbf{Otras ténicas:}
    \begin{itemize}
        \item \textbf{SMOTE:} Utiliza la información de los vecinos más cercanos para generar nuevas muestras de la clase minoritaria, haciendo así que las fronteras de la clase no se distorsionen. 
        \item \textbf{ROSE:} Esta técnica genera nuevas muestras en la vecindad de las ya existentes para equilibrar la frecuencia de clases. Es útil para problemas de clasificación binaria y se puede combinar con técnicas de sampling para evaluar modelos de aprendizaje. A la hora de generar las muestras se utiliza una distribución de probabilidad centrada en la muestra y con una matriz de covarianza concreta.
    \end{itemize}
\end{itemize}

\subsubsection{Medidas de rendimiento}
El error de clasificación y la precisión (accuracy) no son métricas apropiadas cuando tenemos métricas desbalanceadas. 

Algunas medidas de eficacia considerando la clase mayoritaria como la negativa son:
\begin{itemize}
    \item \textbf{Matriz de confusión.}
    \item \textbf{Precisión o valor predictivo positivo.} Es la proporción de verdaderos positivos sobre el total de predicciones positivas. $PPV = \frac{TP}{TP + FP}$.
    \item \textbf{Recall o sensibilidad.} Es la proporción de verdaderos positivos sobre el total de positivos reales. $Recall = \frac{TP}{TP + FN}$.
    \item \textbf{F1-score.} Es la media armónica de la precisión y el recall. $F1 = 2 \cdot \frac{PPV \cdot Recall}{PPV + Recall}$.
    \item \textbf{AUC-ROC.} Es el área bajo la curva ROC.\@ Cuanto más cerca de 1, mejor.
\end{itemize}

Depende de la combinación entre \textit{recall} y \textit{precisión} habrá difernetes interpretaciones del modelo:
\begin{itemize}
    \item \textbf{Recall alto y precisión alta:} La clase es detectada perfectamente por el modelo.
    \item \textbf{Recall bajo y precisión alta:} El modelo no puede detectar bien la clase pero cuando lo hace es muy fiable.
    \item \textbf{Recall alto y precisión baja:} La clase es detectada aceptablemente por el modelo pero también incluye muestas de otras clases.
    \item \textbf{Recall bajo y precisión baja:} El modelo no detecta bien la clase y cuando lo hace no es fiable.
\end{itemize}

\subsubsection{Modelos para datos desbalanceados}
Existen algoritmos optimizados para clases desbalanceadas, de forma que tienen en cuenta la distrivución de clases en la construcción del modelo. Algunos ejemplos son:
\begin{itemize}
    \item \textbf{SVM:} Dan buenos resultados para problemas desbalanceados y existen varias adaptaciones (z-SVM y GSVM-RU).
    \item \textbf{kNN:} kENN y CCNND.\@
    \item \textbf{Aprendizaje sensitivo al costo:} Consiste en cambiar el coste de los errores, dando mayor importancia a los falsos positivos de la clase mayoritaria o a los verdaderos positivos de la clase minoritaria. 
    
    Tiene varios problemas: la matriz de costes no se conoce y puede ser complicado definicarla, puede causar sobreajuste y algunos estudios apuntan a que es igual de eficiente que técnicas de muestreo.

    \item \textbf{Métodos de ensamble:} Reducen la varianza de la clasificación y existen métodos adaptados a las clases desbalanceadas (SMOTEBoost, RUSBott, DataBoostIM, cost-sensitive boosting y SMOTEBagging).
    \item \textbf{One-Class learning:} Son conocidos como métodos basados en reconocimiento. El modelo es entrenado para representar adecuadamente la clase minoritaria. Muchas ténicas no están preparads para ser entradas con una sola clase, aunque hay algunas adaptaciones (One Classs SVM, Isolation Forest, Minimum Covariance Determinant\dots).
\end{itemize}

\newpage
\section{Reducción de dimensionalidad}

\subsection{Métodos}
Existen dos tipos de métodos para la reducción de la dimensionalidad:
\begin{itemize}
    \item \textbf{Extracción de características:} El objetivo es encontrar $k$ dimensiones que sean combinación de las $d (d > k)$ dimensiones originales. Si son extracciones supervisadas maximizan la discriminación entre clases, si son no supervisadas minimizan la pérdida de información.
    
    Algunas técnicas son:
    \begin{itemize}
        \item Análisis lineal discriminante (LDA).
        \item Análisis discriminante generalizado (GDA).
        \item Análsiis de componentes principales (PCA).
        \item Modelos d evariables latentes basados en procesos gaussianos constreñidos.
        \item ``t-distributed stochastic neighbor embedding'' (t-SNE).
        \item ``Uniform Manifold Approximation and Projection'' (UMAP).
        \item Técnicas basadas en Deep Learning (convolucuón, embeddings, transformers\ldots)
    \end{itemize}

    \item \textbf{Selección de características:} EL objetivo es encontrar $k$ dimensiones entre las $d$ dimensiones originales que aporten la mayor cantidad de información, descartando las $(d-k)$ restantes.
\end{itemize}

\subsection{Selección de características}
La selección de características trata de encontrar un subconjunto de características óptimo dado un determinado criterio de selección.

El \textbf{subconjunto óptimo} de características es el subconjunto mínmimo que permite construir una hipótesis consistente con los datos de entrenamiento. Una definición más formal sería:

\textbf{Subconjunto óptimo (definición):} \\
Sea $F$ el conjunto de todas las características y $CR$ el conjunto de clases, el \textit{subconjunto óptimo} es el conjunto mínimo de características $G$ tal que $P(C|G)$ es igual, o lo más cercano posible, a $P(C|F)$.

\subsubsection{Fases de selección de características}
El proceso de selección de características se desarrolla en dos fases:
\begin{itemize}
    \item \textbf{Generación:} Se van generando los distintos subconjuntos de características candidatos a ser el subconjunto óptimo.
    \item \textbf{Evaluación:} Se evalúan todos los subconjutnos candidatos hasta que se cumpla algún criterio de parada.
\end{itemize}

Dependiendo de si se hacen o no las dos fases podemos tener:
\begin{itemize}
    \item \textbf{Métodos basados en la selección de conjuntos de características:} Son aquellos que incluyen alguna estrategia para la generación y evaluación de características.
    \item \textbf{Métodos de evaluación de características:} Son aquellos que solo generan ningún subconjunto, solo evalúan las variables estableciendo un ``ranking'' de acuerdo a la medida.
\end{itemize}

\vspace{1 em}
\textbf{\underline{Generación de subconjuntos}}

Una búsqueda exhaustiva de todas los posibles subconjuntos no es computacionalmente posible, ya que por ejemplo con $n = 100$ características, \linebreak tendríamos $10^{100}$ subconjuntos posibles.

Se pueden realizar diferentes estrategias de búsqueda:
\begin{itemize}
    \item \textbf{Exhaustiva:} Se barre todo el espacio de posibles subconjuntos, por lo que solo es posible para pocas características. Es la única forma de \textbf{garantizar} el \textbf{subconjunto óptimo}.
    \item \textbf{Heurística:} Se dispone de alguna información de qué subconjunto es el más prometedor. No garantiza el subconjunto óptimo pero encuentra una buena solución en un tiempo razonable.
    \item \textbf{Aleatoria:} Se parte de una configuración inicial formada por un conjunto finito de posibles subconjuntos. Se va modificando poco a poco la configuración inicial para ir convergiendo a una solución. Tampoco garantiza una solución óptima.
\end{itemize}

\vspace{1 em}
\textbf{\underline{Dirección de búsqueda}}

Para definir la estrategia de búsqueda hay que definir la \textbf{dirección de búsqueda}:
\begin{itemize}
    \item \textbf{Hacia delante (forward):} Se empieza con el conjunto vacío y se van añadiendo una características cada vez.
    \item \textbf{Hacia atrás (backward):} Se empieza con el conjunto completo y se va elimiando una característica cada vez.
    \item \textbf{Bidireccional:} Se comienza por los dos extremos del espacio de búsqueda y se realiza de forma paralela hacia delante y hacia atrás.
\end{itemize}

\vspace{1 em}
\textbf{\underline{Evaluación}}

Hay diferentes tipos de evaluación:
\begin{itemize}
    \item \textbf{Métodos basados en filtros:} Evalúan la relevancia de características sin tener en cuenta la tarea que se va a realizar con los datos.
    \item \textbf{Métodos basados en envoltura (wrappers):} Cada subconjunto candidato es evaluado a través de la eficacia de un predictor, teniendo así en cuenta la tarea que se va a realizar.
    \item \textbf{Métodos empotrados (embedded):} En los que el proceso de seleción de variables está integrado en la técnica de construcción del predictor.
\end{itemize}

\subsection{Evaluadores de características}
Estos métodos eliminan la fase de búsqueda de subconjuntos, por lo que hacen es evaluar cada variables con una determinada medida y establecer un ``ranking'' de las mismas.

Son técnicas muy rápidas y fácilmente escalables, auqnue tienen el inconveniente de que a veces es difícil establecer el umbral de corte.

Las medidas utilizadas para evaluar las características son:
\begin{itemize}
    \item \textbf{Medidas de evaluación por pares:} Son aquellas que evalúan la dependencia de las variables con la variable objetivo. Estas se pueden agrupar en:
    \begin{itemize}
        \item \textbf{Basadas en correlación:} Miden la correlación entre la variable y la variable objetivo.
        \item \textbf{Basadas en incertidumbre:} Se apoyan en medidas típicas de la teoría de la información como la entropía.
        \item \textbf{Basadas en test de hipótesis:} Se obtienen \textit{p-values} que pueden ser usados para clasificar las variables.
        \item \textbf{Basadas en el poder discriminativo:} Utilizan un modelo de una sola variable y estiman su error.
    \end{itemize}
    \item \textbf{Medidas de evaluación simultáneas:} Son aquellas que realizan la evaluación de forma simultánea en todas las variables. El ejemplo más característico es la familia \textbf{Relief}, que utiliza medidas de distancia para calcular la relevancia.
\end{itemize}

\subsection{Filtros}
Los métodos basados filtros evalúan la relevancia de las caraceterísticas teniendo en cuenta solo las propiedades intrínsecas de las mismas, por lo que no tienen en cuenta la tarea que se va a realizar. Estos métodos son independientes de la técnica de clasificación o regresión que se va a utilizar y no se ven sesgados influencia.

\subsubsection{Medidas de relevancia}
\begin{itemize}
    \item \textbf{Ratio de inconsistencias:} Una inconsistencia aparece cuando dos instancias iguales, de acuerdo con el subconjunto de características evaluado, pertenecen a clases diferentes.
    \item \textbf{Medidas basadas en distancias:} Se centra en encontrar el subconjunto de caracterśticas que mejor separe las diferentes clases. 
\end{itemize}

Algunos algoritmos representativos son:
\begin{itemize}
    \item \textbf{FOCUS:} Búsqueda exhaustiva con medida de consistencia.
    \item \textbf{Cluster-based Feature Selection Approach:} k-medias basado en medidas de correlación en el espacio de características, utilizando el criterio del índice silueta como criterio para elegir el número óptimo de clusters.
    \item \textbf{CFS:} Búsqueda heurística (ramificación y poda) con una medida basada en la correlación.
\end{itemize}

\subsubsection{Métodos basados en envoltura (``wrappers'')}
En este tipo de métodos, la evaluación de cada uno de los subconjuntos candidates se realiza mediante la construcción de un clasificador (o regesor). Por lo tanto, como medida de evaluación utilizan la capacidad predictiva de clasificador, seleccioando así el subconjunto de caracterísitcas que produce el mejor clasificador. 

\begin{itemize}
    \item \textbf{Ventajas:} Producen mejores resultados al estar orientados al problema de clasificación.
    \item \textbf{Desventajas:} Tienen un mayor riesgo de sobreajuste que los filtros y son más costosos computacionalmente.
\end{itemize}

Vamos a ver ahora un algoritmo de ``wrapper'' con un esquema de eliminación recursiva:
\begin{enumerate}
    \item Construir un modelo con todas las características
    \item Evaluar el modelo
    \item Calcular la relevancia de las caraceterísticas
    \item Crear una lita con las características ordenadas de mayot a menor relevancia
    \item Para $size = {n,\dots,1}$ hacer:
    \begin{enumerate}
        \item Crear un subconjunto $S_{size}$ con las $size$ características más relevantes
        \item Construir un modelo con las características $S_{size}$
        \item Evaluar el modelo
        \item \textit{[Opcional]} Recalcular la relevancia de las características
    \end{enumerate}
    \item Crear una lista con todos los $S_i$ y el resultado de la evaluación
    \item Determinar el subconjunto óptimo $S_{opt}$
\end{enumerate}

Se puede adaptar el esquema anterior para realizar una búsqueda hacia delante de subconjuntos: se empieza por un conjunto de tamaño 1 y se van agreganto las características más relevantes.

Para evitar el sobreajuste y el alto coste computacional hay esquemas que incluyen un bucle externo para llevar a cabo un remuestreo. Vamos a ver un algoritmo con este bucle:
\begin{enumerate}
    \item para cada iteración de remuestreo hacer:
    \begin{enumerate}
        \item Crear los conjuntos de entrenamienot $E$ y prueba $T$
        \item Construir un modelo sobre E con todas las características
        \item Evaluar el modelo $T$
        \item Calcular la relevancia de las caraceterísticas
        \item Crear una lista con las características ordenadas de mayor a menor relevancia
    \end{enumerate}
\end{enumerate}

\end{document}
