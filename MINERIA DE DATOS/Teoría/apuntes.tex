% Clase de documento
\documentclass[12pt, letterpaper]{article}

% Paquetes
\usepackage[utf8]{inputenc}
\usepackage[spanish]{babel}
\usepackage{biblatex}
\usepackage{csquotes}
\usepackage{datetime}
\usepackage{amsmath}
\usepackage{lipsum}
\usepackage{hyperref}
\usepackage{fancyhdr}
\usepackage{parskip}

%------------ 
% Decoración
%------------
\fancyhf{}
\setlength{\headheight}{15.71667pt}
\addtolength{\topmargin}{-3.71667pt}
\fancyhf{}

% Header
\fancyhead[L]{\textsc{\doctitle}}
\renewcommand{\sectionmark}[1]{\markright{#1}}
\fancyhead[R]{\textit{\nouppercase{\rightmark}}}

% Footer
\renewcommand{\footrulewidth}{0.4pt}
\fancyfoot[C]{Página \thepage}

% Título
\newcommand{\doctitle}{Apuntes de Minería de Datos}
\title{\doctitle}
\author{Juan Luis Serradilla Tormos}
\date{\monthname[\month] de \the\year}

% Bibliografía
\addbibresource{test.bib}

% Eliminar sangría
\setlength{\parindent}{0pt}

% Aumentar la separación entre párrafos
\setlength{\parskip}{1em plus 0.5em minus 0.2em}

%-----------
% Documento
%-----------
\begin{document}

% Mostrar header y footer
\pagestyle{fancy}

% Mostrar el título
\maketitle

% Índice
\newpage
\tableofcontents

% Contenido
\newpage
\section{Preprocesamiento de datos}

\subsection{Introducción}
\begin{itemize}
    \item El resultado de la minería de datos depende en gran medida de la calidad de los datos.
    \item El conjunto de datos estará formado por objetos
    \item Los objetos se describen por medio de atributos
    \item Un atributo tiene asociado un tipo que define de los valores que puede tomar
\end{itemize}

\subsection{Limpieza de datos}
Los errores en los datos pueden deberse a diferentes causas:
\begin{itemize}
    \item \textbf{Datos incompletos}: Pueden faltar atributos de interés, valores de los propios\ldots
    \item \textbf{Datos ruidosos}: Datos con ruido o errores, valores duplicados\ldots
    \item \textbf{Datos inconsistentes}: Datos que discrepan en códigos y nombres, en valores duplicados, etc. Por ejemplo.
    \begin{itemize}
        \item Edad = ''42'', Fecha de nacimiento = ''12/07/2015''
        \item Objetos con escala ``1,2,3'' y otros con escala ``A,B,C''
    \end{itemize}
    \item \textbf{Errores intencionados:} Datos que se introducen para encubrir falta de datos. Por ejemplo, encontrar la misma fecha de nacimiento para un gran grupo de personas ya que no tenían fecha 
\end{itemize}

\subsubsection{Datos ausentes}
Los datos ausentes pueden producir varios errores:
\begin{itemize}
    \item \textbf{Pérdida de eficacia:} Se extraen menos patronas y las conclusiones son menos concluyentes.
    \item \textbf{Complicaciones al analizar:} Surgen complicaciones debido a que hay técnicas que no están preparadas para gestionarlos.
    \item \textbf{Sesgo:} Puede haber sesgo en los datos que se extraen.
\end{itemize}

Por todo esto, al limpiar un dataset es necesario tener en cuenta los datos ausentes. Hay varias formas de detectarlos:
\begin{itemize}
    \item Generalmente se representan como valores nulos.
    \item Puede haber nulos camuflados, es decir, valores que no son nulos pero los representan. Esto se debe a que la integridad del sistema no permite introducir nulos en campos con ciertos formatos (direcciones, teléfonos, códigos postales\ldots)
\end{itemize}

\vspace{1em}
\underline{\textbf{Soluciones}}

Vamos a ver soluciones al tratamiento de los datos ausentes:
\begin{itemize}
    \item \textbf{No hacer nada:} Hay métodos (como los árboles de decisión) que son robustos a los datos ausentes.
    \item \textbf{Eliminar los atributos:} Esta es una solución extrema que es necesaria en el alto porcentaje de nulos. En otros casos podemos encontrar un atributo dependiente de mayor calidad.
    \item \textbf{Eliminar el objeto:} Suele hacerse cuando en un problema de clasificación al clase está ausente, pero no es efectivo si el porcentaje de ausentes varía mucho entre atributos. Además, puede introducir sesgo.
    \item \textbf{Reemplazar:} Se puede reemplazar el hueco por un valor. Hay varias formas de hacerlo:
        \begin{itemize}
            \item Manualmente si no hay muchos valores ausentes.
            \item Por un valor que preserve la media o la varianza en datos numéricos, o la moda en datos nominales.
            \item Imputación: consiste en reemplazar estos valores faltantes con estimaciones o valores plausibles, permitiendo que el dataset esté completo y sea útil para el análisis. Hay varias formas de usarla:
                \begin{itemize}
                    \item Usar el valor medio (de todos los valores de los atributos o de solo los que pertenecen a la misma clase).
                    \item Usar el valor más probable.
                    \item Predecir el valor mediante alguna técnica (regresión, KNN, etc).
                \end{itemize}
            \item Mediante técnicas específicas. Por ejemplo, la detección del sexo a través del nombre.
        \end{itemize}
\end{itemize}

A pesar de que soluciones como la imputación sean técnicas muy útiles y frecuentes, hay que tener en cuenta que se sigue perdiendo información, e incluso que el dato que introducimos sea erróneo.

\subsubsection{Datos ruidosos}
Entendemos el ruido como un error o varianza aleatoria en una medición de una variable.

Existen varios métodos para eliminar el ruido:
\begin{itemize}
    \item \textbf{Discretización:} Este método suaviza un conjunto de valores consultando los vecinos. 
    \begin{itemize}
        \item Los valores ordenados se distribuyen en categorías con el mismo número de elementos (igual frecuencia, igual anchura\dots).
        \item Se sustotuyen los valores de cada categoría por otros: media, mediana, extremo más cercano\dots. 
    \end{itemize}
    \item \textbf{Regresión:} Se ajustan los valores a una función.
    \item \textbf{Clustering:} Se agrupan los valores para identificar los outliers (valores atípicos).
\end{itemize}

\subsubsection{Datos inconsistentes y discrepancias}
Antes de empezar a solucionar los datos ausentes y ruidosos hayq ue detectar las discrepancias en los datos. Las inconsistencias se deben a:
\begin{itemize}
    \item Formularios de entrada mal diseñados.
    \item Errores en los dispositivos de entrada.
    \item Error humano al introducir datos.
    \item Obsolescencia de los datos.
    \item Datos recogidos para otros usos.
    \item Formato inconsistente.
\end{itemize}

Hay dos tipos de herramientas para solucionar estos problemas:
\begin{itemize}
    \item \textbf{Depuración de datos:} Estas herramientas utilizan conocimiento en el dominio para detectar y corregir errores.
    \item \textbf{Auditoría de datos:} Encuentran discrepancias mediante un análisis que permite descubrir reglas y relaciones en los datos, detectando violaciones de estas mismas reglas.
\end{itemize} 

\subsubsection{Variables con varianza cercana a cero}
En muchos casos tendremos variables con varianza cero, es decir, de un solo valor. En otros casos, existirán variables que presentan muy poca variedad en sus resultados, es decir, con varianza cercana a cero o muy desbalanceadas. 

Para detectar estas variables se utilizan dos métricas:
\begin{itemize}
    \item \textbf{Ratio entre frecuencia y valor más frecuencte:} Este valor es 1 para variables balanceadas y crece con variables desbalanceadas.
    \item \textbf{Porcentaje de valores únicos:} Este valor es 0 para variables con un solo valor y 1 para variables con todos los valores únicos.
\end{itemize}

Con estas técnicas, si el ratio de frecuencia supera un límite establecido y el porcentaje de valores únicos cae por debajo de este, podemos considerar que la variable posee varianza cercana a cero y eliminarla.


\subsection{Transformaciones de datos}
Las técnicas de transformación nos permiten preparar los datos de forma apropiada para aplicar las técnicas de minería. La mayoría de las técnicas son \textbf{sobreyectivas} es decir, a cada valor original le corresponde un valor transformado.

Entre las técnicas de transformación tenemos:
\begin{itemize}
    \item \textbf{Suavizado:} Elimina el ruido.
    \item \textbf{Agregación:} Resume o agrega datos, usual en cubos de datos. Por ejemplo, acumular las ventanas mensuales en anuales.
    \item \textbf{Generalización:} Convierte datos de bajo a alto nivel.
    \item \textbf{Creación de atributos:} Se crean nuevos atributos a partir de los ya existentes.
    \item \textbf{Normalización:} Escala los datos a un determinado rango, usualmente $\{0,1\}$ o $\{-1,1\}$.
\end{itemize}

\subsubsection{Normalización}
Consiste en escalar los valores a un determinado rango. Es necesario ya que algunas técnicas de minería requieren que los datos estén normalizados, sobre todo en las basadas en concepto de distancia.

Destacan varios métodos de normalización:
\begin{itemize}
    \item \textbf{Normalización min-max:} Se realiza una transformación lineal sobre los datos originales.
    \[
        x' = \frac{x - \min(x)}{\max(x) - \min(x)}(\max(x') - \min(x')) + \min(x')
    \]
    \item \textbf{Normalización por transformada $z$ (z-score):} Los valores de una variable $A$ son normalizados en función de su media $\bar{A}$ y su desviación típica $\sigma_A$.
    \[
        x' = \frac{x - \bar{A}}{\sigma_A}
    \]

    Este método es útil cuando los rangos de las variables son desconocidos o existen valores anormales que dominan en la normalización.

    La desviación y la media resultantes de la nueva variable serán $1$ y $0$ respectivamente.

    \item \textbf{Normalización por escala decimal:} Este tipo de normalización se basa en el desplazamiento del punto decimal en los valores dle atributo. El número de posiciones que se desplaza el punto decimal depende del valor aboluto máximo de la variable $A$.
    \[
        x' = \frac{x}{10^j}
    \]
    donde $j$ es el entero más pequeño que hace que $\lvert x \rvert < 1$.

\end{itemize}

\subsubsection{Discretización}
La discretización es la conversión de un valor numérico a un valor nominal ordenado, que representa un intervalo o ``bin''. Un ejemplo sería al conversión de la nota en escala $\{1,10\}$ a una escala $\{A,B,C,D,E,F\}$.

Razones para discretizar:
\begin{itemize}
    \item Algunas técnicas de minería solo aceptan atributos discretos.
    \item Cuando existen ciertos umbrales significativos.
    \item Para integrar escalas diferentes.
    \item Cuando la interpretación de la escala no es lineal.
\end{itemize}

Tipos de discretización:
\begin{itemize}
    \item \textbf{Supervisada y no supervisada:} Si la técnica utiliza información sobre la clase será \textbf{supervisada}. En caso contrario diremos que es \textbf{no supervisada}.
    \item \textbf{Local o global:} Los métodos \textbf{gloales} aplican los mismos puntos de corte a todas las instancias. Por otro lado, los \textbf{locales} utilizan diferentes puntos de corte a diferentes ocnjuntos de instancias.
    \item \textbf{Ascendente (bottom-up) o descendente (top-down):}
    \begin{itemize}
        \item \textbf{Top-sown (splitting):} Se comienza seleccionando uno o más puntos para dividir el rango del atributo. Se repite el proceso con cada nuevo intervalo hasta que no se pueda dividir más.
        \item \textbf{Bottom-up (merging):} Se van fusionando puntos cercanos entre sí para formar intervalores y repetir el proceso con nuevos intervalos.
    \end{itemize}
\end{itemize}

Las ténicas más comunes en la discretización son las siguientes:
\begin{itemize}
    \item \textbf{Binning:} Es una técnica descente y no supervisada. 
    \item \textbf{Análisis del histograma:} Es una técnica descendente y no supervisada.
    \item \textbf{Discretización basada en la entropía:} Es una técnica descendente y supervisada.
    \item \textbf{Fusión de intervalos mediante análisis $\chi^2$:} Es una técnica ascendente y supervisada.
    \item \textbf{Análisis de cluster:} Es una técnica ascendente y no supervisada.
\end{itemize}

\vspace{1 em}
\underline{\textbf{Binning}}

Vamos a ver las diferentes técnicas de \textit{Binning} que hay.

\begin{itemize}
    \item \textbf{Binning con intervalos de la misma longitud (equal-width):} Se divide el rango de valores en intervalos de misma longitud. Para determinal la longitud de los intervalos se usa:
    \[
        w = \frac{\max(x) - \min(x)}{N}
    \]
    donde $N$ es el número de intervalos y $x$ es el atributo a discretizar. Los límites de los intervalos son $[\min(x) + iw, \min(x) + (i+1)w]$ con $i = 0,1,\ldots,N-1$. 

    Esta técnica puede verse alterada por la presencia de outliers y datos sesgados.

    \item \textbf{Binning por intervalos de la misma amplitud (equal-depth, frequency):} Se divide el rango de valores en intervalos que contengan aproximadamente el mismo número de elementos. 
    
    Para saver cuántos elementos debee tener cada intervalo se divide el número total de instancias por el número de intervalos. Para determinal cuáles son los valores en los que realizar la partición se suele utulizar el punto medio entre los dos extremos de los intervalos. En el caso de que valores repetidos caigan en intervalos diferentes, habrá que tomar la decisión de a qué intervalo se asignan dichos valores, permitiendo que existan invervalos con un número de valores alejados de la media.
\end{itemize}

\vspace{1 em}
\underline{\textbf{Discretización basada en histograma}}

Un \textbf{histograma} nos muestra la frecuencia de cada uno de los posibles valores del atributo, agrupando en un mismo \textit{balde} pares valor-frecuencia. Podemos discretizar el rango de valores de un atributo agrupando baldes:
\begin{itemize}
    \item \textbf{Intervalos de la misma longitud}
    \item \textbf{Intervalos de la misma frecuencia}
    \item \textbf{Varianza óptima.} Se consideran todas las posibilidades de agrupación de baldes y se selecciona la de menor varianza.
    \item \textbf{Máxima diferencia.} Los límites de los baldes se establecen entre los valores consecutivos con $\beta - 1$ mayores distancias, siendo $\beta$ el número de baldes deseado.
\end{itemize}

Los histogramas son muy efectivos tanto en datos densos como en dispersos, al igual que tanto para datos uniformes como para altamente sesgados. Además, las particiones basadas en varianza y la diferencia suelen ser más precisas y prácticas.

Hay varios criterios a la hora de elegir el número de intervalos que podemos destacar:
\begin{itemize}
    \item \textbf{Raíz cuadrada:}
    \[
        n_{intervalos} = \sqrt{n_{muestras}}
    \]
    \[
        ancho = \frac{\max(x) - \min(x)}{\sqrt{n_{muestras}}}
    \]

    \item \textbf{Sturges:}
    \[
        n_{intervalos} = 1 + \log_2(n_{muestras})
    \]
    \[
        ancho = \frac{\max(x) - \min(x)}{1 + \log_2(n_{muestras})}
    \]

    \item \textbf{Rice:}
    \[
        n_{intervalos} = \lceil 2n^{1/3} \rceil   \]
    \[
        ancho = \frac{\max(x) - \min(x)}{\lceil 2\sqrt[3]{n} \rceil}
    \]

    \item \textbf{Scott:}
    \[
        n_{intervalos} = \frac{\max(x) - \min(x)}{\frac{3.5\sigma}{n^{1/3}}}
    \]
    \[
        ancho = \frac{3.5\sigma}{n^{1/3}}
    \]

    \item \textbf{Freedman-Diaconis:}
    \[
        n_{intervalos} = \frac{\max(x) - \min(x)}{\frac{2 \cdot IQR(x)}{n^{1/3}}}
    \]
    \[
        ancho = \frac{2 \cdot IQR(x)}{n^{1/3}}
    \]
\end{itemize}

\vspace{1 em}
\underline{\textbf{Discretización basada en la entropía}}

Es una técnica descendente y supervisada que utiliza el concepto que gannacia de información, utilizando la entropía como variable objetivo para determinar los puntos de corte, muy parecido a lo que hacen los árboles de decisión.

El proceso es el siguiente:
\begin{itemize}
    \item \textbf{Cálculo de la entropía:} Se calcula la entropía inicial con la fórmula.
    \[
        H(S) = -\sum_{i=1}^{c} p_i \log_2 p_i
    \]
    donde $p_i$ es la probabilidad de que un objeto pertenezca a la clase $i$.
    \item \textbf{Selección de puntos de corte:} Se prueban diferente puntos de corte dentro del atributo continuo, quedando en conjunto izquierdo y derecho.
    \item \textbf{Cálculo de la ganancia de información:} Se calcula la ganancia de información.
    \[
        IG(T) = H(S) - \bigg( \frac{|S_{izq}|}{|S|}H(S_{izq}) + \frac{|S_{der}|}{|S|}H(S_{der}) \bigg)
    \]

    \item Se elige el punto que maximiza la ganancia de información.
    \item \textbf{División recursiva:} Se repite el proceso hasta cumplir el criterio de parada. Este puede ser:
    \begin{itemize}
        \item La ganancia de información es menor que un umbral.
        \item Se alcanza el número mínimo de insancias por intervalo.
    \end{itemize}
\end{itemize}

\vspace{1 em}
\underline{\textbf{Fusión de intervalos mediante análisis $\chi^2$}}

Es una técnica ascendente y supervisada que fusiona intervalos adyacentes que presenten una distribución de clases parecida. Preserva la relación entre la característica y la variable objetivo.

El proceso es el siguiente:
\begin{itemize}
    \item \textbf{Inicialización:} Ordenar los valores de la característica continua.
    \item \textbf{Binning inicial:} Cada valor es un \textit{bin} (intervalo) separado.
    \item \textbf{Cálculo de $\chi^2$:} Se calcula el estadístico $\chi^2$ para cada par de bins adyacentes.
    \item \textbf{Fusión de bins:} Se fusionan los bins con menor $\chi^2$.
    \item \textbf{Condición de parada:} Se repite el proceso hasta que se cumpla una condición de parada (número de bins, $\chi^2$ mínimo\dots).
    \item \textbf{Bins finales:} Los bins finales son los que mejor preservan la relación con la variable objetibo.
\end{itemize}

\vspace{1 em}
\underline{\textbf{Análisis de clusters}}

Se pueden usar algoritmos de clustering para discretizar atributos numéricos. Para ello hay que asociar una categoría a cada gurpo o cluster. Pueden generar discretizaciones de alta calidad, ya que tienen en cuentra la distrivución del atributo a discretizar además de la distnacia de los datos. Además, las técnicas de clustering jerárquico nos permiten obtener una jerarquía conceptual.

\subsubsection{De variables categóricas a numéricas}
Las variables categóricas son variables cuyo dominio lo forman un número finito de etiquetas o categorías. Puedes ser nominales (sin orden) o ordinales (con orden).

Algunas técnicas son:
\begin{itemize}
    \item \textbf{Codificación ordinal:} Se aplica a variables categóricas ordinales. La idea es mantener el orden de categorías asignando un número entero a cada categoría (por ejemplo pasar de $\{bajo, medio, alto\}$ a $\{1,2,3\}$).
    \item \textbf{Codificación one-hot:} Se aplica a categorías nominales. Se crea una nueva variable binaria para cada categoría, asignando un 1 si la categoría está presente y un 0 si no lo está.
    \item \textbf{Codificación por variables \textit{dummy}:} La codificación One-Hot tiene el problema de introducir información redundante. Por ello, la solución es crear $N-1$ variables para $N$ categorías, donde la categoría $N$ se representa con todas las variables a 0.
\end{itemize}


\subsection{Datos desbalanceados}

Se dice que en un dataset los datos están desbalancedaos cuando una o más clases presentan proporciones muy bajas respecto a las otras clases en el conjunto de entrenamiento. Un ejemplo sería el tener una clase $A$ con un 94\% y una clase $B$ co un 6\%.

En el caso de los datos desbalanceados, se espera que un modelo prediga la clase mayoritaria mejor que lo haría un clasificador que elija siempre la clase mayoritaria. Por ejemplo, en el ejemplo anterior un clasificador que prediga la clase $A$ con un 93\% sería un mal clasificador, pero si predice con un 95\% de precisión sería un buen clasificador.

Soluciones:
\begin{itemize}
    \item Utilizar técnicas de muestreo para mitigar el desbalanceo de clases.
    \item Utilizar otras medidas de rendimiento a la hora de evaluar modelos.
    \item Utilizar modelo que permitan mitigar esta problemática.
\end{itemize}

\subsubsection{Técnicas de muestreo}
Vamos a explicar las técnicas de muestreo que sirven para mitigar el desbalanceo de clases.
\begin{itemize}
    \item \textbf{Técnicas básicas:}
    \begin{itemize}
        \item \textbf{Downsampling:} Seleccionar aleatoriamente un subconjunto de todas las clases para que sus frecuencias se ajusten a la de la clase minoritaria.
        \item \textbf{Upsampling:} Realizar un muestreo aleatorio con reemplazo para que sus frecuencias se adapten a las de la clase mayoritaria.
    \end{itemize}

    \item \textbf{Otras ténicas:}
    \begin{itemize}
        \item \textbf{SMOTE:} Utiliza la información de los vecinos más cercanos para generar nuevas muestras de la clase minoritaria, haciendo así que las fronteras de la clase no se distorsionen. 
        \item \textbf{ROSE:} Esta técnica genera nuevas muestras en la vecindad de las ya existentes para equilibrar la frecuencia de clases. Es útil para problemas de clasificación binaria y se puede combinar con técnicas de sampling para evaluar modelos de aprendizaje. A la hora de generar las muestras se utiliza una distribución de probabilidad centrada en la muestra y con una matriz de covarianza concreta.
    \end{itemize}
\end{itemize}

\subsubsection{Medidas de rendimiento}
El error de clasificación y la precisión (accuracy) no son métricas apropiadas cuando tenemos métricas desbalanceadas. 

Algunas medidas de eficacia considerando la clase mayoritaria como la negativa son:
\begin{itemize}
    \item \textbf{Matriz de confusión.}
    \item \textbf{Precisión o valor predictivo positivo.} Es la proporción de verdaderos positivos sobre el total de predicciones positivas. $PPV = \frac{TP}{TP + FP}$.
    \item \textbf{Recall o sensibilidad.} Es la proporción de verdaderos positivos sobre el total de positivos reales. $Recall = \frac{TP}{TP + FN}$.
    \item \textbf{F1-score.} Es la media armónica de la precisión y el recall. $F1 = 2 \cdot \frac{PPV \cdot Recall}{PPV + Recall}$.
    \item \textbf{AUC-ROC.} Es el área bajo la curva ROC.\@ Cuanto más cerca de 1, mejor.
\end{itemize}

Depende de la combinación entre \textit{recall} y \textit{precisión} habrá difernetes interpretaciones del modelo:
\begin{itemize}
    \item \textbf{Recall alto y precisión alta:} La clase es detectada perfectamente por el modelo.
    \item \textbf{Recall bajo y precisión alta:} El modelo no puede detectar bien la clase pero cuando lo hace es muy fiable.
    \item \textbf{Recall alto y precisión baja:} La clase es detectada aceptablemente por el modelo pero también incluye muestas de otras clases.
    \item \textbf{Recall bajo y precisión baja:} El modelo no detecta bien la clase y cuando lo hace no es fiable.
\end{itemize}

\subsubsection{Modelos para datos desbalanceados}
Existen algoritmos optimizados para clases desbalanceadas, de forma que tienen en cuenta la distrivución de clases en la construcción del modelo. Algunos ejemplos son:
\begin{itemize}
    \item \textbf{SVM:} Dan buenos resultados para problemas desbalanceados y existen varias adaptaciones (z-SVM y GSVM-RU).
    \item \textbf{kNN:} kENN y CCNND.\@
    \item \textbf{Aprendizaje sensitivo al costo:} Consiste en cambiar el coste de los errores, dando mayor importancia a los falsos positivos de la clase mayoritaria o a los verdaderos positivos de la clase minoritaria. 
    
    Tiene varios problemas: la matriz de costes no se conoce y puede ser complicado definicarla, puede causar sobreajuste y algunos estudios apuntan a que es igual de eficiente que técnicas de muestreo.

    \item \textbf{Métodos de ensamble:} Reducen la varianza de la clasificación y existen métodos adaptados a las clases desbalanceadas (SMOTEBoost, RUSBott, DataBoostIM, cost-sensitive boosting y SMOTEBagging).
    \item \textbf{One-Class learning:} Son conocidos como métodos basados en reconocimiento. El modelo es entrenado para representar adecuadamente la clase minoritaria. Muchas ténicas no están preparads para ser entradas con una sola clase, aunque hay algunas adaptaciones (One Classs SVM, Isolation Forest, Minimum Covariance Determinant\dots).
\end{itemize}


\end{document}
